{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
    "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load SQuAD dataset and select only 5 examples\n",
    "# squad = load_dataset(\"squad\", split=\"train[:1]\")\n",
    "\n",
    "# for example in squad:\n",
    "#     print(f\"Context: {example['context']}\")\n",
    "#     print(f\"Question: {example['question']}\")\n",
    "#     print(f\"Answer: {example['answers']['text'][0]}\")\n",
    "#     print(\"=\"*80)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def format_qa(example):\n",
    "#     prompt = f\"Context: {example['context']}\\nQuestion: {example['question']}\\nAnswer:\"\n",
    "#     answer = example['answers']['text'][0] + tokenizer.eos_token\n",
    "#     return {\"input_text\": prompt, \"output_text\": answer}\n",
    "\n",
    "# formatted = squad.map(format_qa)\n",
    "\n",
    "# def tokenize(example):\n",
    "#     input_ids = tokenizer(example['input_text'], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "#     labels = tokenizer(example['input_text'] + \" \" + example['output_text'], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "#     return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "# tokenized_dataset = formatted.map(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class QADataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             \"input_ids\": self.data[idx][\"input_ids\"],\n",
    "#             \"attention_mask\": self.data[idx][\"input_ids\"] != tokenizer.pad_token_id,\n",
    "#             \"labels\": self.data[idx][\"labels\"]\n",
    "#         }\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# qa_dataset = QADataset(tokenized_dataset)\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./qa-gpt2\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=2,\n",
    "#     logging_steps=1,\n",
    "#     save_steps=10,\n",
    "#     save_total_limit=1,\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=qa_dataset,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# def generate_answer(context, question):\n",
    "#     prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "#     output = model.generate(input_ids, max_new_tokens=10,  eos_token_id=tokenizer.eos_token_id,pad_token_id=tokenizer.eos_token_id)  # Avoid warning if pad token is undefined)\n",
    "#     answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#     return answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# # Test on training examples\n",
    "# for example in squad:\n",
    "#     print(\"Q:\", example['question'])\n",
    "#     print(\"Predicted A:\", generate_answer(example['context'], example['question']))\n",
    "#     print(\"True A:\", example['answers']['text'][0])\n",
    "#     print(\"=\"*60)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model with Separate Key Query and Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:32.844029Z",
     "iopub.status.busy": "2025-07-27T13:42:32.843496Z",
     "iopub.status.idle": "2025-07-27T13:42:40.510000Z",
     "shell.execute_reply": "2025-07-27T13:42:40.509239Z",
     "shell.execute_reply.started": "2025-07-27T13:42:32.844002Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2Model, GPT2PreTrainedModel\n",
    "from transformers.modeling_utils import Conv1D\n",
    "\n",
    "# --------- Custom Attention Module using Conv1D ----------\n",
    "class CustomGPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.q_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "        self.k_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "        self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Register causal mask buffer (same as original GPT2)\n",
    "        max_positions = config.max_position_embeddings\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "                1, 1, max_positions, max_positions\n",
    "            ),\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4))\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        # embed_dim should equal num_heads * head_dim\n",
    "        assert embed_dim == self.num_heads * self.head_dim, f\"Embed dim {embed_dim} != num_heads * head_dim {self.num_heads * self.head_dim}\"\n",
    "        \n",
    "        # reshape to (batch_size, seq_len, num_heads, head_dim)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        # permute to (batch_size, num_heads, seq_len, head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def _merge_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (self.embed_dim,)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def forward(self, hidden_states, layer_past=None, attention_mask=None,\n",
    "                head_mask=None, use_cache=False, output_attentions=False):\n",
    "    \n",
    "        # Project to Q, K, V\n",
    "        query = self.q_proj(hidden_states)\n",
    "        key = self.k_proj(hidden_states)\n",
    "        value = self.v_proj(hidden_states)\n",
    "\n",
    "        # Split heads\n",
    "        query = self._split_heads(query)\n",
    "        key = self._split_heads(key)\n",
    "        value = self._split_heads(value)\n",
    "\n",
    "        # Handle past keys/values for generation\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        if use_cache:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attn_weights = attn_weights * self.scale\n",
    "\n",
    "        # Apply causal mask\n",
    "        query_length, key_length = query.size(-2), key.size(-2)\n",
    "        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "        mask_value = torch.finfo(attn_weights.dtype).min\n",
    "        mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
    "        attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
    "\n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # Softmax\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = attn_weights.type(value.dtype)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Apply head mask if provided\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "        attn_output = self._merge_heads(attn_output)\n",
    "\n",
    "        # Final projection\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:40.512257Z",
     "iopub.status.busy": "2025-07-27T13:42:40.511508Z",
     "iopub.status.idle": "2025-07-27T13:42:40.518055Z",
     "shell.execute_reply": "2025-07-27T13:42:40.517278Z",
     "shell.execute_reply.started": "2025-07-27T13:42:40.512235Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------- Custom GPT2 Block ----------\n",
    "class CustomGPT2Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.attn = CustomGPT2Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = GPT2Block(config).mlp\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        layer_past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_outputs[0]\n",
    "        outputs = attn_outputs[1:]\n",
    "\n",
    "        hidden_states = residual + attn_output\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        return (hidden_states,) + outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:40.519032Z",
     "iopub.status.busy": "2025-07-27T13:42:40.518792Z",
     "iopub.status.idle": "2025-07-27T13:42:40.535290Z",
     "shell.execute_reply": "2025-07-27T13:42:40.534644Z",
     "shell.execute_reply.started": "2025-07-27T13:42:40.519005Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------- Custom GPT2 Model ----------\n",
    "class CustomGPT2Model(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.h = nn.ModuleList([CustomGPT2Block(config) for _ in range(config.num_hidden_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:40.536558Z",
     "iopub.status.busy": "2025-07-27T13:42:40.536321Z",
     "iopub.status.idle": "2025-07-27T13:42:40.553224Z",
     "shell.execute_reply": "2025-07-27T13:42:40.552637Z",
     "shell.execute_reply.started": "2025-07-27T13:42:40.536532Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        # Initialize the parent class first\n",
    "        super(GPT2PreTrainedModel, self).__init__(config)\n",
    "        \n",
    "        # Replace the transformer with our custom one\n",
    "        self.transformer = CustomGPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Forward through transformer (excluding labels)\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        hidden_states = transformer_outputs[0]\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past_key_values:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "\n",
    "            # Some generation methods already pass only the last input ID\n",
    "            if input_ids.shape[1] > past_length:\n",
    "                remove_prefix_length = past_length\n",
    "            else:\n",
    "                # Default to old behavior: keep only final ID\n",
    "                remove_prefix_length = input_ids.shape[1] - 1\n",
    "\n",
    "            input_ids = input_ids[:, remove_prefix_length:]\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "        else:\n",
    "            position_ids = None\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"position_ids\": position_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:40.554226Z",
     "iopub.status.busy": "2025-07-27T13:42:40.553923Z",
     "iopub.status.idle": "2025-07-27T13:42:40.573529Z",
     "shell.execute_reply": "2025-07-27T13:42:40.572963Z",
     "shell.execute_reply.started": "2025-07-27T13:42:40.554202Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------- Copy Weights from Original GPT2 Model ----------\n",
    "def copy_weights(original_model, custom_model):\n",
    "    orig_state_dict = original_model.state_dict()\n",
    "    custom_state_dict = custom_model.state_dict()\n",
    "\n",
    "    for name, param in orig_state_dict.items():\n",
    "        if \"attn.c_attn.weight\" in name:\n",
    "            layer_num = int(name.split('.')[2])\n",
    "            prefix = f'transformer.h.{layer_num}.attn.'\n",
    "            \n",
    "            # Original c_attn weight shape: (embed_dim, 3 * embed_dim)\n",
    "            # Need to split along dim=1 (the 3 * embed_dim dimension)\n",
    "            embed_dim = param.shape[0]\n",
    "            q_weight, k_weight, v_weight = torch.split(param, embed_dim, dim=1)\n",
    "            \n",
    "            # Conv1D weight shape is (input_dim, output_dim), no transpose needed\n",
    "            custom_state_dict[f'{prefix}q_proj.weight'].copy_(q_weight)\n",
    "            custom_state_dict[f'{prefix}k_proj.weight'].copy_(k_weight)  \n",
    "            custom_state_dict[f'{prefix}v_proj.weight'].copy_(v_weight)\n",
    "\n",
    "        elif \"attn.c_attn.bias\" in name:\n",
    "            layer_num = int(name.split('.')[2])\n",
    "            prefix = f'transformer.h.{layer_num}.attn.'\n",
    "            hidden_size = param.shape[0] // 3\n",
    "\n",
    "            q_bias, k_bias, v_bias = torch.split(param, hidden_size)\n",
    "            custom_state_dict[f'{prefix}q_proj.bias'].copy_(q_bias)\n",
    "            custom_state_dict[f'{prefix}k_proj.bias'].copy_(k_bias)\n",
    "            custom_state_dict[f'{prefix}v_proj.bias'].copy_(v_bias)\n",
    "\n",
    "        elif \"attn.c_proj.weight\" in name:\n",
    "            # Copy c_proj weights directly\n",
    "            layer_num = int(name.split('.')[2])\n",
    "            prefix = f'transformer.h.{layer_num}.attn.'\n",
    "            custom_state_dict[f'{prefix}c_proj.weight'].copy_(param)\n",
    "            \n",
    "        else:\n",
    "            if name in custom_state_dict:\n",
    "                custom_state_dict[name].copy_(param)\n",
    "\n",
    "    custom_model.load_state_dict(custom_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:41.681772Z",
     "iopub.status.busy": "2025-07-27T13:42:41.681479Z",
     "iopub.status.idle": "2025-07-27T13:42:41.685308Z",
     "shell.execute_reply": "2025-07-27T13:42:41.684528Z",
     "shell.execute_reply.started": "2025-07-27T13:42:41.681747Z"
    }
   },
   "outputs": [],
   "source": [
    "# config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "# original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# custom_model = CustomGPT2LMHeadModel(config)\n",
    "# copy_weights(original_model, custom_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5e-5)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:07:41.305338Z",
     "iopub.status.busy": "2025-07-27T16:07:41.305029Z",
     "iopub.status.idle": "2025-07-27T16:41:42.576131Z",
     "shell.execute_reply": "2025-07-27T16:41:42.575294Z",
     "shell.execute_reply.started": "2025-07-27T16:07:41.305319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▍                | 200/2500 [01:17<14:52,  2.58it/s, loss=0.869]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 - Loss: 0.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▉               | 400/2500 [02:35<13:33,  2.58it/s, loss=0.907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400 - Loss: 0.9071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████▎             | 600/2500 [03:53<12:16,  2.58it/s, loss=0.855]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600 - Loss: 0.8553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▊            | 800/2500 [05:10<11:00,  2.57it/s, loss=0.967]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800 - Loss: 0.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▊          | 1000/2500 [06:28<09:43,  2.57it/s, loss=0.889]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 - Loss: 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████████▏        | 1200/2500 [07:46<08:27,  2.56it/s, loss=0.854]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1200 - Loss: 0.8538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|██████████        | 1400/2500 [09:04<07:08,  2.57it/s, loss=0.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1400 - Loss: 0.7401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▉      | 1600/2500 [10:22<05:51,  2.56it/s, loss=0.614]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1600 - Loss: 0.6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|████████████▏    | 1800/2500 [11:40<04:32,  2.57it/s, loss=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1800 - Loss: 0.6954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|█████████████▌   | 2000/2500 [12:58<03:15,  2.56it/s, loss=0.664]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000 - Loss: 0.6640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████▉  | 2200/2500 [14:16<01:57,  2.55it/s, loss=0.669]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2200 - Loss: 0.6685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|████████████████▎| 2400/2500 [15:35<00:39,  2.55it/s, loss=0.868]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2400 - Loss: 0.8684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                 | 100/2500 [00:39<15:37,  2.56it/s, loss=0.607]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2600 - Loss: 0.6067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██▏               | 300/2500 [01:57<14:27,  2.54it/s, loss=0.774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2800 - Loss: 0.7736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▌              | 500/2500 [03:16<13:07,  2.54it/s, loss=0.779]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000 - Loss: 0.7788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|█████             | 700/2500 [04:35<11:51,  2.53it/s, loss=0.687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3200 - Loss: 0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████▊            | 900/2500 [05:55<10:41,  2.49it/s, loss=0.57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3400 - Loss: 0.5699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████▍         | 1100/2500 [07:17<09:42,  2.40it/s, loss=0.469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3600 - Loss: 0.4695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▊        | 1300/2500 [08:39<08:10,  2.45it/s, loss=0.523]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3800 - Loss: 0.5232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████████▏      | 1500/2500 [10:03<07:16,  2.29it/s, loss=0.498]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000 - Loss: 0.4983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|████████████▏     | 1700/2500 [11:27<05:34,  2.39it/s, loss=0.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4200 - Loss: 0.7405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▉    | 1900/2500 [12:49<04:06,  2.43it/s, loss=0.515]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4400 - Loss: 0.5154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|██████████████▎  | 2100/2500 [14:12<02:42,  2.46it/s, loss=0.614]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4600 - Loss: 0.6145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|███████████████▋ | 2300/2500 [15:35<01:22,  2.43it/s, loss=0.568]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4800 - Loss: 0.5681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000 - Loss: 0.6705\n",
      "\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▍                | 200/2500 [01:23<16:02,  2.39it/s, loss=0.506]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5200 - Loss: 0.5061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▉               | 400/2500 [02:47<14:33,  2.40it/s, loss=0.467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5400 - Loss: 0.4675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████▎             | 600/2500 [04:11<13:09,  2.41it/s, loss=0.459]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5600 - Loss: 0.4587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▊            | 800/2500 [05:35<11:48,  2.40it/s, loss=0.469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5800 - Loss: 0.4686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▊          | 1000/2500 [06:59<10:24,  2.40it/s, loss=0.342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000 - Loss: 0.3420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████████▏        | 1200/2500 [08:23<09:21,  2.32it/s, loss=0.396]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6200 - Loss: 0.3957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|██████████        | 1400/2500 [09:46<07:37,  2.40it/s, loss=0.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6400 - Loss: 0.4199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▉      | 1600/2500 [11:09<06:13,  2.41it/s, loss=0.414]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6600 - Loss: 0.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|████████████▏    | 1800/2500 [12:33<04:51,  2.41it/s, loss=0.475]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6800 - Loss: 0.4754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|██████████████▍   | 2000/2500 [13:57<03:37,  2.30it/s, loss=0.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7000 - Loss: 0.2597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████▉  | 2200/2500 [15:20<02:04,  2.40it/s, loss=0.322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7200 - Loss: 0.3218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|████████████████▎| 2400/2500 [16:44<00:41,  2.39it/s, loss=0.564]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7400 - Loss: 0.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                 | 100/2500 [00:42<16:38,  2.40it/s, loss=0.288]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7600 - Loss: 0.2884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██▏               | 300/2500 [02:05<15:15,  2.40it/s, loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7800 - Loss: 0.2535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▌              | 500/2500 [03:29<14:29,  2.30it/s, loss=0.439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000 - Loss: 0.4385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|█████             | 700/2500 [04:53<12:28,  2.40it/s, loss=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8200 - Loss: 0.3643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████▍           | 900/2500 [06:17<11:06,  2.40it/s, loss=0.285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8400 - Loss: 0.2850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████▍         | 1100/2500 [07:40<09:42,  2.40it/s, loss=0.279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8600 - Loss: 0.2787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▊        | 1300/2500 [09:03<08:03,  2.48it/s, loss=0.323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8800 - Loss: 0.3228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████████▏      | 1500/2500 [10:26<06:58,  2.39it/s, loss=0.376]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9000 - Loss: 0.3757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|███████████▌     | 1700/2500 [11:49<05:32,  2.41it/s, loss=0.286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9200 - Loss: 0.2863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▉    | 1900/2500 [13:12<04:09,  2.41it/s, loss=0.263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9400 - Loss: 0.2635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|███████████████   | 2100/2500 [14:35<02:46,  2.40it/s, loss=0.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9600 - Loss: 0.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|███████████████▋ | 2300/2500 [15:58<01:23,  2.41it/s, loss=0.247]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9800 - Loss: 0.2472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10000 - Loss: 0.3761\n",
      "\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▍                | 200/2500 [01:23<16:35,  2.31it/s, loss=0.284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10200 - Loss: 0.2839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▉               | 400/2500 [02:46<14:33,  2.40it/s, loss=0.241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10400 - Loss: 0.2413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████▌              | 600/2500 [04:09<13:09,  2.41it/s, loss=0.27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10600 - Loss: 0.2697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▊            | 800/2500 [05:32<11:46,  2.41it/s, loss=0.308]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10800 - Loss: 0.3084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▊          | 1000/2500 [06:55<10:22,  2.41it/s, loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11000 - Loss: 0.3095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████████▏        | 1200/2500 [08:18<09:01,  2.40it/s, loss=0.268]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11200 - Loss: 0.2676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████████▌       | 1400/2500 [09:41<07:36,  2.41it/s, loss=0.318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11400 - Loss: 0.3177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▉      | 1600/2500 [11:04<06:13,  2.41it/s, loss=0.297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11600 - Loss: 0.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|████████████▏    | 1800/2500 [12:27<04:50,  2.41it/s, loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11800 - Loss: 0.1873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|██████████████▍   | 2000/2500 [13:50<03:28,  2.40it/s, loss=0.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12000 - Loss: 0.2603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████▉  | 2200/2500 [15:13<02:04,  2.41it/s, loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12200 - Loss: 0.1861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|████████████████▎| 2400/2500 [16:37<00:41,  2.40it/s, loss=0.282]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12400 - Loss: 0.2825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                 | 100/2500 [00:41<16:38,  2.40it/s, loss=0.242]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12600 - Loss: 0.2420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██▏               | 300/2500 [02:04<14:47,  2.48it/s, loss=0.297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12800 - Loss: 0.2966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▊               | 500/2500 [03:27<13:52,  2.40it/s, loss=0.27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13000 - Loss: 0.2697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|█████▌              | 700/2500 [04:50<12:26,  2.41it/s, loss=0.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13200 - Loss: 0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████▍           | 900/2500 [06:13<11:04,  2.41it/s, loss=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13400 - Loss: 0.2673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████▍         | 1100/2500 [07:37<09:41,  2.41it/s, loss=0.262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13600 - Loss: 0.2621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████████▎        | 1300/2500 [09:00<08:18,  2.41it/s, loss=0.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13800 - Loss: 0.2496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████████▏      | 1500/2500 [10:23<06:56,  2.40it/s, loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14000 - Loss: 0.2033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|████████████▏     | 1700/2500 [11:46<05:32,  2.41it/s, loss=0.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14200 - Loss: 0.2303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▉    | 1900/2500 [13:10<04:08,  2.41it/s, loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14400 - Loss: 0.2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|██████████████▎  | 2100/2500 [14:33<02:46,  2.40it/s, loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14600 - Loss: 0.1956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|████████████████▌ | 2300/2500 [15:56<01:23,  2.41it/s, loss=0.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14800 - Loss: 0.2305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15000 - Loss: 0.2110\n",
      "\n",
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▍                | 200/2500 [01:22<15:55,  2.41it/s, loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15200 - Loss: 0.1556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▉               | 400/2500 [02:45<14:32,  2.41it/s, loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15400 - Loss: 0.1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████▎             | 600/2500 [04:06<12:30,  2.53it/s, loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15600 - Loss: 0.2237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▊            | 800/2500 [05:25<11:09,  2.54it/s, loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15800 - Loss: 0.1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▊          | 1000/2500 [06:44<09:53,  2.53it/s, loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16000 - Loss: 0.1891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████████▏        | 1200/2500 [08:03<08:31,  2.54it/s, loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16200 - Loss: 0.1884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████████▌       | 1400/2500 [09:22<07:15,  2.53it/s, loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16400 - Loss: 0.1726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▉      | 1600/2500 [10:41<05:54,  2.54it/s, loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16600 - Loss: 0.2238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|████████████▏    | 1800/2500 [12:00<04:35,  2.54it/s, loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16800 - Loss: 0.2081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|█████████████▌   | 2000/2500 [13:19<03:17,  2.53it/s, loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17000 - Loss: 0.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████▉  | 2200/2500 [14:37<01:58,  2.54it/s, loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17200 - Loss: 0.1639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|████████████████▎| 2400/2500 [15:56<00:39,  2.50it/s, loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17400 - Loss: 0.1822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▊                  | 100/2500 [00:39<15:46,  2.54it/s, loss=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17600 - Loss: 0.1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██▏               | 300/2500 [01:58<14:26,  2.54it/s, loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17800 - Loss: 0.1652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▌              | 500/2500 [03:17<13:07,  2.54it/s, loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18000 - Loss: 0.1490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|█████             | 700/2500 [04:36<11:48,  2.54it/s, loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18200 - Loss: 0.1462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████▍           | 900/2500 [05:55<10:30,  2.54it/s, loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18400 - Loss: 0.1731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████▉          | 1100/2500 [07:14<09:13,  2.53it/s, loss=0.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18600 - Loss: 0.1399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▎       | 1300/2500 [08:32<07:53,  2.53it/s, loss=0.0911]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18800 - Loss: 0.0911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████████▊       | 1500/2500 [09:51<06:34,  2.53it/s, loss=0.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19000 - Loss: 0.1404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|███████████▌     | 1700/2500 [11:10<05:15,  2.54it/s, loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19200 - Loss: 0.1817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▉    | 1900/2500 [12:29<03:56,  2.54it/s, loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19400 - Loss: 0.1367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|██████████████▎  | 2100/2500 [13:48<02:38,  2.53it/s, loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19600 - Loss: 0.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|███████████████▋ | 2300/2500 [15:07<01:18,  2.54it/s, loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19800 - Loss: 0.1435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20000 - Loss: 0.1796\n",
      "\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▍                | 200/2500 [01:18<15:05,  2.54it/s, loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20200 - Loss: 0.1410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▉               | 400/2500 [02:37<13:49,  2.53it/s, loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20400 - Loss: 0.1137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████▎             | 600/2500 [03:56<12:26,  2.54it/s, loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20600 - Loss: 0.2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▊            | 800/2500 [05:15<11:08,  2.54it/s, loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20800 - Loss: 0.1389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▊          | 1000/2500 [06:34<09:50,  2.54it/s, loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21000 - Loss: 0.1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████████▏        | 1200/2500 [07:53<08:32,  2.54it/s, loss=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21200 - Loss: 0.1761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████████▌       | 1400/2500 [09:11<07:12,  2.54it/s, loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21400 - Loss: 0.1144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▉      | 1600/2500 [10:30<05:53,  2.54it/s, loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21600 - Loss: 0.1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|████████████▏    | 1800/2500 [11:49<04:39,  2.51it/s, loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21800 - Loss: 0.1283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|█████████████▌   | 2000/2500 [13:08<03:17,  2.53it/s, loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 22000 - Loss: 0.1143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████▉  | 2200/2500 [14:27<01:58,  2.54it/s, loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 22200 - Loss: 0.1362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|████████████████▎| 2400/2500 [15:46<00:39,  2.53it/s, loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 22400 - Loss: 0.1108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                 | 100/2500 [00:39<15:48,  2.53it/s, loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 22600 - Loss: 0.1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██▏               | 300/2500 [01:58<14:25,  2.54it/s, loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 22800 - Loss: 0.1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▌              | 500/2500 [03:17<13:09,  2.53it/s, loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23000 - Loss: 0.1395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|█████             | 700/2500 [04:36<11:49,  2.54it/s, loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23200 - Loss: 0.1149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████▍           | 900/2500 [05:55<10:31,  2.54it/s, loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23400 - Loss: 0.1431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████▉          | 1100/2500 [07:14<09:11,  2.54it/s, loss=0.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23600 - Loss: 0.1699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▊        | 1300/2500 [08:33<07:53,  2.54it/s, loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23800 - Loss: 0.1183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████████▏      | 1500/2500 [09:52<06:33,  2.54it/s, loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24000 - Loss: 0.1038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|███████████▌     | 1700/2500 [11:10<05:14,  2.54it/s, loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24200 - Loss: 0.1248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|█████████████▋    | 1900/2500 [12:29<03:56,  2.54it/s, loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24400 - Loss: 0.1104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|██████████████▎  | 2100/2500 [13:48<02:40,  2.50it/s, loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24600 - Loss: 0.1491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|███████████████▋ | 2300/2500 [15:07<01:19,  2.53it/s, loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24800 - Loss: 0.1547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25000 - Loss: 0.1325\n",
      "\n",
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▎               | 200/2500 [01:18<15:06,  2.54it/s, loss=0.0871]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25200 - Loss: 0.0871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▉               | 400/2500 [02:37<13:47,  2.54it/s, loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25400 - Loss: 0.1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████             | 600/2500 [03:56<12:28,  2.54it/s, loss=0.0931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25600 - Loss: 0.0931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▊            | 800/2500 [05:15<11:10,  2.53it/s, loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25800 - Loss: 0.1091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▊          | 1000/2500 [06:34<09:52,  2.53it/s, loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 26000 - Loss: 0.1137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████████▏        | 1200/2500 [07:53<08:33,  2.53it/s, loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 26200 - Loss: 0.1063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|████████▉       | 1400/2500 [09:12<07:14,  2.53it/s, loss=0.0871]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 26400 - Loss: 0.0871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▉      | 1600/2500 [10:31<05:54,  2.54it/s, loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 26600 - Loss: 0.1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|████████████▏    | 1800/2500 [11:50<04:35,  2.54it/s, loss=0.081]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 26800 - Loss: 0.0810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|█████████████▌   | 2000/2500 [13:08<03:17,  2.53it/s, loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27000 - Loss: 0.1212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████▉  | 2200/2500 [14:27<01:58,  2.54it/s, loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27200 - Loss: 0.1126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████▎| 2400/2500 [15:46<00:39,  2.54it/s, loss=0.0915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27400 - Loss: 0.0915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                | 100/2500 [00:39<15:45,  2.54it/s, loss=0.0779]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27600 - Loss: 0.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██▏               | 300/2500 [01:58<14:29,  2.53it/s, loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27800 - Loss: 0.1481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▌              | 500/2500 [03:17<13:08,  2.54it/s, loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 28000 - Loss: 0.1329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|████▊            | 700/2500 [04:36<11:50,  2.53it/s, loss=0.0946]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 28200 - Loss: 0.0946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████           | 900/2500 [05:54<10:32,  2.53it/s, loss=0.0933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 28400 - Loss: 0.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████         | 1100/2500 [07:13<09:13,  2.53it/s, loss=0.0795]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 28600 - Loss: 0.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▎       | 1300/2500 [08:32<07:52,  2.54it/s, loss=0.0735]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 28800 - Loss: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████████▏      | 1500/2500 [09:51<06:35,  2.53it/s, loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 29000 - Loss: 0.1109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████████▉     | 1700/2500 [11:10<05:15,  2.53it/s, loss=0.0803]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 29200 - Loss: 0.0803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▉    | 1900/2500 [12:29<03:59,  2.50it/s, loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 29400 - Loss: 0.1094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|██████████████▎  | 2100/2500 [13:48<02:37,  2.54it/s, loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 29600 - Loss: 0.1308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|██████████████▋ | 2300/2500 [15:07<01:18,  2.54it/s, loss=0.0943]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 29800 - Loss: 0.0943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30000 - Loss: 0.0923\n",
      "\n",
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▍                | 200/2500 [01:18<15:06,  2.54it/s, loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30200 - Loss: 0.1188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▉               | 400/2500 [02:37<13:46,  2.54it/s, loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30400 - Loss: 0.1386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████             | 600/2500 [03:56<12:29,  2.54it/s, loss=0.0933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30600 - Loss: 0.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▍           | 800/2500 [05:15<11:09,  2.54it/s, loss=0.0899]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30800 - Loss: 0.0899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▍         | 1000/2500 [06:34<09:50,  2.54it/s, loss=0.0939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 31000 - Loss: 0.0939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|███████▋        | 1200/2500 [07:53<08:35,  2.52it/s, loss=0.0952]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 31200 - Loss: 0.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|████████▉       | 1400/2500 [09:12<07:12,  2.54it/s, loss=0.0849]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 31400 - Loss: 0.0849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▉      | 1600/2500 [10:31<05:55,  2.53it/s, loss=0.082]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 31600 - Loss: 0.0820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████████▌    | 1800/2500 [11:50<04:36,  2.53it/s, loss=0.0858]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 31800 - Loss: 0.0858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████████▊   | 2000/2500 [13:08<03:17,  2.54it/s, loss=0.0931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 32000 - Loss: 0.0931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████▉  | 2200/2500 [14:27<01:58,  2.52it/s, loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 32200 - Loss: 0.1055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████▎| 2400/2500 [15:46<00:39,  2.53it/s, loss=0.0717]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 32400 - Loss: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                | 100/2500 [00:39<15:49,  2.53it/s, loss=0.0932]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 32600 - Loss: 0.0932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██▏               | 300/2500 [01:58<14:29,  2.53it/s, loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 32800 - Loss: 0.1035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▍             | 500/2500 [03:17<13:06,  2.54it/s, loss=0.0899]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 33000 - Loss: 0.0899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|█████             | 700/2500 [04:36<11:46,  2.55it/s, loss=0.092]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 33200 - Loss: 0.0920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████           | 900/2500 [05:54<10:29,  2.54it/s, loss=0.0846]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 33400 - Loss: 0.0846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████         | 1100/2500 [07:13<09:10,  2.54it/s, loss=0.0985]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 33600 - Loss: 0.0985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▎       | 1300/2500 [08:32<07:53,  2.54it/s, loss=0.0933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 33800 - Loss: 0.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████████▏      | 1500/2500 [09:51<06:34,  2.53it/s, loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 34000 - Loss: 0.1021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████████▉     | 1700/2500 [11:09<05:14,  2.54it/s, loss=0.0814]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 34200 - Loss: 0.0814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▏   | 1900/2500 [12:28<03:55,  2.54it/s, loss=0.0765]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 34400 - Loss: 0.0765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|█████████████▍  | 2100/2500 [13:47<02:37,  2.53it/s, loss=0.0706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 34600 - Loss: 0.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|██████████████▋ | 2300/2500 [15:06<01:18,  2.54it/s, loss=0.0973]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 34800 - Loss: 0.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35000 - Loss: 0.1034\n",
      "\n",
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▎               | 200/2500 [01:18<15:07,  2.54it/s, loss=0.0902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35200 - Loss: 0.0902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▋              | 400/2500 [02:37<13:47,  2.54it/s, loss=0.0761]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35400 - Loss: 0.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████             | 600/2500 [03:56<12:30,  2.53it/s, loss=0.0744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35600 - Loss: 0.0744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▍           | 800/2500 [05:15<11:09,  2.54it/s, loss=0.0835]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35800 - Loss: 0.0835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▍         | 1000/2500 [06:34<09:51,  2.54it/s, loss=0.0817]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36000 - Loss: 0.0817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|███████▋        | 1200/2500 [07:52<08:34,  2.53it/s, loss=0.0612]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36200 - Loss: 0.0612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|████████▉       | 1400/2500 [09:11<07:12,  2.54it/s, loss=0.0733]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36400 - Loss: 0.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▏     | 1600/2500 [10:30<05:54,  2.54it/s, loss=0.0702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36600 - Loss: 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████████▌    | 1800/2500 [11:49<04:35,  2.54it/s, loss=0.0684]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36800 - Loss: 0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|█████████████▌   | 2000/2500 [13:08<03:16,  2.54it/s, loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 37000 - Loss: 0.1157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|███████████████▊  | 2200/2500 [14:26<01:57,  2.55it/s, loss=0.07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 37200 - Loss: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████▎| 2400/2500 [15:45<00:39,  2.54it/s, loss=0.0974]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 37400 - Loss: 0.0974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                | 100/2500 [00:39<15:42,  2.55it/s, loss=0.0731]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 37600 - Loss: 0.0731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██               | 300/2500 [01:57<14:25,  2.54it/s, loss=0.0799]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 37800 - Loss: 0.0799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▍             | 500/2500 [03:16<13:05,  2.55it/s, loss=0.0638]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 38000 - Loss: 0.0638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|████▊            | 700/2500 [04:35<11:49,  2.54it/s, loss=0.0841]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 38200 - Loss: 0.0841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████           | 900/2500 [05:53<10:29,  2.54it/s, loss=0.0763]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 38400 - Loss: 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████▍         | 1100/2500 [07:12<09:10,  2.54it/s, loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 38600 - Loss: 0.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▎       | 1300/2500 [08:30<07:50,  2.55it/s, loss=0.0806]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 38800 - Loss: 0.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████████▌      | 1500/2500 [09:49<06:33,  2.54it/s, loss=0.0763]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 39000 - Loss: 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████████▉     | 1700/2500 [11:08<05:14,  2.54it/s, loss=0.0778]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 39200 - Loss: 0.0778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▏   | 1900/2500 [12:26<03:55,  2.54it/s, loss=0.0675]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 39400 - Loss: 0.0675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|█████████████▍  | 2100/2500 [13:45<02:37,  2.54it/s, loss=0.0848]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 39600 - Loss: 0.0848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|██████████████▋ | 2300/2500 [15:04<01:18,  2.54it/s, loss=0.0664]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 39800 - Loss: 0.0664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40000 - Loss: 0.0692\n",
      "\n",
      "Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▎               | 200/2500 [01:18<15:06,  2.54it/s, loss=0.0876]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40200 - Loss: 0.0876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▋              | 400/2500 [02:37<13:44,  2.55it/s, loss=0.0718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40400 - Loss: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████             | 600/2500 [03:56<12:31,  2.53it/s, loss=0.0683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40600 - Loss: 0.0683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▊            | 800/2500 [05:14<11:07,  2.55it/s, loss=0.075]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40800 - Loss: 0.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▍         | 1000/2500 [06:33<09:51,  2.54it/s, loss=0.0788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41000 - Loss: 0.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|███████▋        | 1200/2500 [07:52<08:32,  2.54it/s, loss=0.0959]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41200 - Loss: 0.0959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|████████▉       | 1400/2500 [09:10<07:13,  2.54it/s, loss=0.0687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41400 - Loss: 0.0687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▏     | 1600/2500 [10:29<05:54,  2.54it/s, loss=0.0588]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41600 - Loss: 0.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|████████████▏    | 1800/2500 [11:48<04:35,  2.54it/s, loss=0.066]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41800 - Loss: 0.0660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████████▊   | 2000/2500 [13:06<03:16,  2.55it/s, loss=0.0726]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 42000 - Loss: 0.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████  | 2200/2500 [14:25<01:57,  2.55it/s, loss=0.0507]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 42200 - Loss: 0.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████▎| 2400/2500 [15:43<00:39,  2.54it/s, loss=0.0623]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 42400 - Loss: 0.0623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                 | 100/2500 [00:39<15:42,  2.55it/s, loss=0.065]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 42600 - Loss: 0.0650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██               | 300/2500 [01:57<14:26,  2.54it/s, loss=0.0549]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 42800 - Loss: 0.0549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▍             | 500/2500 [03:16<13:04,  2.55it/s, loss=0.0836]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 43000 - Loss: 0.0836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|████▊            | 700/2500 [04:35<11:46,  2.55it/s, loss=0.0677]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 43200 - Loss: 0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████▍           | 900/2500 [05:53<10:31,  2.53it/s, loss=0.062]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 43400 - Loss: 0.0620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████         | 1100/2500 [07:12<09:11,  2.54it/s, loss=0.0505]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 43600 - Loss: 0.0505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▎       | 1300/2500 [08:31<07:51,  2.55it/s, loss=0.0645]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 43800 - Loss: 0.0645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████████▌      | 1500/2500 [09:50<06:33,  2.54it/s, loss=0.0764]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 44000 - Loss: 0.0764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████████▉     | 1700/2500 [11:08<05:15,  2.54it/s, loss=0.0608]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 44200 - Loss: 0.0608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▏   | 1900/2500 [12:27<03:56,  2.54it/s, loss=0.0549]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 44400 - Loss: 0.0549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|█████████████▍  | 2100/2500 [13:46<02:37,  2.55it/s, loss=0.0666]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 44600 - Loss: 0.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|██████████████▋ | 2300/2500 [15:04<01:18,  2.55it/s, loss=0.0839]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 44800 - Loss: 0.0839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 45000 - Loss: 0.0806\n",
      "\n",
      "Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▎               | 200/2500 [01:18<15:01,  2.55it/s, loss=0.0704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 45200 - Loss: 0.0704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▋              | 400/2500 [02:37<13:44,  2.55it/s, loss=0.0581]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 45400 - Loss: 0.0581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████             | 600/2500 [03:55<12:25,  2.55it/s, loss=0.0536]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 45600 - Loss: 0.0536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▍           | 800/2500 [05:14<11:07,  2.55it/s, loss=0.0602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 45800 - Loss: 0.0602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▍         | 1000/2500 [06:33<09:50,  2.54it/s, loss=0.0695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 46000 - Loss: 0.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|███████▋        | 1200/2500 [07:51<08:30,  2.55it/s, loss=0.0874]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 46200 - Loss: 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|████████▉       | 1400/2500 [09:10<07:12,  2.54it/s, loss=0.0703]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 46400 - Loss: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▉      | 1600/2500 [10:28<05:54,  2.54it/s, loss=0.058]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 46600 - Loss: 0.0580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████████▌    | 1800/2500 [11:47<04:35,  2.55it/s, loss=0.0877]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 46800 - Loss: 0.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████████▊   | 2000/2500 [13:05<03:16,  2.55it/s, loss=0.0572]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 47000 - Loss: 0.0572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████  | 2200/2500 [14:24<01:58,  2.54it/s, loss=0.0725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 47200 - Loss: 0.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████▎| 2400/2500 [15:43<00:39,  2.54it/s, loss=0.0497]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 47400 - Loss: 0.0497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                | 100/2500 [00:39<15:43,  2.54it/s, loss=0.0612]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 47600 - Loss: 0.0612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██               | 300/2500 [01:57<14:27,  2.54it/s, loss=0.0662]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 47800 - Loss: 0.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▍             | 500/2500 [03:16<13:05,  2.55it/s, loss=0.0656]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 48000 - Loss: 0.0656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|████▊            | 700/2500 [04:35<11:45,  2.55it/s, loss=0.0545]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 48200 - Loss: 0.0545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████           | 900/2500 [05:53<10:31,  2.54it/s, loss=0.0514]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 48400 - Loss: 0.0514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████         | 1100/2500 [07:12<09:11,  2.54it/s, loss=0.0631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 48600 - Loss: 0.0631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▎       | 1300/2500 [08:31<07:53,  2.54it/s, loss=0.0595]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 48800 - Loss: 0.0595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████████▌      | 1500/2500 [09:49<06:33,  2.54it/s, loss=0.0695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 49000 - Loss: 0.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████████▉     | 1700/2500 [11:08<05:14,  2.54it/s, loss=0.0693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 49200 - Loss: 0.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▏   | 1900/2500 [12:26<03:55,  2.54it/s, loss=0.0526]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 49400 - Loss: 0.0526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|█████████████▍  | 2100/2500 [13:45<02:36,  2.55it/s, loss=0.0594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 49600 - Loss: 0.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|██████████████▋ | 2300/2500 [15:04<01:18,  2.54it/s, loss=0.0793]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 49800 - Loss: 0.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50000 - Loss: 0.0602\n",
      "\n",
      "Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▎               | 200/2500 [01:18<15:03,  2.55it/s, loss=0.0548]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50200 - Loss: 0.0548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▉               | 400/2500 [02:37<13:46,  2.54it/s, loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50400 - Loss: 0.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████             | 600/2500 [03:55<12:25,  2.55it/s, loss=0.0678]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50600 - Loss: 0.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▍           | 800/2500 [05:14<11:08,  2.54it/s, loss=0.0579]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50800 - Loss: 0.0579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▍         | 1000/2500 [06:33<09:49,  2.55it/s, loss=0.0726]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 51000 - Loss: 0.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|███████▋        | 1200/2500 [07:51<08:31,  2.54it/s, loss=0.0598]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 51200 - Loss: 0.0598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|████████▉       | 1400/2500 [09:10<07:13,  2.54it/s, loss=0.0659]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 51400 - Loss: 0.0659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▏     | 1600/2500 [10:28<05:53,  2.55it/s, loss=0.0761]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 51600 - Loss: 0.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████████▌    | 1800/2500 [11:47<04:34,  2.55it/s, loss=0.0734]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 51800 - Loss: 0.0734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████████▊   | 2000/2500 [13:06<03:16,  2.54it/s, loss=0.0613]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 52000 - Loss: 0.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████  | 2200/2500 [14:24<01:57,  2.55it/s, loss=0.0619]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 52200 - Loss: 0.0619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████▎| 2400/2500 [15:43<00:39,  2.55it/s, loss=0.0516]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 52400 - Loss: 0.0516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                | 100/2500 [00:39<15:42,  2.55it/s, loss=0.0679]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 52600 - Loss: 0.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██               | 300/2500 [01:57<14:24,  2.54it/s, loss=0.0526]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 52800 - Loss: 0.0526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▍             | 500/2500 [03:16<13:04,  2.55it/s, loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 53000 - Loss: 0.0816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|████▊            | 700/2500 [04:35<11:45,  2.55it/s, loss=0.0829]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 53200 - Loss: 0.0829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████           | 900/2500 [05:53<10:31,  2.53it/s, loss=0.0552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 53400 - Loss: 0.0552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████         | 1100/2500 [07:12<09:10,  2.54it/s, loss=0.0551]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 53600 - Loss: 0.0551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▎       | 1300/2500 [08:31<07:52,  2.54it/s, loss=0.0517]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 53800 - Loss: 0.0517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████████▌      | 1500/2500 [09:49<06:35,  2.53it/s, loss=0.0621]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 54000 - Loss: 0.0621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████████▉     | 1700/2500 [11:08<05:15,  2.54it/s, loss=0.0831]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 54200 - Loss: 0.0831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▏   | 1900/2500 [12:26<03:55,  2.55it/s, loss=0.0577]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 54400 - Loss: 0.0577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|█████████████▍  | 2100/2500 [13:45<02:37,  2.55it/s, loss=0.0606]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 54600 - Loss: 0.0606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|██████████████▋ | 2300/2500 [15:04<01:18,  2.55it/s, loss=0.0472]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 54800 - Loss: 0.0472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 55000 - Loss: 0.0549\n",
      "\n",
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▎               | 200/2500 [01:18<15:05,  2.54it/s, loss=0.0637]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 55200 - Loss: 0.0637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▋              | 400/2500 [02:37<13:43,  2.55it/s, loss=0.0564]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 55400 - Loss: 0.0564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████             | 600/2500 [03:55<12:26,  2.55it/s, loss=0.0564]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 55600 - Loss: 0.0564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▍           | 800/2500 [05:14<11:07,  2.55it/s, loss=0.0852]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 55800 - Loss: 0.0852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▍         | 1000/2500 [06:32<09:47,  2.55it/s, loss=0.0529]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 56000 - Loss: 0.0529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|███████▋        | 1200/2500 [07:51<08:32,  2.53it/s, loss=0.0549]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 56200 - Loss: 0.0549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|████████▉       | 1400/2500 [09:09<07:13,  2.54it/s, loss=0.0522]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 56400 - Loss: 0.0522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▏     | 1600/2500 [10:28<05:53,  2.54it/s, loss=0.0673]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 56600 - Loss: 0.0673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████████▌    | 1800/2500 [11:47<04:35,  2.54it/s, loss=0.0801]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 56800 - Loss: 0.0801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████████▊   | 2000/2500 [13:05<03:17,  2.54it/s, loss=0.0613]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 57000 - Loss: 0.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████  | 2200/2500 [14:24<01:57,  2.54it/s, loss=0.0579]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 57200 - Loss: 0.0579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████▎| 2400/2500 [15:43<00:39,  2.55it/s, loss=0.0525]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 57400 - Loss: 0.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                | 100/2500 [00:39<15:44,  2.54it/s, loss=0.0519]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 57600 - Loss: 0.0519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██               | 300/2500 [01:58<14:26,  2.54it/s, loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 57800 - Loss: 0.0713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███▍             | 500/2500 [03:16<13:06,  2.54it/s, loss=0.0538]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 58000 - Loss: 0.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|████▊            | 700/2500 [04:35<11:46,  2.55it/s, loss=0.0575]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 58200 - Loss: 0.0575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|██████▍           | 900/2500 [05:54<10:29,  2.54it/s, loss=0.054]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 58400 - Loss: 0.0540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████         | 1100/2500 [07:12<09:10,  2.54it/s, loss=0.0721]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 58600 - Loss: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████▎       | 1300/2500 [08:31<07:51,  2.54it/s, loss=0.0587]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 58800 - Loss: 0.0587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████████▌      | 1500/2500 [09:50<06:32,  2.54it/s, loss=0.0458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 59000 - Loss: 0.0458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████████▉     | 1700/2500 [11:08<05:14,  2.54it/s, loss=0.0591]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 59200 - Loss: 0.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▉    | 1900/2500 [12:27<03:55,  2.55it/s, loss=0.045]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 59400 - Loss: 0.0450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|█████████████▍  | 2100/2500 [13:45<02:37,  2.54it/s, loss=0.0807]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 59600 - Loss: 0.0807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|██████████████▋ | 2300/2500 [15:04<01:18,  2.55it/s, loss=0.0508]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 59800 - Loss: 0.0508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60000 - Loss: 0.0520\n",
      "\n",
      "Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▎               | 200/2500 [01:18<15:04,  2.54it/s, loss=0.0517]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60200 - Loss: 0.0517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|██▋              | 400/2500 [02:37<13:45,  2.54it/s, loss=0.0554]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60400 - Loss: 0.0554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|████             | 600/2500 [03:55<12:25,  2.55it/s, loss=0.0642]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60600 - Loss: 0.0642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|█████▍           | 800/2500 [05:14<11:07,  2.55it/s, loss=0.0642]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60800 - Loss: 0.0642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▊          | 1000/2500 [06:32<09:50,  2.54it/s, loss=0.066]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 61000 - Loss: 0.0660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|███████▋        | 1200/2500 [07:51<08:30,  2.55it/s, loss=0.0527]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 61200 - Loss: 0.0527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|████████▉       | 1400/2500 [09:10<07:12,  2.55it/s, loss=0.0468]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 61400 - Loss: 0.0468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████████▏     | 1600/2500 [10:28<05:53,  2.54it/s, loss=0.0537]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 61600 - Loss: 0.0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████████▌    | 1800/2500 [11:47<04:35,  2.54it/s, loss=0.0758]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 61800 - Loss: 0.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████████▊   | 2000/2500 [13:05<03:16,  2.54it/s, loss=0.0787]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 62000 - Loss: 0.0787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████▉  | 2200/2500 [14:24<01:57,  2.54it/s, loss=0.051]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 62200 - Loss: 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████▎| 2400/2500 [15:42<00:39,  2.54it/s, loss=0.0483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 62400 - Loss: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_scheduler\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "from torch.optim import AdamW,SGD\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# Load and prepare SQuAD dataset\n",
    "# -------------------------------\n",
    "squad = load_dataset(\"squad\", split=\"train[:20000]\")  # Small subset for testing\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Initialize the fast tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to EOS\n",
    "\n",
    "def format_qa(example):\n",
    "    prompt = f\"Context: {example['context']}\\nQuestion: {example['question']}\\nAnswer:\"\n",
    "    answer = example['answers']['text'][0] + tokenizer.eos_token\n",
    "    return {\"input_text\": prompt, \"output_text\": answer}\n",
    "\n",
    "formatted = squad.map(format_qa)\n",
    "\n",
    "def tokenize(example):\n",
    "    input_enc = tokenizer(example['input_text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    output_enc = tokenizer(example['input_text'] + \" \" + example['output_text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    return {\n",
    "        \"input_ids\": input_enc[\"input_ids\"],\n",
    "        \"attention_mask\": input_enc[\"attention_mask\"],\n",
    "        \"labels\": output_enc[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "tokenized = formatted.map(tokenize)\n",
    "\n",
    "# -------------------------------\n",
    "# PyTorch Dataset\n",
    "# -------------------------------\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.data[idx][\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(self.data[idx][\"attention_mask\"]),\n",
    "            \"labels\": torch.tensor(self.data[idx][\"labels\"]),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "qa_dataset = QADataset(tokenized)\n",
    "\n",
    "# DataLoader with default collate_fn\n",
    "dataloader = DataLoader(qa_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Model and Optimizer Setup\n",
    "# -------------------------------\n",
    "#model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = CustomGPT2LMHeadModel(config)\n",
    "copy_weights(original_model, model)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if 'attn.q_proj' in n or 'attn.k_proj' in n], \"lr\": 4e-4},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if 'attn.q_proj' not in n and 'attn.k_proj' not in n], \"lr\": 5e-5}\n",
    "    ],\n",
    "    weight_decay=0.01,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epochs = 25\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Scheduler\n",
    "num_training_steps = len(dataloader) * epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop with Logging\n",
    "# -------------------------------\n",
    "logging_steps = 200\n",
    "global_step = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    loop = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for step, batch in enumerate(loop):\n",
    "        global_step += 1\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if global_step % logging_steps == 0:\n",
    "            print(f\"Step {global_step} - Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:41:42.577928Z",
     "iopub.status.busy": "2025-07-27T16:41:42.577633Z",
     "iopub.status.idle": "2025-07-27T16:41:42.581643Z",
     "shell.execute_reply": "2025-07-27T16:41:42.580833Z",
     "shell.execute_reply.started": "2025-07-27T16:41:42.577911Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:41:42.582723Z",
     "iopub.status.busy": "2025-07-27T16:41:42.582445Z",
     "iopub.status.idle": "2025-07-27T16:41:42.597443Z",
     "shell.execute_reply": "2025-07-27T16:41:42.596857Z",
     "shell.execute_reply.started": "2025-07-27T16:41:42.582701Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Lowercase, remove punctuation, and normalize whitespace.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def generate_outputs(model, tokenizer, input_ids, answer_ids, device):\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    generated = input_ids.clone()\n",
    "    all_attentions = []\n",
    "    true_token_probs = []\n",
    "\n",
    "    for step in range(len(answer_ids)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=generated, output_attentions=True)\n",
    "        logits = outputs.logits\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        true_token_id = answer_ids[step].item()\n",
    "        true_prob = probs[0, true_token_id].item()\n",
    "        true_token_probs.append(true_prob)\n",
    "\n",
    "        # Debug print (optional)\n",
    "        # print(f\"Step {step}: True token id = {true_token_id}, Prob = {true_prob:.8f}\")\n",
    "\n",
    "        next_token = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        # Save attention from last token (list of tensors: layers × [batch, heads, seq_len])\n",
    "        all_attentions.append([a[:, :, -1, :] for a in attentions])\n",
    "\n",
    "        generated = torch.cat([generated, next_token.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        if next_token.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode only the generated tokens AFTER the prompt\n",
    "    gen_tokens = generated[0, input_ids.shape[-1]:]\n",
    "    Final_output = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    return Final_output, all_attentions, true_token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:41:42.599243Z",
     "iopub.status.busy": "2025-07-27T16:41:42.598744Z",
     "iopub.status.idle": "2025-07-27T16:46:09.171979Z",
     "shell.execute_reply": "2025-07-27T16:46:09.171238Z",
     "shell.execute_reply.started": "2025-07-27T16:41:42.599225Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m      2\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m analysis \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tqdm(squad):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Main evaluation loop\n",
    "count = 0\n",
    "model.eval()\n",
    "analysis = []\n",
    "for example in tqdm(squad):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer_text = example[\"answers\"][\"text\"][0]\n",
    "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    # Encode prompt with offsets\n",
    "    encodings = tokenizer(prompt, return_offsets_mapping=True, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    offsets = encodings[\"offset_mapping\"][0].tolist()  # list of (start, end)\n",
    "\n",
    "    # Locate where context starts in prompt (to offset answer span properly)\n",
    "    context_start_in_prompt = prompt.find(context)\n",
    "    answer_start_char = example[\"answers\"][\"answer_start\"][0] + context_start_in_prompt\n",
    "    answer_end_char = answer_start_char + len(answer_text)\n",
    "\n",
    "    # Map character span of answer to token indices in the prompt\n",
    "    token_indices = [\n",
    "        i for i, (start, end) in enumerate(offsets)\n",
    "        if start < answer_end_char and end > answer_start_char\n",
    "    ]\n",
    "\n",
    "    # Encode ground-truth answer tokens WITH leading space because generated tokens usually include it\n",
    "    answer_ids = tokenizer.encode(\" \" + answer_text, add_special_tokens=False)\n",
    "    answer_ids = torch.tensor(answer_ids).to(device)\n",
    "\n",
    "    # Generate predicted answer + track true token probabilities\n",
    "    output, attns, true_token_probs = generate_outputs(model, tokenizer, input_ids, answer_ids, device)\n",
    "\n",
    "    # Normalize answers for fair comparison\n",
    "    norm_true = normalize_text(answer_text)\n",
    "    norm_pred = normalize_text(output)\n",
    "\n",
    "    # Average true token probability over all steps\n",
    "    avg_true_token_prob = sum(true_token_probs) / len(true_token_probs)\n",
    "\n",
    "    # Compute average attention score over heads and steps for true token indices\n",
    "    total_attention = 0.0\n",
    "    for step_attn in attns:\n",
    "        last_layer_attn = step_attn[1][0]  # last layer, batch 0: (num_heads, seq_len)\n",
    "        step_total = 0.0\n",
    "        for h in range(last_layer_attn.shape[0]):\n",
    "            head_attn = last_layer_attn[h]\n",
    "            step_total += sum(head_attn[j] for j in token_indices if j < head_attn.shape[0])\n",
    "        avg_step_attn = step_total / last_layer_attn.shape[0]  # average over heads\n",
    "        total_attention += avg_step_attn\n",
    "    true_token_attention_score = total_attention / len(attns)  # average over steps\n",
    "\n",
    "    # Print results\n",
    "    #print(\"=\" * 60)\n",
    "    #print(f\"Example #{count + 1}\")\n",
    "    #print(\"True Answer     :\", answer_text)\n",
    "    #print(\"Predicted Answer:\", output)\n",
    "    #print(\"Normalized True :\", norm_true)\n",
    "    #print(\"Normalized Pred :\", norm_pred)\n",
    "    #print(\"Avg True Token Prob: {:.6f}\".format(avg_true_token_prob))\n",
    "    #print(\"True Token Attention Score: {:.6f}\".format(true_token_attention_score.item()))\n",
    "\n",
    "    analysis.append([norm_true,norm_pred,avg_true_token_prob,true_token_attention_score.item()])\n",
    "\n",
    "    # count += 1\n",
    "    # if count >= 200:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:46:09.173128Z",
     "iopub.status.busy": "2025-07-27T16:46:09.172878Z",
     "iopub.status.idle": "2025-07-27T16:46:09.201571Z",
     "shell.execute_reply": "2025-07-27T16:46:09.201057Z",
     "shell.execute_reply.started": "2025-07-27T16:46:09.173087Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save to file\n",
    "with open(\"diff_train_qa_eval_results_1_l1.json\", \"w\") as f:\n",
    "    json.dump(analysis, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:46:09.202450Z",
     "iopub.status.busy": "2025-07-27T16:46:09.202230Z",
     "iopub.status.idle": "2025-07-27T16:46:09.229197Z",
     "shell.execute_reply": "2025-07-27T16:46:09.228647Z",
     "shell.execute_reply.started": "2025-07-27T16:46:09.202433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7650, 12350)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = np.array(analysis)[:,-1].astype(float)\n",
    "sum(tt<=0.05), sum(tt>0.05)\n",
    "#386 114 # 10 times  500 dp\n",
    "#325, 208# 100 times 500 dp\n",
    "#419, 81  500 dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:46:09.229996Z",
     "iopub.status.busy": "2025-07-27T16:46:09.229791Z",
     "iopub.status.idle": "2025-07-27T16:46:09.234760Z",
     "shell.execute_reply": "2025-07-27T16:46:09.234134Z",
     "shell.execute_reply.started": "2025-07-27T16:46:09.229980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.11294791, 0.04883548, 0.01880982, ..., 0.00853668, 0.06882869,\n",
       "        0.11168077]),\n",
       " 0.2566,\n",
       " 0.0298)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt,1283/5000,149/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:46:23.764424Z",
     "iopub.status.busy": "2025-07-27T16:46:23.764173Z",
     "iopub.status.idle": "2025-07-27T16:46:24.020628Z",
     "shell.execute_reply": "2025-07-27T16:46:24.019614Z",
     "shell.execute_reply.started": "2025-07-27T16:46:23.764408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "bin_edges = [np.array([0, 0.05, 0.1, 0.5, 1]), np.array([0, 0.05, 0.1, 0.5,  1])]\n",
    "\n",
    "num = 200\n",
    "\n",
    "hist,_,_= np.histogram2d( np.array(analysis)[:,-1].astype(float),  np.array(analysis)[:,-2].astype(float), bins=bin_edges)\n",
    "\n",
    "hist = hist/num\n",
    "# Assuming `mean_hist_norm` and `var_hist_norm` are your 2D arrays of the same shape\n",
    "# If you prefer to show std instead of variance in the annotation, convert variance to std:\n",
    "\n",
    "\n",
    "# Create annotation labels with both mean and std (formatted as strings)\n",
    "annot_array = np.empty_like(hist, dtype=object)\n",
    "\n",
    "\n",
    "for i in range(hist.shape[0]):\n",
    "    for j in range(hist.shape[1]):\n",
    "        annot_array[i, j] = f\"{hist[i, j]:.1f}\"#\\n(±{std_hist_norm[i, j]:.1f})\"  # mean ± std\n",
    "\n",
    "# # Define tick positions for edges of bins (for correct labeling)\n",
    "x_edges = np.array([0, 0.05, 0.1, 0.5, 1])\n",
    "y_edges = np.array([0, 0.05, 0.1, 0.5, 1])\n",
    "xtick_positions = np.arange(len(x_edges) - 1) + 1.0  # Right edges\n",
    "ytick_positions = np.arange(len(y_edges) - 1) + 1.0  # Top edges\n",
    "\n",
    "# Plot heatmap for mean values (color intensity)\n",
    "plt.figure(figsize=(6, 6))\n",
    "ax = sns.heatmap(hist.T, annot=annot_array.T, cmap=sns.color_palette(\"coolwarm\"), \n",
    "                  annot_kws={\"size\":18}, cbar=False, vmin=5, vmax=70,fmt=\"\")\n",
    "\n",
    "# Adjust tick positions for x and y axes (move them to the edges)\n",
    "ax.set_xticks(xtick_positions)\n",
    "ax.set_yticks(ytick_positions)\n",
    "\n",
    "# Set the labels for ticks (x and y edges)\n",
    "ax.set_xticklabels(x_edges[1:])\n",
    "ax.set_yticklabels(y_edges[1:])\n",
    "\n",
    "# # Make the tick labels bold\n",
    "ax.tick_params(axis='x', labelsize=14)  # Bold x-axis labels\n",
    "ax.tick_params(axis='y', labelsize=14)  # Bold y-axis labels\n",
    "\n",
    "\n",
    "# Invert y-axis to align with typical heatmap style\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(r\"Distinct Token Attention\", fontweight=\"bold\", fontsize=16)\n",
    "plt.ylabel(r\"Relevant Token Probability\", fontweight=\"bold\", fontsize=16)\n",
    "#plt.title(\"Mean (±Std Dev) Heatmap\")\n",
    "\n",
    "# # Save the figure\n",
    "plt.savefig(\"faster_qk_10_times_squad_train_l1.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:49:55.982762Z",
     "iopub.status.busy": "2025-07-27T16:49:55.982170Z",
     "iopub.status.idle": "2025-07-27T16:49:55.986376Z",
     "shell.execute_reply": "2025-07-27T16:49:55.985595Z",
     "shell.execute_reply.started": "2025-07-27T16:49:55.982740Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# def generate_answer(context, question):\n",
    "#     prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "#     output = model.generate(input_ids, max_new_tokens=10,  eos_token_id=tokenizer.eos_token_id,pad_token_id=tokenizer.eos_token_id)  # Avoid warning if pad token is undefined)\n",
    "#     answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#     return answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# count = 0 \n",
    "# # Test on training examples\n",
    "# for example in squad:\n",
    "#     print(\"Q:\", example['question'])\n",
    "#     print(\"Predicted A:\", generate_answer(example['context'], example['question']))\n",
    "#     print(\"True A:\", example['answers']['text'][0])\n",
    "#     print(\"=\"*60)\n",
    "#     count +=1\n",
    "#     if count>50:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:49:56.943020Z",
     "iopub.status.busy": "2025-07-27T16:49:56.942751Z",
     "iopub.status.idle": "2025-07-27T16:54:47.539988Z",
     "shell.execute_reply": "2025-07-27T16:54:47.539300Z",
     "shell.execute_reply.started": "2025-07-27T16:49:56.942999Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Normalize answer text for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.IGNORECASE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    \"\"\"Split text into tokens\"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact_match(a_gold, a_pred):\n",
    "    \"\"\"Compute exact match score\"\"\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    \"\"\"Compute F1 score\"\"\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    \n",
    "    if not gold_toks and not pred_toks:\n",
    "        return 1.0\n",
    "    \n",
    "    if not gold_toks or not pred_toks:\n",
    "        return 0.0\n",
    "    \n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = num_same / len(pred_toks)\n",
    "    recall = num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "\n",
    "def generate_answer(context, question):\n",
    "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "    output = model.generate(input_ids, max_new_tokens=10, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return answer.split(\"Answer:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:49:56.943020Z",
     "iopub.status.busy": "2025-07-27T16:49:56.942751Z",
     "iopub.status.idle": "2025-07-27T16:54:47.539988Z",
     "shell.execute_reply": "2025-07-27T16:54:47.539300Z",
     "shell.execute_reply.started": "2025-07-27T16:49:56.942999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on SQuAD examples...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/20000 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|█████████████████████████████████████| 20000/20000 [31:32<00:00, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Total Questions: 20000\n",
      "Exact Match Score: 57.14%\n",
      "F1 Score: 75.62%\n",
      "\n",
      "Detailed Statistics:\n",
      "Questions with EM = 1: 11428 (57.1%)\n",
      "Questions with F1 > 0.5: 15104 (75.5%)\n",
      "Average F1 for non-zero scores: 0.853\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Perfect Matches (EM=1): 11428 examples\n",
      "  1. Predicted: 'Saint Bernadette Soubirous' | True: 'Saint Bernadette Soubirous'\n",
      "  2. Predicted: 'the Main Building' | True: 'the Main Building'\n",
      "  3. Predicted: 'September 1876' | True: 'September 1876'\n",
      "\n",
      "Partial Matches (EM=0, F1>0.3): 5721 examples\n",
      "  1. Predicted: 'a copper statue of Christ with arms raised with the' | True: 'a copper statue of Christ' | F1: 0.667\n",
      "  2. Predicted: 'a Marian place of prayer and reflection. It is' | True: 'a Marian place of prayer and reflection' | F1: 0.857\n",
      "  3. Predicted: 'twice a year' | True: 'twice' | F1: 0.667\n",
      "\n",
      "No Matches (F1=0): 2277 examples\n",
      "  1. Predicted: '' | True: 'a golden statue of the Virgin Mary'\n",
      "  2. Predicted: 'the' | True: 'the Science Department'\n",
      "  3. Predicted: 'adaptation.\n",
      "Answer' | True: 'climate change'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics tracking\n",
    "count = 0\n",
    "exact_match_scores = []\n",
    "f1_scores = []\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "print(\"Evaluating model on SQuAD examples...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test on training examples\n",
    "for example in tqdm(squad):\n",
    "    # Generate prediction\n",
    "    predicted_answer = generate_answer(example['context'], example['question'])\n",
    "    true_answers = example['answers']['text']  # List of all valid answers\n",
    "    \n",
    "    # Take the best score across all possible answers (SQuAD style)\n",
    "    em_score = max(compute_exact_match(true_ans, predicted_answer) for true_ans in true_answers)\n",
    "    f1_score = max(compute_f1(true_ans, predicted_answer) for true_ans in true_answers)\n",
    "    \n",
    "    # Store scores\n",
    "    exact_match_scores.append(em_score)\n",
    "    f1_scores.append(f1_score)\n",
    "    predictions.append(predicted_answer)\n",
    "    ground_truths.append(true_answers[0])  # First answer for display\n",
    "    \n",
    "    # Display results\n",
    "    #print(f\"Question {count + 1}:\")\n",
    "    #print(f\"Q: {example['question']}\")\n",
    "    #print(f\"Predicted A: {predicted_answer}\")\n",
    "    #print(f\"True A: {true_answers[0]}\")\n",
    "    #print(f\"EM Score: {em_score} | F1 Score: {f1_score:.3f}\")\n",
    "    #print(\"=\" * 60)\n",
    "    \n",
    "    # count += 1\n",
    "    # if count > 5000:\n",
    "    #     break\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_em = sum(exact_match_scores) / len(exact_match_scores) * 100\n",
    "overall_f1 = sum(f1_scores) / len(f1_scores) * 100\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Questions: {len(exact_match_scores)}\")\n",
    "print(f\"Exact Match Score: {overall_em:.2f}%\")\n",
    "print(f\"F1 Score: {overall_f1:.2f}%\")\n",
    "\n",
    "# Additional statistics\n",
    "print(f\"\\nDetailed Statistics:\")\n",
    "print(f\"Questions with EM = 1: {sum(exact_match_scores)} ({sum(exact_match_scores)/len(exact_match_scores)*100:.1f}%)\")\n",
    "print(f\"Questions with F1 > 0.5: {sum(1 for f1 in f1_scores if f1 > 0.5)} ({sum(1 for f1 in f1_scores if f1 > 0.5)/len(f1_scores)*100:.1f}%)\")\n",
    "print(f\"Average F1 for non-zero scores: {sum(f1 for f1 in f1_scores if f1 > 0) / max(1, sum(1 for f1 in f1_scores if f1 > 0)):.3f}\")\n",
    "\n",
    "# Show some examples of different performance levels\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find examples with different performance levels\n",
    "perfect_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if em == 1]\n",
    "partial_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if em == 0 and f1 > 0.3]\n",
    "no_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if f1 == 0]\n",
    "\n",
    "if perfect_matches:\n",
    "    print(f\"\\nPerfect Matches (EM=1): {len(perfect_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(perfect_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}'\")\n",
    "\n",
    "if partial_matches:\n",
    "    print(f\"\\nPartial Matches (EM=0, F1>0.3): {len(partial_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(partial_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}' | F1: {f1_scores[idx]:.3f}\")\n",
    "\n",
    "if no_matches:\n",
    "    print(f\"\\nNo Matches (F1=0): {len(no_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(no_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}'\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./faster_saved_1/tokenizer_config.json',\n",
       " './faster_saved_1/special_tokens_map.json',\n",
       " './faster_saved_1/vocab.json',\n",
       " './faster_saved_1/merges.txt',\n",
       " './faster_saved_1/added_tokens.json',\n",
       " './faster_saved_1/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a directory to save\n",
    "save_path = \"./faster_saved_1\"\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "# Path to saved model\n",
    "save_path = \"./faster_saved_1\"\n",
    "\n",
    "# Load config\n",
    "config = GPT2Config.from_pretrained(save_path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(save_path)\n",
    "\n",
    "# Load custom model\n",
    "model = CustomGPT2LMHeadModel.from_pretrained(save_path, config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "squad_validation = load_dataset(\"squad\", split=\"train[20000:25000]\")  # Small subset for testing\n",
    "\n",
    "\n",
    "\n",
    "# formatted_validation = squad_validation.map(format_qa) # format_qa is a function\n",
    "\n",
    "\n",
    "# tokenized_validation = formatted_validation.map(tokenize) # tokenize is a function\n",
    "\n",
    "\n",
    "\n",
    "# qa_dataset_validation = QADataset(tokenized_validation)\n",
    "\n",
    "# # DataLoader with default collate_fn\n",
    "# dataloader_validation = DataLoader(qa_dataset_validation, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [04:36<00:00, 18.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Main evaluation loop\n",
    "count = 0\n",
    "model.eval()\n",
    "analysis_validation = []\n",
    "for example in tqdm(squad_validation):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer_text = example[\"answers\"][\"text\"][0]\n",
    "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    # Encode prompt with offsets\n",
    "    encodings = tokenizer(prompt, return_offsets_mapping=True, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    offsets = encodings[\"offset_mapping\"][0].tolist()  # list of (start, end)\n",
    "\n",
    "    # Locate where context starts in prompt (to offset answer span properly)\n",
    "    context_start_in_prompt = prompt.find(context)\n",
    "    answer_start_char = example[\"answers\"][\"answer_start\"][0] + context_start_in_prompt\n",
    "    answer_end_char = answer_start_char + len(answer_text)\n",
    "\n",
    "    # Map character span of answer to token indices in the prompt\n",
    "    token_indices = [\n",
    "        i for i, (start, end) in enumerate(offsets)\n",
    "        if start < answer_end_char and end > answer_start_char\n",
    "    ]\n",
    "\n",
    "    # Encode ground-truth answer tokens WITH leading space because generated tokens usually include it\n",
    "    answer_ids = tokenizer.encode(\" \" + answer_text, add_special_tokens=False)\n",
    "    answer_ids = torch.tensor(answer_ids).to(device)\n",
    "\n",
    "    # Generate predicted answer + track true token probabilities\n",
    "    output, attns, true_token_probs = generate_outputs(model, tokenizer, input_ids, answer_ids, device)\n",
    "\n",
    "    # Normalize answers for fair comparison\n",
    "    norm_true = normalize_text(answer_text)\n",
    "    norm_pred = normalize_text(output)\n",
    "\n",
    "    # Average true token probability over all steps\n",
    "    avg_true_token_prob = sum(true_token_probs) / len(true_token_probs)\n",
    "\n",
    "    # Compute average attention score over heads and steps for true token indices\n",
    "    total_attention = 0.0\n",
    "    for step_attn in attns:\n",
    "        last_layer_attn = step_attn[1][0]  # last layer, batch 0: (num_heads, seq_len)\n",
    "        step_total = 0.0\n",
    "        for h in range(last_layer_attn.shape[0]):\n",
    "            head_attn = last_layer_attn[h]\n",
    "            step_total += sum(head_attn[j] for j in token_indices if j < head_attn.shape[0])\n",
    "        avg_step_attn = step_total / last_layer_attn.shape[0]  # average over heads\n",
    "        total_attention += avg_step_attn\n",
    "    true_token_attention_score = total_attention / len(attns)  # average over steps\n",
    "\n",
    "    # Print results\n",
    "    #print(\"=\" * 60)\n",
    "    #print(f\"Example #{count + 1}\")\n",
    "    #print(\"True Answer     :\", answer_text)\n",
    "    #print(\"Predicted Answer:\", output)\n",
    "    #print(\"Normalized True :\", norm_true)\n",
    "    #print(\"Normalized Pred :\", norm_pred)\n",
    "    #print(\"Avg True Token Prob: {:.6f}\".format(avg_true_token_prob))\n",
    "    #print(\"True Token Attention Score: {:.6f}\".format(true_token_attention_score.item()))\n",
    "\n",
    "    analysis_validation.append([norm_true,norm_pred,avg_true_token_prob,true_token_attention_score.item()])\n",
    "\n",
    "    # count += 1\n",
    "    # if count >= 200:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['taongi', 'san bartol', 0.0008148198340971717, 0.015021972358226776],\n",
       " ['álvaro de saavedra cerón',\n",
       "  'álvaro de saavedra cerón',\n",
       "  0.8262106943875551,\n",
       "  0.16219961643218994],\n",
       " ['the maluku islands',\n",
       "  'maluku islands',\n",
       "  0.07137719295750422,\n",
       "  0.09300446510314941],\n",
       " ['los pintados', 'los pintados', 0.9775886088609695, 0.08572913706302643],\n",
       " ['los jardines', 'los pintados', 0.28674430163350056, 0.025672970339655876],\n",
       " ['enewetak or bikini atoll',\n",
       "  'enewetak or bikini atoll',\n",
       "  0.8905504941940308,\n",
       "  0.1292472928762436],\n",
       " ['álvaro de saavedra cerón',\n",
       "  'álvaro de saavedra cerón',\n",
       "  0.8933156317099928,\n",
       "  0.16157497465610504],\n",
       " ['the maluku islands',\n",
       "  'the pacific islands',\n",
       "  0.2135789506349471,\n",
       "  0.03604326397180557],\n",
       " ['los pintados', 'los pintados', 0.9680976122617722, 0.08604539930820465],\n",
       " ['los jardines', 'los jardines', 0.979377880692482, 0.07243435084819794],\n",
       " ['eight days', 'eight days', 0.9888300895690918, 0.05582287907600403],\n",
       " ['los barbudos', 'los barbés', 0.6333535675269862, 0.05965568125247955],\n",
       " ['mejit', 'los barb', 8.412803881967246e-07, 0.006106609012931585],\n",
       " ['january 10', 'january 10', 0.9480263888835907, 0.020780859515070915],\n",
       " ['corrales', 'corral', 0.0004843630672439758, 0.039180487394332886],\n",
       " ['10°n', 'paj', 0.010175146500568258, 0.004589402116835117],\n",
       " ['miguel lópez de legazpi',\n",
       "  'miguel lópez de legazón',\n",
       "  0.7383253953885287,\n",
       "  0.13034412264823914],\n",
       " ['los barbudos', 'los barbés', 0.6453258143737912, 0.056863561272621155],\n",
       " ['placeres', 'los barb', 0.0028186663934093303, 0.003130754455924034],\n",
       " ['january 12', 'january 12', 0.8736946284770966, 0.02317602001130581],\n",
       " ['ujelang', 'los barbés', 4.0514486064086724e-06, 0.00876106508076191],\n",
       " ['captain john charles marshall and thomas gilbert',\n",
       "  'captain john charles marshall and thomas gilbert',\n",
       "  0.9203365359987531,\n",
       "  0.08554472029209137],\n",
       " ['jolet jen anij', 'jolet jen anij', 0.8500755205750465, 0.07131397724151611],\n",
       " ['adam johann von krusenstern',\n",
       "  'adam johann von krusenstern',\n",
       "  0.9848155304789543,\n",
       "  0.1075567901134491],\n",
       " ['louis isidore duperrey',\n",
       "  'louis isidore duperrey',\n",
       "  0.9847404105322701,\n",
       "  0.11266651004552841],\n",
       " ['mulgrave', 'mulgrave', 0.9995910227298737, 0.06870631873607635],\n",
       " ['thomas gilbert', 'thomas gilbert', 0.9790356457233429, 0.06579547375440598],\n",
       " ['1788', '1788', 0.9830860197544098, 0.05901888757944107],\n",
       " ['jolet jen anij', 'jolet jen anij', 0.8635690857966741, 0.07070319354534149],\n",
       " ['gifts from god', 'jolet j', 2.515260465007878e-07, 0.008615104481577873],\n",
       " ['mulgrave island',\n",
       "  'mulgrave island',\n",
       "  0.9667219320933024,\n",
       "  0.09200071543455124],\n",
       " ['captain donsette', 'donsette', 0.07140602420493103, 0.09497390687465668],\n",
       " ['naiad', 'naiad', 0.9907092452049255, 0.06068526953458786],\n",
       " ['glencoe and sea nymph',\n",
       "  'the san franciscobased ships',\n",
       "  0.0397284789179501,\n",
       "  0.014152619987726212],\n",
       " ['1857', '1857', 0.9636666476726532, 0.028461625799536705],\n",
       " ['ebon', 'ebon', 0.9944234192371368, 0.05885837972164154],\n",
       " ['captain donsette', 'naiad', 0.015364349863816074, 0.015001513995230198],\n",
       " ['1845', '18', 0.5199025236070156, 0.022957656532526016],\n",
       " ['schooner', 'schooner', 0.9990867773691813, 0.08371593058109283],\n",
       " ['san francisco', 'san francisco', 0.9998749494552612, 0.06303606927394867],\n",
       " ['ebon', 'ebon', 0.7235479056835175, 0.0632736012339592],\n",
       " ['the spanish empire',\n",
       "  'spanish empire',\n",
       "  0.0015291774262777835,\n",
       "  0.07691343128681183],\n",
       " ['the german empire', 'britain', 0.02338207494267408, 0.006708872504532337],\n",
       " ['britain', 'britain', 0.8693088889122009, 0.00145656056702137],\n",
       " ['sms nautilus', 'sms nautilus', 0.9932767897844315, 0.104313924908638],\n",
       " ['king of the ralik islands',\n",
       "  'king of the ralik islands',\n",
       "  0.9875058446611676,\n",
       "  0.0854426771402359],\n",
       " ['november 1', 'november 1', 0.967897355556488, 0.051795121282339096],\n",
       " ['imperial german protectorate',\n",
       "  'royal overseas german',\n",
       "  0.09797867946533242,\n",
       "  0.013980476185679436],\n",
       " ['spain', 'spain', 0.9910123944282532, 0.0022050547413527966],\n",
       " ['copra', 'copra', 0.8845614492893219, 0.05926569178700447],\n",
       " ['1884', '1884', 0.9958131015300751, 0.06336650252342224],\n",
       " ['the jaluit gesellschaft',\n",
       "  'the jumans',\n",
       "  0.1983134408130404,\n",
       "  0.05562113597989082],\n",
       " ['1905', '1905', 0.9993867874145508, 0.005345980636775494],\n",
       " ['the german–spanish treaty of 1899',\n",
       "  'the german–spanish treaty of 1899',\n",
       "  0.75898996421269,\n",
       "  0.08942858129739761],\n",
       " ['the carolines',\n",
       "  'the marshallalls',\n",
       "  0.3435727227587127,\n",
       "  0.013893535360693932],\n",
       " ['the governor of german new guinea',\n",
       "  'the german gesellschaft',\n",
       "  0.2052186416719158,\n",
       "  0.04858425259590149],\n",
       " ['the sacred heart jesu society',\n",
       "  'the holy father of the jes',\n",
       "  0.20086543885433153,\n",
       "  0.03416507691144943],\n",
       " ['1914', '1904', 0.0064207459799945354, 0.005789646878838539],\n",
       " ['1911', '1911', 0.4495868980884552, 0.007000529672950506],\n",
       " ['1912', '1914', 0.0200860146433115, 0.008952917531132698],\n",
       " ['1904', '1904', 0.9676938652992249, 0.006206630729138851],\n",
       " ['the meiji restoration',\n",
       "  'meiji restoration',\n",
       "  0.010631860813267904,\n",
       "  0.1136196106672287],\n",
       " ['east asia', 'east asia', 0.9982189536094666, 0.06040361151099205],\n",
       " ['traders', 'japanese', 0.0003060096933040768, 0.0037658128421753645],\n",
       " ['september 29 1914',\n",
       "  'september 29 1914',\n",
       "  0.8864802569150925,\n",
       "  0.05184132978320122],\n",
       " ['the entente', 'the entente', 0.7999189694722494, 0.04964599013328552],\n",
       " ['the jaluit atoll',\n",
       "  'the atewak at',\n",
       "  0.18407035768226834,\n",
       "  0.024844137951731682],\n",
       " ['june 28 1919', 'june 28 1919', 0.993897333741188, 0.06825161725282669],\n",
       " ['december 17 1920',\n",
       "  'december 17 1920',\n",
       "  0.9805878698825836,\n",
       "  0.0672953650355339],\n",
       " ['1000', '1000', 0.9951947132746378, 0.05826353654265404],\n",
       " ['economic', 'german', 0.027667585760354996, 0.005820771213620901],\n",
       " ['land', 'german', 0.0048829857259988785, 0.001948056509718299],\n",
       " ['palau', 'palau', 0.9977746307849884, 0.05814270302653313],\n",
       " ['matrilineality', 'matrilineal', 0.8255565091967583, 0.10087382793426514],\n",
       " ['the japanese patriarchal system',\n",
       "  'the japanese patriarchal system',\n",
       "  0.678825956583023,\n",
       "  0.05952296406030655],\n",
       " ['march 27 1933', 'march 27 1933', 0.9852082878351212, 0.06108506768941879],\n",
       " ['japanese', 'japanese', 0.9994940757751465, 0.0012433934025466442],\n",
       " ['catholic', 'catholic', 0.9723492860794067, 0.001056942972354591],\n",
       " ['6th fleet', '6th fleet', 0.9356728196144104, 0.10571460425853729],\n",
       " ['kwajalein',\n",
       "  'the marshall islands',\n",
       "  1.2114631841556207e-07,\n",
       "  0.02905709482729435],\n",
       " ['defense of the marshall islands',\n",
       "  'defense of the marshall islands',\n",
       "  0.8664675951004028,\n",
       "  0.0984065905213356],\n",
       " ['world war ii', 'world war ii', 0.9879629611968994, 0.09064579010009766],\n",
       " ['the gilbert and marshall islands campaign',\n",
       "  'gilbert and marshall islands campaign',\n",
       "  0.00059174680731644,\n",
       "  0.10651447623968124],\n",
       " ['1944', '1944', 0.9995673298835754, 0.006248427089303732],\n",
       " ['one', 'two', 4.819927562493831e-05, 0.00389605900272727],\n",
       " ['wotje', 'wotje', 0.9928650856018066, 0.08525578677654266],\n",
       " ['1943', '1943', 0.7417606711387634, 0.009892760775983334],\n",
       " ['5100', '5100', 0.9948873718579611, 0.06581709533929825],\n",
       " ['half', 'half', 0.57614666223526, 0.004560099449008703],\n",
       " ['lack of food', 'casualties', 0.0011872757841047132, 0.009070935659110546],\n",
       " ['world war ii', 'world war ii', 0.9919069210688273, 0.0968974381685257],\n",
       " ['security council resolution 21',\n",
       "  'united nations resolution 21',\n",
       "  0.39292472424767766,\n",
       "  0.05446210503578186],\n",
       " ['1947', '1947', 0.9976630210876465, 0.008893814869225025],\n",
       " ['micronesia',\n",
       "  'the pacific islands',\n",
       "  0.015302363165911492,\n",
       "  0.014349385164678097],\n",
       " ['1946', '1946', 0.9872996211051941, 0.010436581447720528],\n",
       " ['67', '67', 0.9981108903884888, 0.004648618865758181],\n",
       " ['castle bravo', 'castle bravo', 0.9972549378871918, 0.061380207538604736],\n",
       " ['108496', 'over 7', 0.004436498354596805, 0.005139842163771391],\n",
       " ['elugelab', 'elugelab', 0.9872884601354599, 0.08664023131132126],\n",
       " ['project 41', '41', 0.001407187429268486, 0.07054759562015533],\n",
       " ['1956', '1956', 0.9871672987937927, 0.005355797708034515],\n",
       " ['759', '', 0.004641372823687817, 0.004606421105563641],\n",
       " ['the compact of free association',\n",
       "  'the united states marshall islands',\n",
       "  0.30779082000848446,\n",
       "  0.036771539598703384],\n",
       " ['1986', '1986', 0.999060332775116, 0.0065798647701740265],\n",
       " ['kwajalein atoll', 'atoll', 0.019325941176996746, 0.11157756298780441],\n",
       " ['1990', '1990', 0.9967398047447205, 0.0057102711871266365],\n",
       " ['security council resolution 683',\n",
       "  'security council resolution 683',\n",
       "  0.9593056201934814,\n",
       "  0.09838353842496872],\n",
       " ['majuro', 'majuro', 0.9995793700218201, 0.060752496123313904],\n",
       " ['christmas', 'christmas', 0.9945194125175476, 0.003023360623046756],\n",
       " ['091', '091', 0.8606875141461691, 0.021343909204006195],\n",
       " ['extreme waves and high tides',\n",
       "  'extreme waves and high tide',\n",
       "  0.7183049362152815,\n",
       "  0.10670286417007446],\n",
       " ['2013', '2013', 0.9954859614372253, 0.00833443459123373],\n",
       " ['the northern atolls',\n",
       "  'the marshall islands',\n",
       "  0.1973702389375563,\n",
       "  0.028445981442928314],\n",
       " ['6000', '6000', 0.9986125032107035, 0.041667383164167404],\n",
       " ['1', '0', 0.043050069361925125, 0.0038618508260697126],\n",
       " ['diarrhea', 'fever', 0.1939956545829773, 0.005327329970896244],\n",
       " ['the united states president',\n",
       "  'president',\n",
       "  0.06747409779927693,\n",
       "  0.07457247376441956],\n",
       " ['tony de brum', 'tony de brum', 0.9899996072053909, 0.09896251559257507],\n",
       " ['september 2013', 'september 2013', 0.9471583962440491, 0.05904564633965492],\n",
       " ['majuro declaration for climate leadership',\n",
       "  'majuro declaration for climate leadership',\n",
       "  0.9337484439214071,\n",
       "  0.10913660377264023],\n",
       " ['climate change', 'climate change', 0.9979915022850037, 0.02729201689362526],\n",
       " ['the obama administration',\n",
       "  'the administration in',\n",
       "  0.3344718182925135,\n",
       "  0.05103544890880585],\n",
       " ['parliamentarypresidential system',\n",
       "  'mixed parliamentarypresidential',\n",
       "  0.09849814125939088,\n",
       "  0.07252086699008942],\n",
       " ['18', '18', 0.9999990463256836, 0.0017404952086508274],\n",
       " ['every four years',\n",
       "  'every four years',\n",
       "  0.9526742100715637,\n",
       "  0.06084814667701721],\n",
       " ['nitijela', 'majuro', 0.13227840348054087, 0.006730224937200546],\n",
       " ['1979', '1979', 0.9976281523704529, 0.008965007960796356],\n",
       " ['the council of iroij',\n",
       "  'council of the nitij',\n",
       "  0.2503538274228049,\n",
       "  0.04197706654667854],\n",
       " ['twelve tribal chiefs',\n",
       "  'the assembly of',\n",
       "  1.5120872821711186e-07,\n",
       "  0.0099102221429348],\n",
       " ['ten', 'ten', 0.8312516212463379, 0.0013640208635479212],\n",
       " ['the aka', 'udp', 4.1610133032863456e-05, 0.012474648654460907],\n",
       " ['twentyfour', 'four', 8.353111070391606e-06, 0.011767434887588024],\n",
       " ['compact of free association',\n",
       "  'the compact of free',\n",
       "  0.03946940820821965,\n",
       "  0.07452250272035599],\n",
       " ['the us', 'the united states', 0.2190117862344323, 0.034040287137031555],\n",
       " ['technical aid', 'technical aid', 0.7350998073816299, 0.07405748218297958],\n",
       " ['august 9 1991', 'august 9 1991', 0.9391448646783829, 0.07969726622104645],\n",
       " ['september 17 1991',\n",
       "  'september 17 1991',\n",
       "  0.9947002530097961,\n",
       "  0.08274747431278229],\n",
       " ['the united states',\n",
       "  'united states',\n",
       "  0.07955239002088395,\n",
       "  0.057333290576934814],\n",
       " ['resolution 704', 'resolution 704', 0.99014812707901, 0.07710596919059753],\n",
       " ['28 april 2015', '28 april 2015', 0.9895390868186951, 0.07098053395748138],\n",
       " ['mv maersk tigris',\n",
       "  'the marshall islandflagged',\n",
       "  0.0021984150080049153,\n",
       "  0.017739132046699524],\n",
       " ['rickmers ship management',\n",
       "  'germanys rickmers',\n",
       "  0.005777720316984869,\n",
       "  0.03643874078989029],\n",
       " ['34', '34', 0.9443679451942444, 0.001825985498726368],\n",
       " ['the strait of hormuz',\n",
       "  'the strait of hormuz',\n",
       "  0.9074831406275431,\n",
       "  0.08106673508882523],\n",
       " ['the federated states of micronesia',\n",
       "  'theatoll',\n",
       "  0.20056385469849314,\n",
       "  0.022155141457915306],\n",
       " ['wake island', 'wake island', 0.8702702224254608, 0.06894169747829437],\n",
       " ['1900000', 'about 750000 square', 0.01873230060805726, 0.027662131935358047],\n",
       " ['180', 'about', 0.002084556734189391, 0.003021828830242157],\n",
       " ['29', 'five', 0.020336834713816643, 0.006149563472718],\n",
       " ['772000', '2000 square', 0.00022411383453402323, 0.012859047390520573],\n",
       " ['october 2011', 'october 2011', 0.8479854166507721, 0.058593958616256714],\n",
       " ['1776000', '2000000', 0.5123658094884377, 0.02729085646569729],\n",
       " ['shark fishing', 'all fishing', 0.38600766657964414, 0.007047001272439957],\n",
       " ['wake island', 'wake island', 0.9899092018604279, 0.05949268490076065],\n",
       " ['the united states',\n",
       "  'the united states',\n",
       "  0.9305842916170756,\n",
       "  0.08363258838653564],\n",
       " ['1899', '1899', 0.9897847771644592, 0.0136493481695652],\n",
       " ['enenkio', 'enenkio', 0.9127281785011292, 0.12731614708900452],\n",
       " ['may', 'may', 0.4467255473136902, 0.006077527068555355],\n",
       " ['november', 'november', 0.9994373917579651, 0.005779785104095936],\n",
       " ['tropical storms',\n",
       "  'tropical storms',\n",
       "  0.9979487657546997,\n",
       "  0.08087549358606339],\n",
       " ['the philippines',\n",
       "  'the philippines',\n",
       "  0.696284219622612,\n",
       "  0.024821940809488297],\n",
       " ['its very low elevation',\n",
       "  'sea level rise',\n",
       "  3.500206153439489e-05,\n",
       "  0.03616822510957718],\n",
       " ['the president of nauru',\n",
       "  'the president of nauru',\n",
       "  0.9417316913604736,\n",
       "  0.09628452360630035],\n",
       " ['flooding from climate change',\n",
       "  'because of sea level',\n",
       "  0.013512562346079449,\n",
       "  0.03757413476705551],\n",
       " ['1300', '1300', 0.9545284907023112, 0.026141654700040817],\n",
       " ['rainfall', 'rainfall', 0.9997639060020447, 0.004967215470969677],\n",
       " ['drought', 'drought', 0.9998548030853271, 0.006708735600113869],\n",
       " ['twice', '50', 0.14844828844070435, 0.004951330833137035],\n",
       " ['the international labour organization',\n",
       "  'international labour organization',\n",
       "  0.0034838241272439063,\n",
       "  0.12773944437503815],\n",
       " ['its labour laws will comply with international benchmarks',\n",
       "  'it has influenced what in the management of',\n",
       "  0.04124585140927107,\n",
       "  0.08248884230852127],\n",
       " ['business conditions',\n",
       "  'business conditions',\n",
       "  0.6020213812589645,\n",
       "  0.08296529948711395],\n",
       " ['us577 million', '627 million', 0.02600140380857378, 0.04158521816134453],\n",
       " ['us627 million', '627 million', 0.03854081105514714, 0.06594475358724594],\n",
       " ['the amended compact of free association',\n",
       "  'the amended compact of free association',\n",
       "  0.8825063705444336,\n",
       "  0.09149875491857529],\n",
       " ['a trust fund', 'a trust fund', 0.937920351823171, 0.0706053078174591],\n",
       " ['kwajalein atoll',\n",
       "  'kwajalein atoll',\n",
       "  0.9804668525854746,\n",
       "  0.16624924540519714],\n",
       " ['ronald reagan', 'ronald reagan', 0.9997821152210236, 0.11102373898029327],\n",
       " ['ronald reagan ballistic missile defense test site',\n",
       "  'ronald reagan',\n",
       "  0.7305080691973368,\n",
       "  0.19140207767486572],\n",
       " ['united states army',\n",
       "  'united states army',\n",
       "  0.8895255327224731,\n",
       "  0.12826791405677795],\n",
       " ['breadfruit', 'cocon', 3.439934448401516e-10, 0.016914956271648407],\n",
       " ['small farms', 'small farms', 0.9282492399215698, 0.08347968757152557],\n",
       " ['1999', '1999', 0.991081714630127, 0.003619424533098936],\n",
       " ['400', '400', 0.8893405795097351, 0.003733475459739566],\n",
       " ['2005', '2005', 0.9992982149124146, 0.0027882985305041075],\n",
       " ['tuna steaks', 'tuna steaks', 0.7767146031061808, 0.06879717111587524],\n",
       " ['2 million', '2 million', 0.975097139676412, 0.07506557554006577],\n",
       " ['the tobolar copra processing plant',\n",
       "  'tobolar',\n",
       "  0.06874928369757072,\n",
       "  0.10898509621620178],\n",
       " ['majuro', 'majuro', 0.9568196833133698, 0.07049422711133957],\n",
       " ['the meat of the coconut',\n",
       "  'the meat of the palm',\n",
       "  0.62323217689991,\n",
       "  0.03126136213541031],\n",
       " ['6 to 10', '1 liter', 0.004219174408717663, 0.003536033909767866],\n",
       " ['57', '450', 0.0013024969957768917, 0.001073200604878366],\n",
       " ['10000', '10000', 0.9949633479118347, 0.04450640082359314],\n",
       " ['15000', '10000', 0.6630528057770183, 0.006864191964268684],\n",
       " ['72191', 'over two', 0.002297489054421865, 0.004960039630532265],\n",
       " ['kwajalein atoll', 'majuro', 7.011380887433916e-06, 0.013806447386741638],\n",
       " ['springdale arkansas',\n",
       "  'springdale',\n",
       "  0.6475769030318285,\n",
       "  0.08942179381847382],\n",
       " ['marshallese', 'micrones', 0.00041226158383622646, 0.01320075336843729],\n",
       " ['asia', 'asia', 0.9982277750968933, 0.00257730670273304],\n",
       " ['onehalf', 'onethird', 0.48785378207200364, 0.03251297399401665],\n",
       " ['japanese', 'some', 0.05801128223538399, 0.0023970971815288067],\n",
       " ['several thousand years ago',\n",
       "  'several thousand years ago',\n",
       "  0.9449260383844376,\n",
       "  0.09643125534057617],\n",
       " ['congregational', 'congregational', 0.9920298258463541, 0.1060369461774826],\n",
       " ['242', '242', 0.9455097019672394, 0.0466594323515892],\n",
       " ['83', '242', 0.47930981127137784, 0.008000367321074009],\n",
       " ['22', '242', 0.6829170350201821, 0.022700099274516106],\n",
       " ['september 2012', 'september 2012', 0.8683177530765533, 0.06015101075172424],\n",
       " ['ministry of education marshall islands',\n",
       "  'ministry of education',\n",
       "  0.7380590466782451,\n",
       "  0.14683546125888824],\n",
       " ['college of the marshall islands',\n",
       "  'the marshall islands college of',\n",
       "  0.01998533366034465,\n",
       "  0.07308920472860336],\n",
       " ['majuro', 'majuro', 0.9975181519985199, 0.087823286652565],\n",
       " ['bucholz army airfield',\n",
       "  'majuro airport answer',\n",
       "  0.004441071984228573,\n",
       "  0.048771634697914124],\n",
       " ['noble class', 'noble class', 0.8194966912269592, 0.05423356965184212],\n",
       " ['king casimir iii the great',\n",
       "  'king of poland',\n",
       "  0.31101306149804464,\n",
       "  0.041125498712062836],\n",
       " ['grand duchy of lithuania and the crown kingdom of poland',\n",
       "  'the grand duchy of lithuania and the crown of poland',\n",
       "  0.15932783630173494,\n",
       "  0.09028039872646332],\n",
       " ['1569–1795', '1569–1795', 0.9084467172622681, 0.0783187672495842],\n",
       " ['ducal prussia', 'duchy of', 0.0015919767463594553, 0.024355601519346237],\n",
       " ['manor farms', 'manor farms', 0.9437650839487711, 0.062365904450416565],\n",
       " ['obscurity and mystery',\n",
       "  'shrouded in obscurity',\n",
       "  0.0010506728916273762,\n",
       "  0.014596889726817608],\n",
       " ['late 18th century',\n",
       "  'the 18th century',\n",
       "  0.7666752599179745,\n",
       "  0.08837288618087769],\n",
       " ['political and legal privileges',\n",
       "  'in the 18th',\n",
       "  2.3631207189897432e-05,\n",
       "  0.02018967643380165],\n",
       " ['1772 to 1795',\n",
       "  'during the partitions of',\n",
       "  0.06295147605095443,\n",
       "  0.02761853300035],\n",
       " ['russian empire', 'the polish', 0.001946643931660219, 0.0048288749530911446],\n",
       " ['1921', '1921', 0.6253737211227417, 0.019539333879947662],\n",
       " ['legal privileges of the szlachta were legally abolished',\n",
       "  'abolished legal privileges of the nobility in the second polish republic',\n",
       "  0.0008232596744968648,\n",
       "  0.08528833836317062],\n",
       " ['second polish republic',\n",
       "  'second polish republic',\n",
       "  0.9354351758956909,\n",
       "  0.08524671196937561],\n",
       " ['social equals', 'all polish', 0.016050364995862765, 0.03154050558805466],\n",
       " ['polish saying',\n",
       "  'traditional thinking',\n",
       "  0.054734770524191845,\n",
       "  0.0325830802321434],\n",
       " ['regardless of their financial status',\n",
       "  'all of them',\n",
       "  0.030676586891786428,\n",
       "  0.0707627683877945],\n",
       " ['slahta', 'old high german', 0.03483368214182233, 0.010164125822484493],\n",
       " ['noble family', 'noble family of', 0.4146065520488946, 0.07511425018310547],\n",
       " ['rycerz', 'ritter', 0.14310813617904236, 0.009737018495798111],\n",
       " ['ritter', 'slah', 8.289969244401618e-07, 0.011297348886728287],\n",
       " ['erbe', 'old high', 3.609250070722714e-07, 0.014129696413874626],\n",
       " ['german schlachten',\n",
       "  'german schlachten',\n",
       "  0.8761028817721775,\n",
       "  0.09432473033666611],\n",
       " ['to slaughter or to butcher',\n",
       "  'battle answer polish',\n",
       "  0.009142638973178624,\n",
       "  0.03950835019350052],\n",
       " ['schlacht', 'schl', 9.821982018820563e-07, 0.014025149866938591],\n",
       " ['battle', 'battle', 0.9519528746604919, 0.005896201357245445],\n",
       " ['legendary protopolish chief lech',\n",
       "  'german chief',\n",
       "  0.0009530120962359945,\n",
       "  0.07780690491199493],\n",
       " ['magnates', 'magn', 0.0003088160592596978, 0.02177993580698967],\n",
       " ['magnat', 'magn', 0.0005979076086077839, 0.017504116520285606],\n",
       " ['możny', 'magnatci', 0.002177564972344923, 0.021475350484251976],\n",
       " ['możni', 'molzny', 0.0025090669539622468, 0.029717903584241867],\n",
       " ['lithuania', 'poland', 0.031455881893634796, 0.007123323157429695],\n",
       " ['szlachta', 'szachta', 0.7435469590127468, 0.05397283285856247],\n",
       " ['formalized hereditary',\n",
       "  's and',\n",
       "  0.00033076746554353345,\n",
       "  0.01773891970515251],\n",
       " ['hereditary szlachta',\n",
       "  'english nobility',\n",
       "  0.012515831603810649,\n",
       "  0.03162883594632149],\n",
       " ['equivalent in legal status of the english nobility',\n",
       "  'nobility',\n",
       "  0.004227204364724457,\n",
       "  0.10936488211154938],\n",
       " ['nobility', 'nobility', 0.4114130735397339, 0.007809793576598167],\n",
       " ['szlachta', 'szlachta', 0.8362348675727844, 0.05170736834406853],\n",
       " ['by courtesy or error',\n",
       "  'they owned by birth',\n",
       "  0.000295608950971582,\n",
       "  0.024851743131875992],\n",
       " ['oldcommonwealth nobility',\n",
       "  'old commonwealth nobility',\n",
       "  0.15249495304560776,\n",
       "  0.11844320595264435],\n",
       " ['included those almost rich and powerful enough to be magnates down to rascals',\n",
       "  'due to certain inaccuracies it is because of the economic status of some s',\n",
       "  2.4522456592433397e-05,\n",
       "  0.05160007253289223],\n",
       " ['szlachta', 'nobility answer', 2.944676002696975e-07, 0.01029839925467968],\n",
       " ['hereditary juridical status',\n",
       "  'inherited status answer',\n",
       "  0.040883459437975514,\n",
       "  0.0656009092926979],\n",
       " ['to become tenants of the wealthier gentry',\n",
       "  'become tenants of answer the affluent',\n",
       "  5.223116866449747e-06,\n",
       "  0.06481992453336716],\n",
       " ['no', 'all', 0.026519043371081352, 0.007725744508206844],\n",
       " ['retained all their constitutional prerogatives',\n",
       "  'retain their own rights answer',\n",
       "  0.008994632518145305,\n",
       "  0.051929961889982224],\n",
       " ['obscure', 'considered', 0.09256472438573837, 0.006669569760560989],\n",
       " ['odwieczna', 'odwiecece', 0.5110045154462568, 0.07466401904821396],\n",
       " ['descent from the ancient iranian tribes known as sarmatians',\n",
       "  'descent from the ancient iranian tribes known as answer',\n",
       "  0.5232891914591468,\n",
       "  0.06312894821166992],\n",
       " ['alexander the great',\n",
       "  'alexander the great',\n",
       "  0.9560121695200602,\n",
       "  0.04866669327020645],\n",
       " ['had not mixed their bloodlines with those of slaves prisoners and aliens',\n",
       "  'their descent from people of what tribe answer ham',\n",
       "  0.0035189217922875055,\n",
       "  0.08544114232063293],\n",
       " ['lechicilekhi lechitów', 'lechic', 0.5143221945036203, 0.06309705972671509],\n",
       " ['was of a different origin than the slavonic peasants',\n",
       "  'slavonic extraction',\n",
       "  3.569693362237558e-05,\n",
       "  0.04134710505604744],\n",
       " ['about the fifth century',\n",
       "  'fifth century',\n",
       "  0.013212576745165924,\n",
       "  0.06136498972773552],\n",
       " ['military caste', 'hindu society', 0.0219793231372023, 0.009099137969315052],\n",
       " ['the szlachta', 'the lechitocl', 0.019491054641627925, 0.022576717659831047],\n",
       " ['documentation regarding raciborz and alberts tenure',\n",
       "  'the documentation about the origins of the name and cry',\n",
       "  0.00011108075388466757,\n",
       "  0.05848996713757515],\n",
       " ['middle ages and in the early modern period',\n",
       "  'the early modern period answer',\n",
       "  0.00030107311772092525,\n",
       "  0.06344137340784073],\n",
       " ['the ius militare',\n",
       "  'the power to command an army',\n",
       "  0.16650020587009345,\n",
       "  0.018556874245405197],\n",
       " ['define knightly status',\n",
       "  'to define what',\n",
       "  0.04220485428091081,\n",
       "  0.03927535191178322],\n",
       " ['around the 14th century',\n",
       "  '14th century answer',\n",
       "  5.798209278395916e-06,\n",
       "  0.11596925556659698],\n",
       " ['defend the country',\n",
       "  'defend the country',\n",
       "  0.9325450658798218,\n",
       "  0.05910003185272217],\n",
       " ['geography', 'geography', 0.9895464777946472, 0.007010270841419697],\n",
       " ['wiec', 'the wie', 0.053108374025517456, 0.05626191198825836],\n",
       " ['an assembly of free tribesmen',\n",
       "  'an assembly of free tribesmen',\n",
       "  0.9475875198841095,\n",
       "  0.09341880679130554],\n",
       " ['an elected prince', 'prince', 0.0006485798221547157, 0.08022886514663696],\n",
       " ['elites', 'prince', 0.0015928015345707536, 0.008529365994036198],\n",
       " ['clans ród',\n",
       "  'the tribes were ruled by tribes',\n",
       "  0.0001313873700822327,\n",
       "  0.027288708835840225],\n",
       " ['theoretically descending from a common ancestor',\n",
       "  'a sense of solidarity',\n",
       "  0.00023518336021203604,\n",
       "  0.02365710400044918],\n",
       " ['related by blood or marriage',\n",
       "  'theódcl',\n",
       "  2.2208110233101762e-05,\n",
       "  0.018370505422353745],\n",
       " ['grόd', 'grόd', 0.8056268617510796, 0.09788165241479874],\n",
       " ['opole', 'the territory', 6.424376692848455e-06, 0.0077896686270833015],\n",
       " ['mieszko i of poland',\n",
       "  'mieszko i of poland',\n",
       "  0.9042135689939771,\n",
       "  0.10010822117328644],\n",
       " ['c 935 – 25 may 992',\n",
       "  '935 – 25 may 992 answer m',\n",
       "  0.00030386055196018235,\n",
       "  0.1334061175584793],\n",
       " ['preserving the unity of his state',\n",
       "  'preserving the unity of their state',\n",
       "  0.7724372446537018,\n",
       "  0.07673771679401398],\n",
       " ['mieszko is successors',\n",
       "  'mieszko i',\n",
       "  0.7706993610287706,\n",
       "  0.09399514645338058],\n",
       " ['rycerz', 'rycerz', 0.4586968899529893, 0.024960486218333244],\n",
       " ['knight', 'knight', 0.2515279948711395, 0.00218342081643641],\n",
       " ['wealthier families of poland and itinerant knights from abroad seeking their fortunes',\n",
       "  'wealthier families from abroad seeking their fortunes this other class of r',\n",
       "  0.18075786472289554,\n",
       "  0.05928801745176315],\n",
       " ['gradually formed apart from mieszko is and his successors elite retinues',\n",
       "  'the status of szlachtanobility',\n",
       "  5.331118542195069e-07,\n",
       "  0.029831912368535995],\n",
       " ['could serve as officials in state administration',\n",
       "  'regime of granting certain duties to those who',\n",
       "  2.3379655872957095e-05,\n",
       "  0.03864424675703049],\n",
       " ['ad 1138 – ad 1314',\n",
       "  'ad 1138 – ad 1314',\n",
       "  0.9645404265477107,\n",
       "  0.12281990796327591],\n",
       " ['bolesław iiis division of poland among his sons',\n",
       "  'bolesław iiis division of poland',\n",
       "  0.8359798738732934,\n",
       "  0.11041474342346191],\n",
       " ['polish', 'polish', 0.7233507037162781, 0.003737726714462042],\n",
       " ['850 ad', 'circa 800 ad', 0.0003921799950006688, 0.06283318251371384],\n",
       " ['piast dynasty', 'the', 0.0004760202737088548, 0.026697497814893723],\n",
       " ['coproprietors of piast realms',\n",
       "  'coproprietors of what realm',\n",
       "  0.689342195192767,\n",
       "  0.09237771481275558],\n",
       " ['attempted to deprive them of their independence',\n",
       "  'they tried to deprive them of their',\n",
       "  0.0012076106661425001,\n",
       "  0.05163773521780968],\n",
       " ['możni', 'the moż', 0.01755509218652747, 0.031997960060834885],\n",
       " ['die beste leuten', 'ponak', 0.00015538909600523731, 0.025104820728302002],\n",
       " ['ponai', 'ponak', 0.717193216085434, 0.053068216890096664],\n",
       " ['kunigai or kunigaikščiai',\n",
       "  'ponak',\n",
       "  0.07300146297598076,\n",
       "  0.01874864660203457],\n",
       " ['king of lithuania',\n",
       "  'the king of lithuania',\n",
       "  5.733586255018963e-05,\n",
       "  0.021436646580696106],\n",
       " ['бояре', 'bor', 0.0004269103093052136, 0.018831612542271614],\n",
       " ['polish szlachta',\n",
       "  'the luchian nobility',\n",
       "  0.002781195024487735,\n",
       "  0.014135030098259449],\n",
       " ['polonized',\n",
       "  'national consciousness',\n",
       "  1.7416568097357772e-06,\n",
       "  0.00950244814157486],\n",
       " ['they were of roman extraction',\n",
       "  'they were of roman extraction',\n",
       "  0.7116364896297455,\n",
       "  0.030530495569109917],\n",
       " ['šlėkta', 'šlėkta', 0.9166189687592643, 0.051191769540309906],\n",
       " ['lithuanian linguists',\n",
       "  'historical linguists',\n",
       "  0.0013003774946169777,\n",
       "  0.057637885212898254],\n",
       " ['the highest members of the nobility',\n",
       "  'the lower lonians',\n",
       "  0.07533453178644252,\n",
       "  0.018568776547908783],\n",
       " ['russian empire', 'the russian', 0.006666492316071526, 0.003180805593729019],\n",
       " ['removing lithuania from the names of the gubernyas',\n",
       "  'removal of which lithuania name from the guber',\n",
       "  0.014467071163259744,\n",
       "  0.056954871863126755],\n",
       " ['lithuanians are russians seduced by poles and catholicism',\n",
       "  'that lithuanians are russians seduced by poles and what answer',\n",
       "  0.003591012855248287,\n",
       "  0.09626474976539612],\n",
       " ['lithuanian language',\n",
       "  'lithuanian',\n",
       "  0.6656077215448022,\n",
       "  0.07149288058280945],\n",
       " ['grand duchy of lithuania',\n",
       "  'the multilingual grand duch',\n",
       "  0.00015436228689765246,\n",
       "  0.07111473381519318],\n",
       " ['multicultural and multilingual',\n",
       "  'diverse and bilingual',\n",
       "  0.24405678510402057,\n",
       "  0.04590050131082535],\n",
       " ['principalities of halych and volhynia',\n",
       "  'halych and volhynia answer',\n",
       "  5.3315176544151414e-06,\n",
       "  0.1290622055530548],\n",
       " ['intermarried', 'a marriage', 0.01328225432189356, 0.020756583660840988],\n",
       " ['polish and lithuanian nobility',\n",
       "  'the rights of catholics and',\n",
       "  0.001110112757235626,\n",
       "  0.02740289643406868],\n",
       " ['convert to catholicism',\n",
       "  'cultural pressure to',\n",
       "  0.00022242230045558623,\n",
       "  0.014961449429392815],\n",
       " ['union of brest',\n",
       "  'the union of b',\n",
       "  2.843857050410792e-05,\n",
       "  0.05394182726740837],\n",
       " ['1596', '1596', 0.9989493489265442, 0.07472190260887146],\n",
       " ['by monarch', 'monarch', 2.386822458788629e-06, 0.062014155089855194],\n",
       " ['ennoblement',\n",
       "  'reserved for members',\n",
       "  5.08148205228906e-06,\n",
       "  0.011363545432686806],\n",
       " ['undifferentiated coat of arms',\n",
       "  'individual status as szep',\n",
       "  2.0326877488512334e-05,\n",
       "  0.041464872658252716],\n",
       " ['sejm', 'monarch', 0.00021247874501995057, 0.007142084185034037],\n",
       " ['szlachta clan',\n",
       "  'polish nobility',\n",
       "  3.265266051814819e-08,\n",
       "  0.02817649208009243],\n",
       " ['between the 14th century and the mid18th century',\n",
       "  '14th century answer 14th century',\n",
       "  0.03834653704001671,\n",
       "  0.1065462976694107],\n",
       " ['two ennoblements per year',\n",
       "  'about how many answer',\n",
       "  0.01355687804114695,\n",
       "  0.026820790022611618],\n",
       " ['heraldic sources',\n",
       "  'heraldic sources',\n",
       "  0.9986185232798258,\n",
       "  0.11126470565795898],\n",
       " ['1600', '1600', 0.810621956984202, 0.053290486335754395],\n",
       " ['14th century onward',\n",
       "  '14th century',\n",
       "  0.6447290922515094,\n",
       "  0.10530634224414825],\n",
       " ['final years of the late 18th century',\n",
       "  'the 14th century was the last half',\n",
       "  0.007491514307909952,\n",
       "  0.08353739231824875],\n",
       " ['vytautas the great', 'vytas', 0.6153683932693639, 0.07644521445035934],\n",
       " ['bajorai',\n",
       "  'forces comprising nobles',\n",
       "  2.935929539846921e-10,\n",
       "  0.005700115114450455],\n",
       " ['lithuanian pagan given names of their ennobled ancestors',\n",
       "  'the�s rad günts',\n",
       "  2.0926175299701153e-05,\n",
       "  0.026316432282328606],\n",
       " ['goštautai', 'vytas', 1.2512308838343034e-05, 0.020027346909046173],\n",
       " ['union of horodlo',\n",
       "  'goštautas',\n",
       "  0.00034974294680812366,\n",
       "  0.031949885189533234],\n",
       " ['robert bideleux and ian jeffries',\n",
       "  'robert bideleux and ian jeffries',\n",
       "  0.9891781674491035,\n",
       "  0.1288500428199768],\n",
       " ['1374 exemption from the land tax',\n",
       "  'a 1425 guarantee against the ar',\n",
       "  0.039371679892252,\n",
       "  0.024079274386167526],\n",
       " ['requirement that military forces and new taxes be approved by provincial sejms',\n",
       "  'military and new taxes for new settlers answer military forces and',\n",
       "  0.061035488624183225,\n",
       "  0.10140147060155869],\n",
       " ['rights of commoners',\n",
       "  'those issued between 14',\n",
       "  0.01046207534021093,\n",
       "  0.023403532803058624],\n",
       " ['service to the state',\n",
       "  'nobility this was',\n",
       "  0.00369860022328794,\n",
       "  0.010489108972251415],\n",
       " ['really usurpers being commoners',\n",
       "  'usurpers',\n",
       "  1.9348706200054504e-05,\n",
       "  0.0776442289352417],\n",
       " ['hieronim nekanda trepka',\n",
       "  'hereonim sekanda tp',\n",
       "  0.009826948495175028,\n",
       "  0.041958048939704895],\n",
       " ['first half of the 16th century',\n",
       "  'in the first half of the 16',\n",
       "  0.04098970217523871,\n",
       "  0.030741920694708824],\n",
       " ['owning nobilityestates and promised the estate to the denouncer',\n",
       "  'hundreds of these false nobles were converted to what answer usur',\n",
       "  0.0003513247891796893,\n",
       "  0.013517544604837894],\n",
       " ['many rights', 'the male', 0.00011832782911369577, 0.0065991440787911415],\n",
       " ['poland', 'the', 0.008762598969042301, 0.0015916493721306324],\n",
       " ['not of the king or the ruling dynasty',\n",
       "  'the polish nobility was named what',\n",
       "  0.03231240823329198,\n",
       "  0.01679288037121296],\n",
       " ['the extinction of the maleline descendants of the old royal dynasty',\n",
       "  'the extinction of the male lineages of the royal dynasty',\n",
       "  0.35973660955799763,\n",
       "  0.07696492969989777],\n",
       " ['dynastys femaleline descendants',\n",
       "  'the royaltys offspring of the old',\n",
       "  1.6445273951757956e-05,\n",
       "  0.05066782981157303],\n",
       " ['at the time of their election to the throne',\n",
       "  'at the time of their election to the throne',\n",
       "  0.8253559768199921,\n",
       "  0.10214053094387054],\n",
       " ['kingelects pacta conventa',\n",
       "  'the privilege of the nobility in exchange for',\n",
       "  0.00031743102482778346,\n",
       "  0.043198615312576294],\n",
       " ['ad hoc permission to raise an extraordinary tax',\n",
       "  'a tax or levy or ruspol',\n",
       "  0.00080948475805701,\n",
       "  0.06857910752296448],\n",
       " ['privileges', 'privileges', 0.751797616481781, 0.010389769449830055],\n",
       " ['1355', '1355', 0.9835608899593353, 0.05257558077573776],\n",
       " ['buda king casimir iii the great',\n",
       "  'the great',\n",
       "  0.017201657311754086,\n",
       "  0.09240394830703735],\n",
       " ['louis i of hungary',\n",
       "  'louis i of hungary',\n",
       "  0.8619801104068756,\n",
       "  0.0636458545923233],\n",
       " ['the nobility would no longer be subject to extraordinary taxes',\n",
       "  'that the nobility would no longer be subject to what answer',\n",
       "  0.026011774117978184,\n",
       "  0.07929904758930206],\n",
       " ['the king and the court',\n",
       "  'the king and court would',\n",
       "  0.5521660926904133,\n",
       "  0.06193806603550911],\n",
       " ['1374', '1374', 0.9815897941589355, 0.04673980921506882],\n",
       " ['in order to guarantee the polish throne for his daughter jadwiga',\n",
       "  'polish',\n",
       "  0.0002494251007745992,\n",
       "  0.02559962123632431],\n",
       " ['exempted the entire class from all but one tax',\n",
       "  'raise taxes and taxes answer no new',\n",
       "  0.0022888910185157782,\n",
       "  0.012740451842546463],\n",
       " ['abolished', 'the', 0.36200740933418274, 0.0016440111212432384],\n",
       " ['king to pay indemnities',\n",
       "  'pay them answer',\n",
       "  0.00016671984118251556,\n",
       "  0.036610063165426254],\n",
       " ['king władysław ii jagiełło',\n",
       "  'king władysław ii jagieło',\n",
       "  0.7790972338734197,\n",
       "  0.2028130441904068],\n",
       " ['1422', '1222', 0.38205740600824356, 0.016401588916778564],\n",
       " ['ceded', 'ceded', 0.7904633581638336, 0.03334135189652443],\n",
       " ['at kraków in 1433',\n",
       "  'kraków in 1430',\n",
       "  3.12460266995683e-05,\n",
       "  0.06333164870738983],\n",
       " ['brześć kujawski privilege',\n",
       "  'his earlier brześć kujaws',\n",
       "  0.012824277247247393,\n",
       "  0.08342184126377106],\n",
       " ['nobility a guarantee against arbitrary arrest',\n",
       "  'nemimas captiv',\n",
       "  4.192960564308734e-07,\n",
       "  0.0075718555599451065],\n",
       " ['warrant from a court of justice',\n",
       "  'a warrant from a court of',\n",
       "  3.0494967215114837e-05,\n",
       "  0.05164269357919693],\n",
       " ['king władysławs quid pro quo for this boon',\n",
       "  'no słachten of the boyars was given by the king to',\n",
       "  0.07709843064895819,\n",
       "  0.024699201807379723],\n",
       " ['1454', '1454', 0.9951602220535278, 0.04655373468995094],\n",
       " ['the legal basis of voivodship sejmiks',\n",
       "  'the legal basis of what answer viuod',\n",
       "  0.29100097017832044,\n",
       "  0.040507346391677856],\n",
       " ['judicial abuses', 'abuses by', 0.002862161404067365, 0.056837208569049835],\n",
       " ['magnates', 'the tit', 0.00028297398851862, 0.0039664097130298615],\n",
       " ['their participation in the thirteen years war',\n",
       "  'to compensation for their involvement in the war and in',\n",
       "  0.009918417212685538,\n",
       "  0.0831720307469368],\n",
       " ['1492', '1492', 0.9860630631446838, 0.041205815970897675],\n",
       " ['no restrictions on the choice of candidates',\n",
       "  'no restrictions on who could vote',\n",
       "  0.32688920062105165,\n",
       "  0.07765552401542664],\n",
       " ['senators', 'john', 0.0013482387876138091, 0.003619816852733493],\n",
       " ['john i albert', 'john i albert', 0.9774619142214457, 0.08708258718252182],\n",
       " ['jagiellonian dynasty',\n",
       "  'the polish empire',\n",
       "  0.012547070138853573,\n",
       "  0.020591355860233307],\n",
       " ['king john i albert',\n",
       "  'king john i albert',\n",
       "  0.9877527207136154,\n",
       "  0.11039412021636963],\n",
       " ['on april 26 1496',\n",
       "  'april 26 1496 what',\n",
       "  1.6771320891636094e-05,\n",
       "  0.0832286924123764],\n",
       " ['increasing', 'one', 0.0003838184638880193, 0.0013930394779890776],\n",
       " ['one son not the eldest',\n",
       "  'peasants',\n",
       "  0.00042503974873397965,\n",
       "  0.019926901906728745],\n",
       " ['owning land', 'owning land', 0.9223352074623108, 0.059168748557567596],\n",
       " ['23 october 1501',\n",
       "  '23 october 1501',\n",
       "  0.9925134927034378,\n",
       "  0.0701698362827301],\n",
       " ['union of mielnik',\n",
       "  'union of mielnik',\n",
       "  0.9928839564323425,\n",
       "  0.048507314175367355],\n",
       " ['union of mielnik', 'poland', 0.13072025310248137, 0.015793662518262863],\n",
       " ['more to strengthen the magnate dominated senate of poland then the lesser nobility',\n",
       "  'to make the violative to the law more difficult to enforce answer',\n",
       "  0.009984944313599642,\n",
       "  0.03311075642704964],\n",
       " ['disobey the king or his representatives',\n",
       "  'the law was broken by the m',\n",
       "  0.0002169453510494674,\n",
       "  0.019303210079669952],\n",
       " ['3 may 1505', '3 may 1505', 0.9949568957090378, 0.08077199012041092],\n",
       " ['king alexander i jagiellon',\n",
       "  'king i',\n",
       "  0.4606306540469329,\n",
       "  0.09647079557180405],\n",
       " ['forbade the king to pass any new law without the consent of the representatives of the nobility',\n",
       "  'on 3 may 1505 king alexander i i i was giving due to his giving of what',\n",
       "  6.420132328433302e-05,\n",
       "  0.03449820354580879],\n",
       " ['greatly strengthened',\n",
       "  'transferred legislative',\n",
       "  0.00039565533384034596,\n",
       "  0.005546037573367357],\n",
       " ['legislative power from the king to the sejm',\n",
       "  'transferred legislative power from king to sejm via date',\n",
       "  0.16716259809577835,\n",
       "  0.07886821776628494],\n",
       " ['executionist movement',\n",
       "  'polish',\n",
       "  0.00022523814550368115,\n",
       "  0.02100224792957306],\n",
       " ['seek to curb the power of the magnates at the sejm and to strengthen the power of king and country',\n",
       "  'to take control of the power of the king answer to strengthen the power of king and country',\n",
       "  0.00670478684418461,\n",
       "  0.07107309252023697],\n",
       " ['return many leased crown lands to the king',\n",
       "  'return many crown lands to the king and the',\n",
       "  0.1842583675456208,\n",
       "  0.07407660037279129],\n",
       " ['1605', '1605', 0.996314287185669, 0.04840429499745369],\n",
       " ['movement lost its political force',\n",
       "  'his death',\n",
       "  3.7589279221113734e-06,\n",
       "  0.05164482444524765],\n",
       " ['sigismund ii augustus',\n",
       "  'sigismund',\n",
       "  0.7883971706032753,\n",
       "  0.10435928404331207],\n",
       " ['monarchs could be elected from within only the royal family',\n",
       "  'the last king of the dynasty the sigistund',\n",
       "  0.00031681454392595604,\n",
       "  0.03272176906466484],\n",
       " ['any polish noble or foreigner of royal blood',\n",
       "  'practically any polish noble or foreigner could become',\n",
       "  0.0038283351489261097,\n",
       "  0.0699249804019928],\n",
       " ['two documents', 'two', 0.32076550647616386, 0.057023923844099045],\n",
       " ['basic laws of the commonwealth',\n",
       "  'the word kur',\n",
       "  0.01026761481883821,\n",
       "  0.028657669201493263],\n",
       " ['1578', '1578', 0.9875260889530182, 0.054323937743902206],\n",
       " ['king stefan batory',\n",
       "  'stefan batory',\n",
       "  0.0009659479097963164,\n",
       "  0.13182711601257324],\n",
       " ['reduce the enormous pressure on the royal court',\n",
       "  'reduce answer the great pressure',\n",
       "  0.29889834777187646,\n",
       "  0.053505416959524155],\n",
       " ['the nobility class', 'the sz', 0.20944903071639942, 0.020038854330778122],\n",
       " ['lithuanian tribunal',\n",
       "  'lithuanian tribunal',\n",
       "  0.7310531934102377,\n",
       "  0.1088445782661438],\n",
       " ['gain legal privileges over their peers',\n",
       "  'legal privileges over their peers',\n",
       "  0.0011861143033048787,\n",
       "  0.1131020188331604],\n",
       " ['few szlachta were wealthy enough to be known as magnates',\n",
       "  'wealthy members of the szlachta sought to gain over',\n",
       "  0.001743376522220999,\n",
       "  0.06941360980272293],\n",
       " ['crimsons', 'crims', 0.0001493010022386443, 0.060350921005010605],\n",
       " ['own at least 20 villages or estates',\n",
       "  'many villages or estates answer their',\n",
       "  1.9985121183962346e-05,\n",
       "  0.08050999790430069],\n",
       " ['magnates', 'he should', 0.00036985144462564623, 0.0066937836818397045],\n",
       " ['1', '1', 0.8933171629905701, 0.05274726077914238],\n",
       " ['200–300', '200300 to', 0.39431234191094217, 0.09668322652578354],\n",
       " ['30–40', '3040', 0.6281893153985342, 0.06012571603059769],\n",
       " ['monarchs', 'monarchs', 0.9833177030086517, 0.08018489181995392],\n",
       " ['magnates', 'gifts from', 0.000180534620705608, 0.015307208523154259],\n",
       " ['temporary leases',\n",
       "  'temporary and',\n",
       "  0.4612590738761355,\n",
       "  0.06307648867368698],\n",
       " ['never returned',\n",
       "  'return them',\n",
       "  0.00034348103145021014,\n",
       "  0.058409374207258224],\n",
       " ['magnates', 'magnates', 0.9595529735088348, 0.060968246310949326],\n",
       " ['late 16th century',\n",
       "  'late 16th century',\n",
       "  0.8663305789232254,\n",
       "  0.09497148543596268],\n",
       " ['ensured that a family which gained wealth and power could more easily preserve this',\n",
       "  'a strong and powerful family gained wealth and power could preserve a family that',\n",
       "  0.28796834096330226,\n",
       "  0.10495992749929428],\n",
       " ['often rivalled',\n",
       "  'rivalled them',\n",
       "  0.0003971275759574776,\n",
       "  0.1030515506863594],\n",
       " ['estates of the king',\n",
       "  'ordynacja',\n",
       "  1.5062588172298774e-06,\n",
       "  0.021801698952913284],\n",
       " ['1795', '1795', 0.9939424097537994, 0.07391403615474701],\n",
       " ['partitions of poland',\n",
       "  'partitions of poland',\n",
       "  0.9646532833576202,\n",
       "  0.100654736161232],\n",
       " ['szlachta',\n",
       "  'the status of the',\n",
       "  5.0932957014986416e-05,\n",
       "  0.027232415974140167],\n",
       " ['nicholas i', 'nicholas i', 0.9659696221351624, 0.0640864148736],\n",
       " ['628',\n",
       "  'nicholas i reduced 64',\n",
       "  0.00026121969549008917,\n",
       "  0.004716744180768728],\n",
       " ['russian poland on february 19 1864',\n",
       "  'march constitution',\n",
       "  0.0002274128099391722,\n",
       "  0.019699174910783768],\n",
       " ['only sell land to other peasants not szlachta',\n",
       "  'peasants or ex peasants answer peasants and peasants',\n",
       "  0.0001750489186657354,\n",
       "  0.018750283867120743],\n",
       " ['489', '64 of s', 0.015261966264107585, 0.01667940616607666],\n",
       " ['european countries the nobility lost power as the ruler strove for absolute monarchy',\n",
       "  'the nobility of other countries',\n",
       "  1.0250370537099688e-05,\n",
       "  0.08961765468120575],\n",
       " ['actually gained power',\n",
       "  'political system developed',\n",
       "  4.176663731906653e-07,\n",
       "  0.017696358263492584],\n",
       " ['oligarchy', 'anarchy', 0.26178878951759543, 0.021041393280029297],\n",
       " ['absolute monarchy',\n",
       "  'absolute monarchy',\n",
       "  0.9138690531253815,\n",
       "  0.0807853415608406],\n",
       " ['10–12', '10 of all', 0.23363510663037346, 0.01900341361761093],\n",
       " ['8', '8', 0.9516503512859344, 0.03342466428875923],\n",
       " ['most local nobility from the areas that were absorbed by poland–lithuania',\n",
       "  'local nobility from the areas that were absorbed by poland–lithuania',\n",
       "  0.0001380053283853366,\n",
       "  0.0599193200469017],\n",
       " ['1–3', 'about 3', 0.05618241209117514, 0.006612188182771206],\n",
       " ['march constitution of poland',\n",
       "  'march constitution of poland',\n",
       "  0.7990481853485107,\n",
       "  0.06397603452205658],\n",
       " ['closed class', 'low', 0.00038262018642853945, 0.00542532280087471],\n",
       " ['many lowborn individuals',\n",
       "  'lowborn individuals',\n",
       "  5.329159535959714e-05,\n",
       "  0.061653174459934235],\n",
       " ['enormous influence',\n",
       "  'almost all',\n",
       "  0.007359781594459491,\n",
       "  0.004341770429164171],\n",
       " ['any nobleman', 'nobleman', 0.12557852969852235, 0.088685542345047],\n",
       " ['liberum veto',\n",
       "  'commonwealth parliament',\n",
       "  6.942843550442983e-06,\n",
       "  0.013682132586836815],\n",
       " ['noble mother and father',\n",
       "  'a noble mother and',\n",
       "  0.03117324196603022,\n",
       "  0.05759080499410629],\n",
       " ['special services to the state',\n",
       "  'could attain it for special',\n",
       "  1.5815838105304182e-05,\n",
       "  0.016576319932937622],\n",
       " ['polish king', 'polish king', 0.8272640109062195, 0.03962521627545357],\n",
       " ['nobilitacja', 'noblesprincen', 0.009379840559951447, 0.020669786259531975],\n",
       " ['indygenat', 'indygenat', 0.7913287431001663, 0.0844573900103569],\n",
       " ['equals', 'social', 5.143004727869993e-06, 0.0077249775640666485],\n",
       " ['not hereditary',\n",
       "  'not hereditary',\n",
       "  0.6924697905778885,\n",
       "  0.028704799711704254],\n",
       " ['ritual', '', 0.0013640637043863535, 0.003978692926466465],\n",
       " ['other lords were only peers de iure',\n",
       "  'crown of the crown but did not have a title',\n",
       "  0.02420628925802562,\n",
       "  0.08579210191965103],\n",
       " ['any nobility that owned lands',\n",
       "  'polish nobility that owned lands',\n",
       "  0.5545758299529553,\n",
       "  0.05833029747009277],\n",
       " ['ziemianie or ziemiaństwo',\n",
       "  'ziemianie or ziemiestwo',\n",
       "  0.3802837112323018,\n",
       "  0.15466709434986115],\n",
       " ['no', 'no', 0.3273073434829712, 0.009506307542324066],\n",
       " ['coats of arms', 'coats of arms', 0.9972625374794006, 0.09381696581840515],\n",
       " ['heraldic system', 'coats of', 0.01278739957286549, 0.014017697423696518],\n",
       " ['differing in many ways',\n",
       "  'different from other european',\n",
       "  0.04609071656523156,\n",
       "  0.02363339066505432],\n",
       " ['moravia ie poraj and germany',\n",
       "  'moravia',\n",
       "  0.6610114085682047,\n",
       "  0.1547914296388626],\n",
       " ['minorities would be given the noble title',\n",
       "  'the jews and muslims are given what',\n",
       "  0.0003573061497760425,\n",
       "  0.032539162784814835],\n",
       " ['most families sharing origin',\n",
       "  'families adopted into the',\n",
       "  0.0001032264256060067,\n",
       "  0.020045576617121696],\n",
       " ['on the basis of similarity of arms',\n",
       "  'due to similarity of arms',\n",
       "  0.15000129385406474,\n",
       "  0.042970433831214905],\n",
       " ['low and did not exceed 200',\n",
       "  'not high enough to appear in',\n",
       "  0.014472259317656722,\n",
       "  0.045821260660886765],\n",
       " ['brisure', 'brisure', 0.7634258270263672, 0.08096025884151459],\n",
       " ['poland', 'poland', 0.9999350309371948, 0.00413370318710804],\n",
       " ['their fathers', 'from their', 0.06220659536484163, 0.015885004773736],\n",
       " ['sarmatism', 'sarmatism', 0.8064764241377512, 0.023807896301150322],\n",
       " ['powerful ancient nation of sarmatians',\n",
       "  'the ancient nation of sarmatians',\n",
       "  0.8098473746123318,\n",
       "  0.0669831708073616],\n",
       " ['served to integrate the multiethnic nobility',\n",
       "  'sarmatism affected everyone in their lives',\n",
       "  8.05159030820468e-06,\n",
       "  0.012503968551754951],\n",
       " ['peace and pacifism',\n",
       "  'sarmatism',\n",
       "  3.979966948265805e-06,\n",
       "  0.005345706827938557],\n",
       " ['polish and latin',\n",
       "  'polish and latin',\n",
       "  0.9253561099370321,\n",
       "  0.051712747663259506],\n",
       " ['roman catholic or orthodox',\n",
       "  'roman catholic and orthodox',\n",
       "  0.7417734843329526,\n",
       "  0.07330519706010818],\n",
       " ['muslims', 'roman', 4.456570604816079e-05, 0.0036275931634008884],\n",
       " ['ennoblement', 'ennity', 0.1296566858480522, 0.052851855754852295],\n",
       " ['the nobility became almost exclusively catholic',\n",
       "  'the catholic church became almost exclusively',\n",
       "  0.007695826900951001,\n",
       "  0.04768253117799759],\n",
       " ['jews', 'catholic', 0.0004855759325437248, 0.0018942075548693538],\n",
       " ['augustan', 'augustan', 0.9980847239494324, 0.05537844076752663],\n",
       " ['three', 'three', 0.9947677850723267, 0.0022198865190148354],\n",
       " ['aeneid', 'aeneid', 0.9975226322809855, 0.06359194219112396],\n",
       " ['publius vergilius maro',\n",
       "  'virgil',\n",
       "  4.428919877833929e-05,\n",
       "  0.027948250994086266],\n",
       " ['appendix vergiliana',\n",
       "  'aeneid',\n",
       "  0.00022418433059101517,\n",
       "  0.03235941380262375],\n",
       " ['aeneid', 'virgil', 0.0010910672371859202, 0.010296184569597244],\n",
       " ['iliad and odyssey',\n",
       "  'odyssey and the odyssey',\n",
       "  0.05034023561911401,\n",
       "  0.037781037390232086],\n",
       " ['aeneas', 'virgil', 0.00032604416266905173, 0.006402164697647095],\n",
       " ['fulfill his destiny and arrive on the shores of italy',\n",
       "  'fulfill his goal answer fulfill his',\n",
       "  0.21631440803481888,\n",
       "  0.056559301912784576],\n",
       " ['virgil', 'virgil', 0.9999641180038452, 0.03612051531672478],\n",
       " ['varius', 'sueton', 4.5605816012539435e-05, 0.00692103523761034],\n",
       " ['servius and donatus',\n",
       "  'the two great commentators on',\n",
       "  0.004125902404041648,\n",
       "  0.01247295644134283],\n",
       " ['inferences made from his poetry and allegorizing',\n",
       "  'infersibility answer infersibility',\n",
       "  0.10192779594199601,\n",
       "  0.0683915987610817],\n",
       " ['problematic', 'it', 0.0014830984873697162, 0.00771298399195075],\n",
       " ['andes', 'andes', 0.9996849000453949, 0.04500056058168411],\n",
       " ['cremona mediolanum rome and naples',\n",
       "  'cremona mediolanum rome and naples',\n",
       "  0.9417718201875687,\n",
       "  0.11863689869642258],\n",
       " ['rhetoric and law', 'poetry', 0.00016032021422536977, 0.015689609572291374],\n",
       " ['equestrian landowning',\n",
       "  'equestrian landowning',\n",
       "  0.9001805086930593,\n",
       "  0.11140228807926178],\n",
       " ['humble', 'humble', 0.2547779381275177, 0.0018517038552090526],\n",
       " ['rhetoric medicine and astronomy',\n",
       "  'theology and astronomy',\n",
       "  0.007048700994346291,\n",
       "  0.04673121124505997],\n",
       " ['extremely shy and reserved',\n",
       "  'as a person',\n",
       "  0.0001384258775942726,\n",
       "  0.003954099491238594],\n",
       " ['parthenias', 'parthenias', 0.9757879575093588, 0.039740294218063354],\n",
       " ['social aloofness', 'quas', 4.251895609208578e-07, 0.0035447319969534874],\n",
       " ['the culex', 'the gnat', 0.014497493324483912, 0.009621482342481613],\n",
       " ['theocritus', 'cygil', 0.010610631856226682, 0.0016901470953598619],\n",
       " ['octavian', 'julius caesar', 0.0038588062965993672, 0.002373748691752553],\n",
       " ['eclogues',\n",
       "  'battle against caesar was',\n",
       "  0.0007022242960780106,\n",
       "  0.007579911965876818],\n",
       " ['asinius pollio',\n",
       "  'the land confiscation and',\n",
       "  0.002878113750989627,\n",
       "  0.002003346337005496],\n",
       " ['ecl 2',\n",
       "  'land confiscation and',\n",
       "  2.2097992552483348e-07,\n",
       "  0.0016224789433181286],\n",
       " ['varius rufus', 'horace', 0.023547012778024623, 0.028186332434415817],\n",
       " ['before 37 bc', 'after 37 bc', 0.45763675433893997, 0.04411068558692932],\n",
       " ['maecenas', 'maecenas', 0.9896832555532455, 0.10044752061367035],\n",
       " ['horace', 'horace', 0.9620016515254974, 0.07382596284151077],\n",
       " ['georgics', 'the georg', 0.0006370821502059698, 0.003997597843408585],\n",
       " ['maecenas', 'maecenas', 0.9963009357452393, 0.06182778626680374],\n",
       " ['running a farm', 'running a farm', 0.9919275641441345, 0.02744949422776699],\n",
       " ['gallus', 'gallus', 0.884230226278305, 0.04943034052848816],\n",
       " ['augustus', 'augustus', 0.9635223746299744, 0.005257557611912489],\n",
       " ['battle of actium',\n",
       "  'battle of actium',\n",
       "  0.9215403348207474,\n",
       "  0.0864405706524849],\n",
       " ['31 bc', '31 bc', 0.7113122344017029, 0.07040240615606308],\n",
       " ['maecenas', 'maecenas', 0.9423080533742905, 0.10972949862480164],\n",
       " ['the aeneid', 'virgils', 0.06958519549784095, 0.005070172715932131],\n",
       " ['augustus', 'augustus', 0.9847094416618347, 0.0020166318863630295],\n",
       " ['12', '12', 0.994619607925415, 0.0013546645641326904],\n",
       " ['odyssey', 'en', 6.644267705269158e-05, 0.002714968053624034],\n",
       " ['iliad', 'the odyssey', 5.008488401022267e-05, 0.010458186268806458],\n",
       " ['juno', 'throughout', 5.6742042033874895e-06, 0.0020492756739258766],\n",
       " ['dido', 'jupiter', 0.05818949086718561, 0.0031135426834225655],\n",
       " ['5', 'book', 0.00021067168563604355, 0.0022473703138530254],\n",
       " ['anchises', 'horn', 0.013973732367048797, 0.012131011113524437],\n",
       " ['sibyl', 'the aene', 0.00034667616557726433, 0.006868274416774511],\n",
       " ['lavinia', 'king', 0.1778584263543337, 0.01828012242913246],\n",
       " ['king evander', 'kingeus', 0.21177451190951282, 0.020062223076820374],\n",
       " ['turnus', 'the king', 0.0035387878457413535, 0.0032341149635612965],\n",
       " ['turnus', 'amata', 0.00011817269705716171, 0.004218241665512323],\n",
       " ['31 bc', '31 bc', 0.9747840464115143, 0.05299660935997963],\n",
       " ['aeneas', 'aeneas', 0.9410384694735209, 0.013758784160017967],\n",
       " ['turnus', 'turnus', 0.9420104026794434, 0.052979256957769394],\n",
       " ['augustus', 'augustus', 0.8136887550354004, 0.0014292167034000158],\n",
       " ['6', 'book', 2.3124515280414926e-07, 0.0038135703653097153],\n",
       " ['virgil reading the aeneid',\n",
       "  'virgil',\n",
       "  0.6724419929087162,\n",
       "  0.07708283513784409],\n",
       " ['augustus', 'augustus', 0.9683264493942261, 0.004061757121235132],\n",
       " ['greece', 'greece', 0.9908578991889954, 0.001858067698776722],\n",
       " ['brundisium harbor', 'brundisium', 0.8146128207445145, 0.08798810094594955],\n",
       " ['september 21 19 bc',\n",
       "  'september 21 19 bc',\n",
       "  0.7872289896011353,\n",
       "  0.05687859654426575],\n",
       " ['lucius varius rufus and plotius tucca',\n",
       "  'lucius rufus and plotius tucca',\n",
       "  0.0916016692751362,\n",
       "  0.09366834163665771],\n",
       " ['the poem be burned',\n",
       "  'virgils own',\n",
       "  0.013292434502477524,\n",
       "  0.006466489285230637],\n",
       " ['silius italicus', 'virgil', 0.04246618482261203, 0.0037015832494944334],\n",
       " ['punica', 'messian', 1.3392488679153658e-05, 0.0010694877710193396],\n",
       " ['silius', 'silius', 0.765874852736791, 0.028411250561475754],\n",
       " ['fourth eclogue', 'punica sili', 8.67720619239943e-09, 0.001821296289563179],\n",
       " ['gregory of tours', 'virgil', 0.008566959840032555, 0.012568585574626923],\n",
       " ['western roman', 'western roman', 0.9825465977191925, 0.0792151466012001],\n",
       " ['master', 'vir', 0.0011958836112171412, 0.0037307641468942165],\n",
       " ['divine comedy', 'the divine', 1.883264610569313e-05, 0.009123038500547409],\n",
       " ['dante', 'dante', 0.9977095127105713, 0.010821511968970299],\n",
       " ['virgil', 'virgil', 0.9763048589229584, 0.07130032777786255],\n",
       " ['eclogues 4',\n",
       "  'virgils reputation was',\n",
       "  0.014881489102123318,\n",
       "  0.01226548757404089],\n",
       " ['christianity', 'christianity', 0.9947056174278259, 0.010187770240008831],\n",
       " ['middle ages', 'middle ages', 0.894050121307373, 0.08334016799926758],\n",
       " ['naples', 'naples', 0.6114057302474976, 0.001608390361070633],\n",
       " ['over two hundred years',\n",
       "  'over two hundred years',\n",
       "  0.997328594326973,\n",
       "  0.08529132604598999],\n",
       " ['fferyllydd', 'feryllt', 0.24136683072052062, 0.048672448843717575],\n",
       " ['medieval wales',\n",
       "  'modern wales',\n",
       "  0.26450638473033905,\n",
       "  0.0068789320066571236],\n",
       " ['12th century', 'around', 0.15732401336390467, 0.004184779245406389],\n",
       " ['middle ages', 'middle ages', 0.8502333760261536, 0.06648300588130951],\n",
       " ['lucretia', 'lucretia', 0.94305419921875, 0.07994014769792557],\n",
       " ['phyllis riding aristotle',\n",
       "  'lucretias',\n",
       "  0.0073881467273020154,\n",
       "  0.02283105067908764],\n",
       " ['lucas van leyden', 'lucretia', 0.03343314723752362, 0.02235896699130535],\n",
       " ['middle ages', 'middle ages', 0.9690020084381104, 0.06492666900157928],\n",
       " ['piedigrotta', 'piedigrotta', 0.9985388159751892, 0.10406474024057388],\n",
       " ['grotta vecchia', 'grottavecchia', 0.630021932721138, 0.06658732891082764],\n",
       " ['vergilius', 'the swan of mant', 0.011392669987984723, 0.003733546007424593],\n",
       " ['late empire and middle ages',\n",
       "  'late ages',\n",
       "  0.3270528612968822,\n",
       "  0.07268038392066956],\n",
       " ['19th', '19th', 0.9992399215698242, 0.048770736902952194],\n",
       " ['oxford', 'oxford', 0.9097234606742859, 0.0020153189543634653],\n",
       " ['europe', 'europe', 0.4016464054584503, 0.0010976882185786963],\n",
       " ['1200 kilometres', '750 mi', 0.00824972506226677, 0.005537278950214386],\n",
       " ['over tens of millions of years',\n",
       "  'over how many years',\n",
       "  0.11396169851845107,\n",
       "  0.033790115267038345],\n",
       " ['mont blanc', 'alp', 0.0006734158956192005, 0.004938465543091297],\n",
       " ['the fourthousanders',\n",
       "  'fourthousanders',\n",
       "  0.0505956717859117,\n",
       "  0.0959048792719841],\n",
       " ['the altitude and size of the range',\n",
       "  'altitude and size of range affects what',\n",
       "  0.06451951289204771,\n",
       "  0.09193485230207443],\n",
       " ['ibex', 'ibex', 0.9998661577701569, 0.07361214607954025],\n",
       " ['edelweiss', 'edelweiss', 0.9923180192708969, 0.1123742163181305],\n",
       " ['paleolithic era', 'paleolithic', 0.7924639085928599, 0.09803126752376556],\n",
       " ['5000 years old', '5000 years old', 0.9732441544532776, 0.06728686392307281],\n",
       " ['the celtic la tène culture',\n",
       "  'celtic la tène culture was',\n",
       "  0.0003254551770261423,\n",
       "  0.11589422821998596],\n",
       " ['hannibal', 'napoleon', 0.045309536159038544, 0.0031304415315389633],\n",
       " ['napoleon', 'napoleon', 0.9987252354621887, 0.002462127711623907],\n",
       " ['bavarian alps', 'bavarian alps', 0.9890312751134237, 0.08678877353668213],\n",
       " ['14 million people',\n",
       "  '14 million people',\n",
       "  0.895916740099589,\n",
       "  0.07697755098342896],\n",
       " ['120 million', '120 million', 0.9978862106800079, 0.06682457774877548],\n",
       " ['tourist industry',\n",
       "  'tourist industry',\n",
       "  0.8616094291210175,\n",
       "  0.0637756809592247],\n",
       " ['alpes', 'alps', 0.5203168950974941, 0.046197958290576935],\n",
       " ['maurus servius honoratus',\n",
       "  'maurus servius honoratus',\n",
       "  0.9041714568932852,\n",
       "  0.13512679934501648],\n",
       " ['celtic languages',\n",
       "  'celtic languages',\n",
       "  0.8974377512931824,\n",
       "  0.07504385709762573],\n",
       " ['indoeuropean origin', 'nonindo', 0.12566220262415886, 0.04075723513960838],\n",
       " ['albania', 'albania', 0.9901123642921448, 0.0028989012353122234],\n",
       " ['the eastern caucasus',\n",
       "  'the eastern caucasus',\n",
       "  0.7750712633132935,\n",
       "  0.055317431688308716],\n",
       " ['scotland', 'eastern', 0.00032254611141979694, 0.005365201737731695],\n",
       " ['alb white and albus',\n",
       "  'alb white and albus',\n",
       "  0.9816863462328911,\n",
       "  0.17457431554794312],\n",
       " ['the association of the tops of tall mountains or steep hills with snow',\n",
       "  'the association of the tallest mountains or mountains with snow answer',\n",
       "  0.22796615243163929,\n",
       "  0.18258148431777954],\n",
       " ['white', 'white', 0.7522119283676147, 0.009866110980510712],\n",
       " ['a grazing pastures in the alpine regions below the glaciers not the peaks',\n",
       "  'a pastures in the alpine regions below the glaciers answer the al',\n",
       "  0.0903040789570193,\n",
       "  0.09994190186262131],\n",
       " ['an alp', 'alp', 0.0002854207603964672, 0.03709061071276665],\n",
       " ['german speaking regions',\n",
       "  'german speaking regions',\n",
       "  0.960036834081014,\n",
       "  0.07186596840620041],\n",
       " ['french speaking regions',\n",
       "  'italian speaking regions',\n",
       "  0.703098272283872,\n",
       "  0.029526107013225555],\n",
       " ['the alps', 'the alps', 0.9228695631027222, 0.007441962603479624],\n",
       " ['25 km 16 mi', '25 km 16 mi', 0.940395200252533, 0.06627462804317474],\n",
       " ['the mediterranean sea north above the po basin extending through france from grenoble eastward through mid and southern switzerland',\n",
       "  '800 km 500 mi',\n",
       "  7.189081425004343e-08,\n",
       "  0.02975497581064701],\n",
       " ['alpine territory', 'alpine areas', 0.6763095892965794, 0.0711379274725914],\n",
       " ['the glacial trough of the rhone valley',\n",
       "  'the glacial trough of the rhone valley',\n",
       "  0.9800357818603516,\n",
       "  0.11610851436853409],\n",
       " ['the easterly portion',\n",
       "  'in slovenia and austria',\n",
       "  0.003279899914902994,\n",
       "  0.022126657888293266],\n",
       " ['northern', 'northern', 0.9212566018104553, 0.00320007698610425],\n",
       " ['the variances in nomenclature',\n",
       "  'a general classification of the eastern alps and',\n",
       "  0.0005998813312464077,\n",
       "  0.0584898516535759],\n",
       " ['geologist stefan schmid',\n",
       "  'stefan schmid',\n",
       "  0.006689637751810356,\n",
       "  0.12440480291843414],\n",
       " ['splügen pass', 'switzerland', 1.8218218770016392e-05, 0.03258692845702171],\n",
       " ['mont blanc', 'mont blanc', 0.9912284016609192, 0.07632224261760712],\n",
       " ['4810 m', '5799 ft', 0.3554233517497778, 0.016573822125792503],\n",
       " ['piz bernina', 'mont blanc at', 8.213524557023261e-06, 0.014151716604828835],\n",
       " ['4049 metres', '13284 ft', 0.20005071370537983, 0.02144571579992771],\n",
       " ['france', 'switzerland', 0.3179873526096344, 0.00220674742013216],\n",
       " ['the jura mountains',\n",
       "  'jura mountains and',\n",
       "  0.02725207784446665,\n",
       "  0.0811355859041214],\n",
       " ['the secondary chain of the alps',\n",
       "  'secondary chain of the alps',\n",
       "  0.01197875614379503,\n",
       "  0.050639472901821136],\n",
       " ['the line of the main chain',\n",
       "  'the main chain of the main',\n",
       "  0.08609466671017192,\n",
       "  0.04931178316473961],\n",
       " ['the alps', 'the alps', 0.9881193339824677, 0.01319404598325491],\n",
       " ['hospices', 'the most', 2.152405625110987e-05, 0.004536713007837534],\n",
       " ['the summits of many of the main passes',\n",
       "  'at the summits of many of the main',\n",
       "  0.02283777519523263,\n",
       "  0.06502756476402283],\n",
       " ['col de liseran',\n",
       "  'the col de liser',\n",
       "  0.009654116595197753,\n",
       "  0.08983579277992249],\n",
       " ['the brenner pass',\n",
       "  'ötztal alps',\n",
       "  0.05996742439542757,\n",
       "  0.0034414147958159447],\n",
       " ['since the 14th century',\n",
       "  'since the 14th century',\n",
       "  0.9182472229003906,\n",
       "  0.030916232615709305],\n",
       " ['985 m 3232 ft', '985 m 3232 ft', 0.9663312964969211, 0.07598239183425903],\n",
       " ['napoleon bonaparte',\n",
       "  'napoleon bonaparte',\n",
       "  0.9330598264932632,\n",
       "  0.06046046316623688],\n",
       " ['col de liseran in savoy france',\n",
       "  'in the france answer',\n",
       "  0.011668257535439909,\n",
       "  0.06381524354219437],\n",
       " ['naturalists', 'naturalists', 0.8947186470031738, 0.07743053138256073],\n",
       " ['the 18th century',\n",
       "  '18th century',\n",
       "  0.00032389405464527954,\n",
       "  0.07421470433473587],\n",
       " ['theory of geosynclines',\n",
       "  'geosynclines',\n",
       "  0.001321622804610752,\n",
       "  0.14027351140975952],\n",
       " ['the theory of plate tectonics',\n",
       "  'plate tectonic',\n",
       "  2.5509627822354427e-05,\n",
       "  0.11920038610696793],\n",
       " ['about 300 million years ago',\n",
       "  '300 million years ago',\n",
       "  0.01640869693962941,\n",
       "  0.09219898283481598],\n",
       " ['a single tectonic plate',\n",
       "  'a single tectonic plate',\n",
       "  0.9667369027932485,\n",
       "  0.08174103498458862],\n",
       " ['the mesozoic era',\n",
       "  '300 million years ago',\n",
       "  0.0005132982659096369,\n",
       "  0.007205940783023834],\n",
       " ['jurassic period', 'mesozo', 0.07468847480650709, 0.0033821607939898968],\n",
       " ['the late cretaceous period',\n",
       "  'the late cretaceous period',\n",
       "  0.8381308515866598,\n",
       "  0.09842231124639511],\n",
       " ['marine sedimentary rocks',\n",
       "  'marine sediments were',\n",
       "  0.2865588221377634,\n",
       "  0.04175073280930519],\n",
       " ['as the rising peaks underwent erosion',\n",
       "  'as the orogeny progressed',\n",
       "  0.14790381773298375,\n",
       "  0.01867750845849514],\n",
       " ['coarse sediments', 'molasse', 5.8529263287039395e-06, 0.013412713073194027],\n",
       " ['flysch', 'flysch', 0.931998074054718, 0.03577262535691261],\n",
       " ['a latestage orogeny',\n",
       "  'a late stage orogeny',\n",
       "  0.23525952603913688,\n",
       "  0.06886591762304306],\n",
       " ['a series of tectonic events',\n",
       "  'the alpine orogeny caused the',\n",
       "  0.0017551717501885864,\n",
       "  0.014590004459023476],\n",
       " ['lithology', 'lithology', 0.7055691778659821, 0.06767155230045319],\n",
       " ['helveticum',\n",
       "  'the western eastern',\n",
       "  0.0191172404998539,\n",
       "  0.01113309245556593],\n",
       " ['geologist', 'geologist', 0.9996468126773834, 0.08991742134094238],\n",
       " ['a metamorphic event',\n",
       "  'a metamorphic event',\n",
       "  0.9867789149284363,\n",
       "  0.10057689249515533],\n",
       " ['the cretaceous period',\n",
       "  'the cenozoic',\n",
       "  0.3596994305745966,\n",
       "  0.034992411732673645],\n",
       " ['nappe formations',\n",
       "  'nappe formation',\n",
       "  0.7336345394141972,\n",
       "  0.11415179818868637],\n",
       " ['cretaceous', 'in the c', 0.03838461355918632, 0.016928451135754585],\n",
       " ['houillière zone', 'houilli', 0.0019859784461441184, 0.06729353219270706],\n",
       " ['high massifs',\n",
       "  'high masses of sediments with',\n",
       "  0.03209015604495251,\n",
       "  0.022420819848775864],\n",
       " ['eastern alps', 'eastern alps', 0.8069023489952087, 0.07377824187278748],\n",
       " ['the structure of the alps',\n",
       "  'the structure of the alps',\n",
       "  0.96271253824234,\n",
       "  0.02825302444398403],\n",
       " ['continental europe',\n",
       "  'continental origin',\n",
       "  0.5215623434633017,\n",
       "  0.058155689388513565],\n",
       " ['the african plate',\n",
       "  'the african plate',\n",
       "  0.7161245942115784,\n",
       "  0.019715765491127968],\n",
       " ['the matterhorn', 'the matterhorn', 0.6912249426047007, 0.0595666877925396],\n",
       " ['folded and fractured',\n",
       "  'erosion formed the',\n",
       "  0.024022569690741152,\n",
       "  0.015300985425710678],\n",
       " ['layers of rock from the various orogenies',\n",
       "  'the orogenies answer orogen',\n",
       "  2.9388437303537813e-06,\n",
       "  0.07121109962463379],\n",
       " ['steep vertical peaks',\n",
       "  'folded and broken',\n",
       "  0.07042662485036526,\n",
       "  0.00977998599410057],\n",
       " ['union internationale des associations dalpinisme uiaa',\n",
       "  'union internationale des associations dalpinisme uiaa',\n",
       "  0.9573324217515833,\n",
       "  0.18589967489242554],\n",
       " ['subpeaks with little prominence that are considered important mountaineering objectives',\n",
       "  'mountains lakes and small tundra answer little',\n",
       "  3.2120422609156326e-05,\n",
       "  0.04401833191514015],\n",
       " ['500 m 1640 ft', '4000 m 13123', 0.003226553100546311, 0.06656531989574432],\n",
       " ['1786', '1786', 0.9786445498466492, 0.07284876704216003],\n",
       " ['first half of the 19th century',\n",
       "  '1786',\n",
       "  0.046585983314647215,\n",
       "  0.022237058728933334],\n",
       " ['1865', '1865', 0.9995953440666199, 0.008813316002488136],\n",
       " ['the end of the golden age of alpinism',\n",
       "  'the end of the golden age of alpinism',\n",
       "  0.9636227667331696,\n",
       "  0.0986231118440628],\n",
       " ['karl blodig', 'karl blodig', 0.9987509995698929, 0.11245685815811157],\n",
       " ['1788', '1788', 0.9907866716384888, 0.07070698589086533],\n",
       " ['1819', '1788', 0.03408150903243268, 0.006639891304075718],\n",
       " ['the mid1850s', '1788', 0.0036011885069709613, 0.01706109754741192],\n",
       " ['1865', '1865', 0.9969112277030945, 0.008229733444750309],\n",
       " ['1938', '1938', 0.9361091256141663, 0.004805264063179493],\n",
       " ['minerals', 'minerals', 0.7274178862571716, 0.007302768062800169],\n",
       " ['copper', 'copper', 0.9994826316833496, 0.0019523454830050468],\n",
       " ['gold', 'copper', 0.09429159760475159, 0.0014649707591161132],\n",
       " ['highquality iron ore',\n",
       "  'high quality iron ore for',\n",
       "  0.2024770896582348,\n",
       "  0.09876655787229538],\n",
       " ['slovenia', 'slovenia', 0.8864529728889465, 0.008477200753986835],\n",
       " ['hundreds of years',\n",
       "  'hundreds of years',\n",
       "  0.8948575258255005,\n",
       "  0.05083122476935387],\n",
       " ['the 18th century',\n",
       "  '18th century',\n",
       "  0.00037447138508639455,\n",
       "  0.06291213631629944],\n",
       " ['leonhard euler', 'leonhard euler', 0.9246691465377808, 0.10637112706899643],\n",
       " ['crystal hunting',\n",
       "  'crystal hunting',\n",
       "  0.9309403896331787,\n",
       "  0.03359773010015488],\n",
       " ['david friedrich wiser',\n",
       "  'david friedrich wiser',\n",
       "  0.9470458030700684,\n",
       "  0.10951928794384003],\n",
       " ['miocene epoch', 'miocene epoch', 0.9018719643354416, 0.13120174407958984],\n",
       " ['glaciation', 'glaciation', 0.9986307919025421, 0.07821675390005112],\n",
       " ['louis agassiz', 'louis agassiz', 0.9860479533672333, 0.10576286166906357],\n",
       " ['the father of the iceage concept',\n",
       "  'father of the iceage concept',\n",
       "  0.006630302420782793,\n",
       "  0.09581954032182693],\n",
       " ['the unteraar glacier',\n",
       "  'unterrear glacier',\n",
       "  0.33743276863644117,\n",
       "  0.07511865347623825],\n",
       " ['100 m 328 ft', '100 m 32 ft', 0.8028457860151926, 0.09174506366252899],\n",
       " ['the middle', 'the middle', 0.9346563518047333, 0.013885567896068096],\n",
       " ['the inn valley', 'eroded', 0.03619929170599145, 0.008715505711734295],\n",
       " ['eroded rocks from the most recent ice age',\n",
       "  'eroded rocks from the most recent ice age',\n",
       "  0.9668967008590699,\n",
       "  0.0879756435751915],\n",
       " ['erosion from earlier ice ages',\n",
       "  'erosion and formation of valleys',\n",
       "  0.21832324297345734,\n",
       "  0.01859244517982006],\n",
       " ['piles of rock picked up during the movement of the glacier',\n",
       "  'piles of rock picked up at the edge of the valley',\n",
       "  0.6314960631123447,\n",
       "  0.09523136168718338],\n",
       " ['at edges center and the terminus of glaciers',\n",
       "  'at the edge center and the terminus of glaciers',\n",
       "  0.09182358157507603,\n",
       "  0.11287382990121841],\n",
       " ['spread in a fanlike shape',\n",
       "  'a like a curtains of ice',\n",
       "  0.08454724030407985,\n",
       "  0.03892814740538597],\n",
       " ['the stress of the movement',\n",
       "  'the stresses of the movement',\n",
       "  0.6316341519355774,\n",
       "  0.03419572860002518],\n",
       " ['unpredictable and dangerous crevasses',\n",
       "  'unpredictable and dangerous crevasses',\n",
       "  0.9698885877927145,\n",
       "  0.10914109647274017],\n",
       " ['a piece of glacier will detach or break',\n",
       "  'glacier breaking in river or glacier answer',\n",
       "  0.0014594475636439618,\n",
       "  0.0548873133957386],\n",
       " ['an avalanche', 'an avalanche', 0.9151868224143982, 0.037462349981069565],\n",
       " ['120 homes', '120', 0.6683351099491119, 0.06676235049962997],\n",
       " ['high levels of precipitation',\n",
       "  'high levels of precipitation',\n",
       "  0.9418129324913025,\n",
       "  0.05059831216931343],\n",
       " ['to 1342 km2 518 sq mi',\n",
       "  '1332 km2 518 sq mi by',\n",
       "  0.005025033101412935,\n",
       "  0.09207562357187271],\n",
       " ['decreased river runoff levels',\n",
       "  'decreased river runoff levels',\n",
       "  0.817680469581059,\n",
       "  0.09759934991598129],\n",
       " ['forty percent', '30 percent', 0.2592744868597947, 0.015401603654026985],\n",
       " ['30', '30', 0.7573608756065369, 0.06651997566223145],\n",
       " ['the alps', 'the alps', 0.9214522838592529, 0.03474185988306999],\n",
       " ['11 percent of the surface area',\n",
       "  'about 11 percent of europe',\n",
       "  0.010849628983123694,\n",
       "  0.04542907327413559],\n",
       " ['90 percent', '90 percent', 0.9906456768512726, 0.06713642179965973],\n",
       " ['80 percent', '80 percent', 0.9894581139087677, 0.07028470188379288],\n",
       " ['500', '500', 0.9965443015098572, 0.005084348376840353],\n",
       " ['switzerland', 'switzerland', 0.9964200258255005, 0.0014681555330753326],\n",
       " ['the alps', 'the mediterranean', 0.33584608510136604, 0.004015643149614334],\n",
       " ['the rhone', 'the river', 0.3363893276244596, 0.02443516068160534],\n",
       " ['glacial meltwater',\n",
       "  'glacial meltwater',\n",
       "  0.8236216083168983,\n",
       "  0.09165512770414352],\n",
       " ['a 30 square kilometre area in switzerland',\n",
       "  'switzerland',\n",
       "  4.998756132579274e-06,\n",
       "  0.038551077246665955],\n",
       " ['germany', 'on', 6.6401986131836566e-09, 0.00376494019292295],\n",
       " ['south side', 'the south', 0.13770416844636202, 0.007695196662098169],\n",
       " ['lakes', 'lakes', 0.9994551539421082, 0.007739854510873556],\n",
       " ['scientists', 'scientists', 0.8613308668136597, 0.013903371058404446],\n",
       " ['snowmaking in the ski resorts',\n",
       "  'snowmaking in the ski resort',\n",
       "  0.5834664827367911,\n",
       "  0.09068556874990463],\n",
       " ['unknown', 'decreased', 2.0860822402823942e-08, 0.004019829444587231],\n",
       " ['the alps', 'the alps', 0.8641488254070282, 0.009631695225834846],\n",
       " ['alpine', 'alpine', 0.9987995624542236, 0.002417837968096137],\n",
       " ['a rise from sea level into the upper regions of the atmosphere',\n",
       "  'mountain level into the upper regions of the atmosphere causes what',\n",
       "  0.009659815228532912,\n",
       "  0.05100370571017265],\n",
       " ['the height of the alps',\n",
       "  'height of the alps',\n",
       "  0.0335275275280992,\n",
       "  0.049368716776371],\n",
       " ['the 18th century',\n",
       "  '18th century',\n",
       "  0.06183763822241417,\n",
       "  0.07716935873031616],\n",
       " ['the weather patterns',\n",
       "  'severe weather',\n",
       "  0.26274819175402325,\n",
       "  0.02523721754550934],\n",
       " ['numerous weather stations',\n",
       "  'weather patterns such',\n",
       "  0.0009276025723282214,\n",
       "  0.02435835264623165],\n",
       " ['italy', 'italy', 0.993180513381958, 0.006239436566829681],\n",
       " ['switzerland', 'france', 0.2000586837530136, 0.010133927688002586],\n",
       " ['the areas that are not arid and receive high precipitation',\n",
       "  'areas that are not arid are receiving high rainfall',\n",
       "  0.00045466047163054843,\n",
       "  0.0731484666466713],\n",
       " ['2600 mm 100 in per year to 3600 mm 140 in per year',\n",
       "  '2600 mm per year with high rainfall in the alps answer 3600 mm per',\n",
       "  0.20431357868956065,\n",
       "  0.07706156373023987],\n",
       " ['high altitudes', 'at altitudes', 0.7019365727901459, 0.019255533814430237],\n",
       " ['at altitudes between 1000 and 3000 m',\n",
       "  '1000 m 3281 ft per year',\n",
       "  1.2478608520568221e-05,\n",
       "  0.039819397032260895],\n",
       " ['five climatic zones', 'five', 0.49695018430793425, 0.028607014566659927],\n",
       " ['colline zone', 'colline zone', 0.8480022152264913, 0.07897374033927917],\n",
       " ['between 500 and 1000 m',\n",
       "  'between 500 to 1000 ft',\n",
       "  0.7743995530264718,\n",
       "  0.047940827906131744],\n",
       " ['from 800 to 1700 m',\n",
       "  '800 to 1700 m',\n",
       "  1.4133250852697249e-05,\n",
       "  0.04978485032916069],\n",
       " ['from 1600 to 2400 m',\n",
       "  '800 to 1700 m 2',\n",
       "  0.00043157696914125014,\n",
       "  0.02343820594251156],\n",
       " ['various models of climate change',\n",
       "  'temperature increase snowfall',\n",
       "  0.0022444632683146894,\n",
       "  0.04738510772585869],\n",
       " ['increased temperatures',\n",
       "  'increased temperatures',\n",
       "  0.9730319380760193,\n",
       "  0.08151978999376297],\n",
       " ['climate change', 'the climate', 0.03548168124689255, 0.01886197179555893],\n",
       " ['thirteen thousand',\n",
       "  'thirteen thousand',\n",
       "  2.337700626746375e-05,\n",
       "  0.04199448227882385],\n",
       " ['by habitat and soil type',\n",
       "  'by habitat and soil type',\n",
       "  0.8821994304656983,\n",
       "  0.05845172330737114],\n",
       " ['chief deciduous trees',\n",
       "  'deciduous trees',\n",
       "  1.483589539849183e-06,\n",
       "  0.05907328054308891],\n",
       " ['a band of short pine trees',\n",
       "  'short pine trees answer',\n",
       "  3.873223936167365e-06,\n",
       "  0.11151202768087387],\n",
       " ['acidic soil', 'acidic', 0.49183097295463085, 0.06680944561958313],\n",
       " ['alpine', 'al', 0.00039255699812201783, 0.003347648773342371],\n",
       " ['because of regional fluctuations in tree lines',\n",
       "  'regional variations in trees answer',\n",
       "  0.0015362611047638867,\n",
       "  0.06879287958145142],\n",
       " ['alpine plants such the alpine gentian',\n",
       "  'alpine',\n",
       "  0.6214687004685402,\n",
       "  0.09334701299667358],\n",
       " ['the illyrian king gentius',\n",
       "  'the illyrian king',\n",
       "  0.5901408678254744,\n",
       "  0.07338933646678925],\n",
       " ['40 species', '40', 0.5365499258041382, 0.05469471588730812],\n",
       " ['isolated cushions', 'cushions', 7.611676441735918e-07, 0.07817281782627106],\n",
       " ['above 4000 m', '4000 ft', 0.0002825230645498777, 0.04798680171370506],\n",
       " ['the king of the alps',\n",
       "  'king of the alps',\n",
       "  0.029752101694322165,\n",
       "  0.04578721150755882],\n",
       " ['edelweiss', 'edelweiss', 0.9433575868606567, 0.08084141463041306],\n",
       " ['extreme and stressful climatic conditions',\n",
       "  'extreme and stresses make it difficult',\n",
       "  0.3259907611858101,\n",
       "  0.0727447047829628],\n",
       " ['medicinal', 'secondary', 0.03145236149430275, 0.009828083217144012],\n",
       " ['the alps', 'the alps', 0.9117085337638855, 0.038097307085990906],\n",
       " ['human interference',\n",
       "  'human interference',\n",
       "  0.9645849764347076,\n",
       "  0.057291075587272644],\n",
       " ['forests of deciduous trees',\n",
       "  'the trees in the austrian alps',\n",
       "  0.00240571882440902,\n",
       "  0.026250874623656273],\n",
       " ['the vegetation', 'the vegetation', 0.9236868917942047, 0.04044115170836449],\n",
       " ['the underlying tundra',\n",
       "  'the original tundra',\n",
       "  0.7474507227540016,\n",
       "  0.053573381155729294],\n",
       " ['30000 species', '30000', 0.7557126376777887, 0.07815761864185333],\n",
       " ['made adaptations',\n",
       "  'adaptations to',\n",
       "  0.0010262517898809165,\n",
       "  0.07413798570632935],\n",
       " ['directly above or below the snow line',\n",
       "  'microclimates of snow fle',\n",
       "  0.05797689969392494,\n",
       "  0.06212626397609711],\n",
       " ['alpine ibex', 'alpine ibex', 0.9808303713798523, 0.07547111064195633],\n",
       " ['as high as 3000 m 9843 ft',\n",
       "  '3000 m 9843 ft answer',\n",
       "  0.014576078299463165,\n",
       "  0.08543230593204498],\n",
       " ['in caves', 'in the', 0.20426643081009388, 0.009604434482753277],\n",
       " ['chamois', 'chamois', 0.9996424714724222, 0.08545293658971786],\n",
       " ['1792', '1792', 0.9906039237976074, 0.06445305049419403],\n",
       " ['underground', 'underground', 0.9844844341278076, 0.0030990242958068848],\n",
       " ['almost exclusively above the tree line as high as 2700 m 8858 ft',\n",
       "  'above the tree line answer 2700 m 858 ft',\n",
       "  4.7481365914627755e-05,\n",
       "  0.12165474891662598],\n",
       " ['beneath the alpine pastures',\n",
       "  'in large colonies they can be',\n",
       "  0.014277182843334651,\n",
       "  0.009834853932261467],\n",
       " ['the alpine chough',\n",
       "  'alpine chough',\n",
       "  0.00040224196932274746,\n",
       "  0.069660983979702],\n",
       " ['they cannot bear the cold temperatures',\n",
       "  'they cannot bear what',\n",
       "  0.5592475262283066,\n",
       "  0.06065116077661514],\n",
       " ['they hibernate underground',\n",
       "  'adders and vipers',\n",
       "  1.7604247189954383e-09,\n",
       "  0.013959543779492378],\n",
       " ['by giving birth to fully developed young rather than laying eggs',\n",
       "  'by giving birth to fully developed young answer',\n",
       "  0.6303223101384998,\n",
       "  0.09030744433403015],\n",
       " ['brown trout', 'moll', 6.8331443693292e-05, 0.00832320936024189],\n",
       " ['120 million years',\n",
       "  '120 million years',\n",
       "  0.8407587210337321,\n",
       "  0.06632598489522934],\n",
       " ['blue moths', 'blue moths', 0.9560883839925131, 0.053292661905288696],\n",
       " ['1800 m 5906 ft', '1800 m 5906 ft', 0.9035942717032, 0.05768129602074623],\n",
       " ['rosalia alpina',\n",
       "  'spiders answer',\n",
       "  3.421131055940801e-05,\n",
       "  0.010985933244228363],\n",
       " ['having been indigenous to the area',\n",
       "  'indigenous to the area',\n",
       "  0.00599021320743427,\n",
       "  0.08236335963010788],\n",
       " ['the alpine orogeny',\n",
       "  'as far back as the',\n",
       "  3.1286703341220443e-06,\n",
       "  0.02161388285458088],\n",
       " ['valais switzerland',\n",
       "  'valais switzerland',\n",
       "  0.5115989788330353,\n",
       "  0.12590521574020386],\n",
       " ['the 1970s', '1970s', 0.09789282620790137, 0.09491842985153198],\n",
       " ['about 10000 years ago',\n",
       "  'around 10000 years ago',\n",
       "  0.8469097539782524,\n",
       "  0.05511079356074333],\n",
       " ['evidence of human habitation',\n",
       "  'evidence of human habitation',\n",
       "  0.7936419792938978,\n",
       "  0.06754746288061142],\n",
       " ['to keep them dry',\n",
       "  'to keep them dry',\n",
       "  0.9930973798036575,\n",
       "  0.06574201583862305],\n",
       " ['standing stones',\n",
       "  'standing stones',\n",
       "  0.5057710739783943,\n",
       "  0.023842886090278625],\n",
       " ['more than 5000 years old',\n",
       "  '5000 years old',\n",
       "  0.08574320300318405,\n",
       "  0.07345311343669891],\n",
       " ['a mummy of a neolithic body',\n",
       "  'a hunter with a primitive appearance',\n",
       "  0.2783399454978946,\n",
       "  0.04704584553837776],\n",
       " ['in 1991', '1991', 1.730924168441561e-07, 0.05748596042394638],\n",
       " ['the similaun glacier',\n",
       "  'similaun glacier',\n",
       "  0.000627943050303896,\n",
       "  0.10968446731567383],\n",
       " ['between 1000 to 1500 bc',\n",
       "  'between 1000 to 1500 bc',\n",
       "  0.9828184962272644,\n",
       "  0.0738890990614891],\n",
       " ['the eastern regions',\n",
       "  'the eastern regions',\n",
       "  0.9188699722290039,\n",
       "  0.05461610481142998],\n",
       " ['the west', 'the rh', 0.5366149097681046, 0.007261510007083416],\n",
       " ['salt', 'salt', 0.791878342628479, 0.0011990698985755444],\n",
       " ['the celts', 'the celts', 0.98038117090861, 0.06714622676372528],\n",
       " ['218 bc', '218 bc', 0.991502583026886, 0.06919761747121811],\n",
       " ['hannibal', 'hannibal', 0.9998775720596313, 0.0067663248628377914],\n",
       " ['roads', 'road', 0.12369991838932037, 0.0034096725285053253],\n",
       " ['roman road markers',\n",
       "  'road markers can',\n",
       "  0.02295900240036038,\n",
       "  0.0890304371714592],\n",
       " ['the roman expansion',\n",
       "  'roman expansion brought',\n",
       "  0.02003925675656622,\n",
       "  0.06676122546195984],\n",
       " ['in 121 bc', '121 bc', 0.00044724149895499404, 0.07895447313785553],\n",
       " ['58 bc', '58 bc', 0.8536134362220764, 0.05274892598390579],\n",
       " ['germanic tribes',\n",
       "  'germanic tribes',\n",
       "  0.9152008096377054,\n",
       "  0.07048916816711426],\n",
       " ['romans', 'romans', 0.962069571018219, 0.01977386139333248],\n",
       " ['feudalism', 'feudalism', 0.995357871055603, 0.07174430042505264],\n",
       " ['castello del buonconsiglio in trento italy',\n",
       "  'château de chardon',\n",
       "  4.172439721413564e-05,\n",
       "  0.0463423877954483],\n",
       " ['château de chillon',\n",
       "  'château de chillon',\n",
       "  0.9482829655919757,\n",
       "  0.14521275460720062],\n",
       " ['power struggles',\n",
       "  'power struggles',\n",
       "  0.8945049941539764,\n",
       "  0.05748792737722397],\n",
       " ['northern italy', 'northern italy', 0.9937004446983337, 0.05720413103699684],\n",
       " ['the house of habsburg',\n",
       "  'the visconti in',\n",
       "  0.13888623678487821,\n",
       "  0.0149897625669837],\n",
       " ['switzerland', 'switzerland', 0.9983224272727966, 0.00684234406799078],\n",
       " ['the napoleonic wars',\n",
       "  'napoleonic wars',\n",
       "  9.107802209945559e-07,\n",
       "  0.15566062927246094],\n",
       " ['napoleon', 'nap', 0.3442555069923401, 0.006170184351503849],\n",
       " ['1798', '1798', 0.9990679919719696, 0.07250279188156128],\n",
       " ['switzerland', 'switzerland', 0.9962283372879028, 0.006048719398677349],\n",
       " ['after the fall of napoléon',\n",
       "  'after the fall of napoléon',\n",
       "  0.8536621034145355,\n",
       "  0.06726939976215363],\n",
       " ['savoy', 'savoy', 0.6454258859157562, 0.060985904186964035],\n",
       " ['to protect the major alpine passes',\n",
       "  'to protect the major alpine passes',\n",
       "  0.9024824159485954,\n",
       "  0.058073319494724274],\n",
       " ['napoléon bonaparte',\n",
       "  'napoléon bonaparte',\n",
       "  0.9636942744255066,\n",
       "  0.08339273184537888],\n",
       " ['the monasteries built in the high alps',\n",
       "  'the monasteries built in the high alps',\n",
       "  0.9374108778105842,\n",
       "  0.0742049366235733],\n",
       " ['the benedictines',\n",
       "  'the benedictines',\n",
       "  0.9596781730651855,\n",
       "  0.0621572881937027],\n",
       " ['the augustinians',\n",
       "  'augustinians',\n",
       "  0.026513677685712334,\n",
       "  0.08865254372358322],\n",
       " ['the 9th or 10th centuries',\n",
       "  '9th century answer 9',\n",
       "  0.0027375822652192655,\n",
       "  0.042546339333057404],\n",
       " ['50000 years', '50000 years', 0.9954329431056976, 0.055167753249406815],\n",
       " ['the high peaks were visited by prehistoric people',\n",
       "  'the high peaks were visited by what people',\n",
       "  0.6751442346721888,\n",
       "  0.05675720050930977],\n",
       " ['seven bear skulls',\n",
       "  'seven bear skulls',\n",
       "  0.6143420586983362,\n",
       "  0.05615277588367462],\n",
       " ['alpine passes', 'alpine passes', 0.8881359100341797, 0.06649091094732285],\n",
       " ['france', 'france', 0.9988872408866882, 0.0015690610744059086],\n",
       " ['climb mont aiguille',\n",
       "  'climb mont aiguille',\n",
       "  0.8840905129909515,\n",
       "  0.07992550730705261],\n",
       " ['a bronze triptych of three crosses',\n",
       "  'a triptych of crosses answer',\n",
       "  0.12942017910249534,\n",
       "  0.07503429800271988],\n",
       " ['1492', '1492', 0.9825097322463989, 0.057116273790597916],\n",
       " ['18th century', '18th century', 0.943668782711029, 0.08508142828941345],\n",
       " ['conrad gessner', 'conrad gessner', 0.9520647376775742, 0.09225745499134064],\n",
       " ['geneva', 'penn', 0.0004177457594778389, 0.0025321205612272024],\n",
       " ['saussure', 'saussure', 0.9905904332796732, 0.03221246227622032],\n",
       " ['jeanjacques rousseau',\n",
       "  'jeanjacques rousseau',\n",
       "  0.8226967006921768,\n",
       "  0.09417156875133514],\n",
       " ['albrecht von haller', 'haller', 0.031385742720167954, 0.08367731422185898],\n",
       " ['after the end of the napoleonic wars',\n",
       "  '1790',\n",
       "  0.012995540760092203,\n",
       "  0.022566065192222595],\n",
       " ['the sublime effects of monumental nature',\n",
       "  'sublime effects of answer',\n",
       "  0.026000994411041034,\n",
       "  0.07563836872577667],\n",
       " ['geneva', 'the', 0.0013708205660805106, 0.0045596035197377205],\n",
       " ['mont blanc', 'mont blanc', 0.9735322594642639, 0.011786953546106815],\n",
       " ['the mid19th century',\n",
       "  'mid19th century',\n",
       "  0.0032521535096661225,\n",
       "  0.08517050743103027],\n",
       " ['austrian', 'ober', 0.0003336600202601403, 0.006590025499463081],\n",
       " ['the alps', 'the alps', 0.8291573226451874, 0.011480916291475296],\n",
       " ['the obersalzberg region',\n",
       "  'obersalzberg region outside',\n",
       "  0.01119893280803863,\n",
       "  0.10043461620807648],\n",
       " ['1923', '1923', 0.9976058006286621, 0.0056947944685816765],\n",
       " ['the third reich',\n",
       "  'the third reich',\n",
       "  0.7466315527757009,\n",
       "  0.029030967503786087],\n",
       " ['austria', 'austria', 0.9989417195320129, 0.004100422374904156],\n",
       " ['switzerland', 'switzerland', 0.8360003232955933, 0.0016931340796872973],\n",
       " ['the swiss commanders',\n",
       "  'general eisenhower',\n",
       "  0.010197767907070462,\n",
       "  0.008817970752716064],\n",
       " ['ski troops', 'scouts for', 0.0845550656845262, 0.016695398837327957],\n",
       " ['italy', 'italy', 0.9260783195495605, 0.00223076855763793],\n",
       " ['austria', 'austria', 0.9236065745353699, 0.0020004347898066044],\n",
       " ['the salt mines surrounding the altaussee area',\n",
       "  'altaussee area',\n",
       "  8.343028094032042e-05,\n",
       "  0.11196472495794296],\n",
       " ['14 million', '14 million', 0.9048424363136292, 0.07512293010950089],\n",
       " ['manufacturing and service jobs',\n",
       "  'manufacturing and service jobs',\n",
       "  0.9636377543210983,\n",
       "  0.07574747502803802],\n",
       " ['alpine culture', 'alpine culture', 0.993373304605484, 0.06927737593650818],\n",
       " ['alpine culture', 'much of', 0.020336909088655375, 0.018926816061139107],\n",
       " ['the medieval period',\n",
       "  'the medieval period',\n",
       "  0.6835405826568604,\n",
       "  0.07058750838041306],\n",
       " ['carpentry', 'woodcar', 0.007533167406791108, 0.01861235871911049],\n",
       " ['farming', 'farming', 0.010371197946369648, 0.001436517108231783],\n",
       " ['tourism', 'the', 0.00024084663891699165, 0.002099669771268964],\n",
       " ['because of the steep and rocky topography of the alps',\n",
       "  'the rocky topography of the alps made it difficult for',\n",
       "  0.014009574538087472,\n",
       "  0.064873106777668],\n",
       " ['midjune', 'june', 0.21436113777053833, 0.028087429702281952],\n",
       " ['cheesemaking', 'cheesemaking', 0.9860124588012695, 0.07928810268640518],\n",
       " ['up to 45 kg 100 lb', '45 kg', 0.0006019849541871736, 0.08812364190816879],\n",
       " ['haymaking', 'haymaking', 0.9822469055652618, 0.05102800205349922],\n",
       " ['twice', 'twice', 0.7567934989929199, 0.0016677690437063575],\n",
       " ['before lent', 'before', 0.5426700077950954, 0.05095149949193001],\n",
       " ['medieval designs that withstand cold winters',\n",
       "  'medieval designs answer',\n",
       "  0.29863666727468496,\n",
       "  0.07807376235723495],\n",
       " ['the stube', 'the stube', 0.9707298278808594, 0.04136329144239426],\n",
       " ['the bernese oberland',\n",
       "  'bernese oberland',\n",
       "  9.239388220197778e-05,\n",
       "  0.11440539360046387],\n",
       " ['south or downhill',\n",
       "  'south or downhill',\n",
       "  0.9392598271369934,\n",
       "  0.06376391649246216],\n",
       " ['solid wood', 'solid wood', 0.9988729059696198, 0.06111104041337967],\n",
       " ['the stube', 'stube', 0.003354870868526779, 0.10132922232151031],\n",
       " ['the stube', 'in the middle', 0.02878795345780466, 0.016831180080771446],\n",
       " ['carved wooden plates',\n",
       "  'carved wooden plates',\n",
       "  0.9542638858159384,\n",
       "  0.08666082471609116],\n",
       " ['elaborately', 'carved and', 8.838395899601892e-06, 0.01213109865784645],\n",
       " ['alpine rocks', 'alpine rocks', 0.991677314043045, 0.063839852809906],\n",
       " ['the higher parts of the valleys',\n",
       "  'in the maurienne valley in',\n",
       "  0.06792313007677346,\n",
       "  0.022241469472646713],\n",
       " ['40', '40', 0.8363754749298096, 0.04523152858018875],\n",
       " ['19', '19', 0.9994584918022156, 0.0013026471715420485],\n",
       " ['romansh', 'romansh', 0.8428587317466736, 0.04642375558614731],\n",
       " ['the alps', 'alps', 0.03425449039787054, 0.10518010705709457],\n",
       " ['austria', 'bav', 0.08427482098340988, 0.003204312641173601],\n",
       " ['over 120 million',\n",
       "  '120 million',\n",
       "  0.014582427612746566,\n",
       "  0.07221078127622604],\n",
       " ['tourism', 'tourism', 0.9720465540885925, 0.003345837350934744],\n",
       " ['the early 19th century',\n",
       "  'the first tourist attraction in',\n",
       "  0.12521067957370463,\n",
       "  0.015758926048874855],\n",
       " ['during the belle époque',\n",
       "  'during the belle époque',\n",
       "  0.9468844334284464,\n",
       "  0.08741497248411179],\n",
       " ['early in the 20th century',\n",
       "  'the 20th century',\n",
       "  0.0027234840902482147,\n",
       "  0.04073789715766907],\n",
       " ['1882', '1882', 0.9743558168411255, 0.06371358036994934],\n",
       " ['st moritz', 'st moritz', 0.9975408464670181, 0.07667043805122375],\n",
       " ['chamonix france',\n",
       "  'the olympic winter olympics in',\n",
       "  0.00911866621249402,\n",
       "  0.013643011450767517],\n",
       " ['st moritz switzerland',\n",
       "  'st moritz',\n",
       "  0.7265343951061368,\n",
       "  0.06578471511602402],\n",
       " ['garmischpartenkirchen germany',\n",
       "  'st moritz',\n",
       "  6.250780873008621e-06,\n",
       "  0.02043161354959011],\n",
       " ['1930', '1930', 0.9874417781829834, 0.001747208065353334],\n",
       " ['1992', '1964 answer', 5.904480635384364e-06, 0.0038666375912725925],\n",
       " ['postworld war i',\n",
       "  'postworld war i',\n",
       "  0.940612006187439,\n",
       "  0.06933870166540146],\n",
       " ['the 1970s', '1970s', 0.10255559519442177, 0.06864559650421143],\n",
       " ['france', 'france', 0.9326626062393188, 0.0012471377849578857],\n",
       " ['4200 km 2600 mi',\n",
       "  '4200 km 2600 mi',\n",
       "  0.9904182612895965,\n",
       "  0.09909280389547348],\n",
       " ['6 million', '6 million', 0.8094739317893982, 0.056678254157304764],\n",
       " ['switzerland', 'switzerland', 0.5771623253822327, 0.0028056360315531492],\n",
       " ['57 km', '57 km', 0.9630017280578613, 0.06631223112344742],\n",
       " ['france', 'france', 0.866616427898407, 0.004670095629990101],\n",
       " ['switzerland', 'switzerland', 0.40194809436798096, 0.004683205392211676],\n",
       " ['reasons of sustainability',\n",
       "  'sustainability of the',\n",
       "  0.3491546548348386,\n",
       "  0.0666515901684761],\n",
       " ['winter', 'winter', 0.9225072860717773, 0.0032469758298248053],\n",
       " ['motorways', 'motorways', 0.9943214356899261, 0.06429362297058105],\n",
       " ['mountain passes',\n",
       "  'mountain passes',\n",
       "  0.6154034733772278,\n",
       "  0.06090625375509262],\n",
       " ['a locus or region of dna that encodes a functional rna or protein product',\n",
       "  'locus of dna that encodes a functional rna or protein product answer gene',\n",
       "  0.006951944044467235,\n",
       "  0.15227045118808746],\n",
       " ['the transmission of genes to an organisms offspring',\n",
       "  'polygenes',\n",
       "  0.0008587741174014563,\n",
       "  0.022444406524300575],\n",
       " ['polygenes many different genes',\n",
       "  'polygenes and what answer',\n",
       "  0.3720021461870226,\n",
       "  0.0650792047381401],\n",
       " ['eye colour or number of limbs',\n",
       "  'eye colour answer number',\n",
       "  0.3015620619986608,\n",
       "  0.06405942887067795],\n",
       " ['blood type risk for specific diseases or the thousands of basic biochemical processes that comprise life',\n",
       "  'eye color or number of limbs',\n",
       "  0.04039231737512074,\n",
       "  0.11939483880996704],\n",
       " ['different variants known as alleles',\n",
       "  'different phenotypes',\n",
       "  0.33020982222049405,\n",
       "  0.040001846849918365],\n",
       " ['encode slightly different versions of a protein',\n",
       "  'cause different phenotypes answer',\n",
       "  0.0015572058691960661,\n",
       "  0.027839861810207367],\n",
       " ['different phenotype traits',\n",
       "  'different phenotypes',\n",
       "  0.34569902285861787,\n",
       "  0.03614438325166702],\n",
       " ['having a different allele of the gene',\n",
       "  'having a different gene e',\n",
       "  0.399014672153624,\n",
       "  0.049445997923612595],\n",
       " ['natural selection or survival of the fittest of the alleles',\n",
       "  'natural selection or survival of what answer genes',\n",
       "  0.3167717050929563,\n",
       "  0.11070304363965988],\n",
       " ['its coding regions',\n",
       "  'its coding regions',\n",
       "  0.9363610943158468,\n",
       "  0.054573483765125275],\n",
       " ['several exons', 'exons of', 6.412020007388423e-05, 0.08915864676237106],\n",
       " ['rna', 'rna', 0.999925971031189, 0.004825290758162737],\n",
       " ['functional noncoding rnas',\n",
       "  'gene products are not what type of',\n",
       "  0.05362198073807477,\n",
       "  0.020467614755034447],\n",
       " ['any discrete locus of heritable genomic sequence which affect an organisms traits by being expressed as a functional product',\n",
       "  'any single gene that affects an organisms traits by being expressed as what answer functional product or gene',\n",
       "  0.04524704668491698,\n",
       "  0.13197600841522217],\n",
       " ['gregor mendel 1822–1884',\n",
       "  'gregor mendel',\n",
       "  0.8046540968120098,\n",
       "  0.12812761962413788],\n",
       " ['n is the number of differing characteristics in the original peas',\n",
       "  'number of different characteristics in original pea plants',\n",
       "  0.024294298351463173,\n",
       "  0.04820580780506134],\n",
       " ['independent assortment',\n",
       "  'distinct inherited',\n",
       "  0.08660021148853048,\n",
       "  0.006625666283071041],\n",
       " ['the distinction between dominant and recessive traits',\n",
       "  'inheritance patterns',\n",
       "  0.004305183544938511,\n",
       "  0.030850842595100403],\n",
       " ['discrete inherited units that give rise to observable physical characteristics',\n",
       "  'discrete inherited units gave rise to observable physical characteristics',\n",
       "  0.26140691158719653,\n",
       "  0.08160000294446945],\n",
       " ['one of blending inheritance',\n",
       "  'blending inheritance which',\n",
       "  0.0003833651057618681,\n",
       "  0.06174342706799507],\n",
       " ['charles darwin', 'charles darwin', 0.9996070265769958, 0.06337616592645645],\n",
       " ['hypothetical particles that would mix during reproduction',\n",
       "  'hypothetical particles that would mix during reproduction',\n",
       "  0.9605992862156459,\n",
       "  0.1002686470746994],\n",
       " ['1866', '1866', 0.8426892161369324, 0.05796201527118683],\n",
       " ['hugo de vries carl correns and erich von tschermak',\n",
       "  'hugo de vries carl correns and erich von tschermak',\n",
       "  0.46201223009602105,\n",
       "  0.15490402281284332],\n",
       " ['γένος génos', 'γένος génos', 0.9829824070135752, 0.1545417308807373],\n",
       " ['race offspring',\n",
       "  'race offspring',\n",
       "  0.02302316273074806,\n",
       "  0.07947777211666107],\n",
       " ['wilhelm johannsen',\n",
       "  'wilhelm johannsen',\n",
       "  0.9445410569508871,\n",
       "  0.10910054296255112],\n",
       " ['the fundamental physical and functional unit of heredity',\n",
       "  'the physical and functional unit of heredity',\n",
       "  0.09688394538156357,\n",
       "  0.12092502415180206],\n",
       " ['william bateson',\n",
       "  'william bateson',\n",
       "  0.9610979954401652,\n",
       "  0.10054516792297363],\n",
       " ['deoxyribonucleic acid dna',\n",
       "  'deoxyribonucleic acid dna',\n",
       "  0.9530877411365509,\n",
       "  0.13816465437412262],\n",
       " ['rosalind franklin',\n",
       "  'james d watson',\n",
       "  0.031056253679361906,\n",
       "  0.00736577995121479],\n",
       " ['james d watson and francis crick',\n",
       "  'james d watson and francis crick',\n",
       "  0.99327702075243,\n",
       "  0.08968420326709747],\n",
       " ['reverse transcription in retroviruses',\n",
       "  'reverse transcription in what answer',\n",
       "  0.40835373797616503,\n",
       "  0.06313946098089218],\n",
       " ['molecular genetics', 'molecular', 0.6607290953397751, 0.029623428359627724],\n",
       " ['in 1972', '1972', 0.00012322811835474567, 0.07304467260837555],\n",
       " ['the gene for bacteriophage ms2 coat protein',\n",
       "  'bacteriophage ms2',\n",
       "  6.845512890039582e-07,\n",
       "  0.1441003382205963],\n",
       " ['frederick sanger',\n",
       "  'frederick sanger',\n",
       "  0.9956294099489847,\n",
       "  0.07992143929004669],\n",
       " ['improved the efficiency of sequencing and turned it into a routine laboratory tool',\n",
       "  'turned it into a complete program',\n",
       "  0.0066949407787665755,\n",
       "  0.09705705940723419],\n",
       " ['the human genome project',\n",
       "  'human genome project',\n",
       "  0.02238042127891049,\n",
       "  0.11723577976226807],\n",
       " ['the 1930s and 1940s',\n",
       "  '1930s and 1940s',\n",
       "  0.007348230410936907,\n",
       "  0.09807094186544418],\n",
       " ['the modern evolutionary synthesis',\n",
       "  'modern evolutionary synthesis',\n",
       "  0.07353296402880005,\n",
       "  0.08412297070026398],\n",
       " ['george c williams',\n",
       "  'george c williams',\n",
       "  0.9790349453687668,\n",
       "  0.069120854139328],\n",
       " ['that which segregates and recombines with appreciable frequency',\n",
       "  'that which preserves and recombines with appreciable frequency',\n",
       "  0.21443571726217614,\n",
       "  0.10350441187620163],\n",
       " ['richard dawkins', 'richard', 0.5664462894201279, 0.06913592666387558],\n",
       " ['long strands of dna deoxyribonucleic acid',\n",
       "  'long',\n",
       "  0.2916923869634047,\n",
       "  0.0841822549700737],\n",
       " ['a chain made from four types of nucleotide subunits',\n",
       "  'a chain of nucleotide subunit',\n",
       "  0.19778863274100936,\n",
       "  0.11684387922286987],\n",
       " ['a fivecarbon sugar 2deoxyribose',\n",
       "  'guanine answer guanine answer gu',\n",
       "  2.379926071206849e-05,\n",
       "  0.04587646201252937],\n",
       " ['adenine cytosine guanine and thymine',\n",
       "  'a fivecarbon sugar a phosphate group and one of the bases',\n",
       "  2.60818207728733e-07,\n",
       "  0.07883957773447037],\n",
       " ['a phosphate group', 'guanine', 2.690860386508782e-05, 0.011366850696504116],\n",
       " ['phosphatesugar',\n",
       "  'phosphate base',\n",
       "  0.3805687433772012,\n",
       "  0.053707581013441086],\n",
       " ['adenine', 'adenine', 0.9892030358314514, 0.047319840639829636],\n",
       " ['cytosine', 'adenine', 0.33476547665371, 0.006242811679840088],\n",
       " ['adenine and thymine align form two hydrogen bonds whereas cytosine and guanine form three hydrogen bonds',\n",
       "  'adenine and guanine are in what kind of relationship answer hydrogen bonds',\n",
       "  0.1596042447758513,\n",
       "  0.0821218267083168],\n",
       " ['the two strands in a double helix must therefore be complementary',\n",
       "  'complementary with their sequence of bases matching answer',\n",
       "  0.00034014448038225146,\n",
       "  0.03776003420352936],\n",
       " ['the chemical composition of the pentose residues of the bases',\n",
       "  'chemical composition of bases',\n",
       "  0.005997776624838025,\n",
       "  0.11497028172016144],\n",
       " ['an exposed hydroxyl group on the deoxyribose',\n",
       "  'the end of a dna',\n",
       "  4.5692511098612464e-05,\n",
       "  0.02863362990319729],\n",
       " ['an exposed phosphate group',\n",
       "  'the end of a',\n",
       "  4.636257995960236e-06,\n",
       "  0.009944381192326546],\n",
       " ['nucleic acid synthesis',\n",
       "  'nucleic acid synthesis',\n",
       "  0.9344871282577515,\n",
       "  0.10237488895654678],\n",
       " ['because new nucleotides are added via a dehydration reaction that uses the exposed 3 hydroxyl as a nucleophile',\n",
       "  'new nucleotide elements are added via what answer a reaction that uses the exposed 3 hydroxyl',\n",
       "  0.0007533651320250422,\n",
       "  0.1330309510231018],\n",
       " ['by transcribing the gene into rna',\n",
       "  'by transcribing the gene into rna',\n",
       "  0.6268129753214973,\n",
       "  0.09872452169656754],\n",
       " ['a second type of nucleic acid that is very similar to dna',\n",
       "  'a second type of nucleic acid that is similar to dna',\n",
       "  0.6199220583784102,\n",
       "  0.08792915940284729],\n",
       " ['the base uracil',\n",
       "  'uracil answer',\n",
       "  3.688985562555202e-05,\n",
       "  0.08200228214263916],\n",
       " ['a series of threenucleotide sequences',\n",
       "  'threenucleotide sequences',\n",
       "  0.0012239041690882571,\n",
       "  0.11043306440114975],\n",
       " ['the genetic code',\n",
       "  'the genetic code',\n",
       "  0.9365613261858622,\n",
       "  0.04494880139827728],\n",
       " ['its genome', 'genome', 0.00014797520407228149, 0.06759817898273468],\n",
       " ['a single very long dna helix',\n",
       "  'a single very long dna helix',\n",
       "  0.8619784265756607,\n",
       "  0.09318097680807114],\n",
       " ['thousands of genes', 'genome', 0.000285406219518336, 0.009701630100607872],\n",
       " ['its locus', 'locus', 1.724063458633888e-05, 0.04765884205698967],\n",
       " ['one allele of a gene', 'one', 0.5052158161997795, 0.05133593827486038],\n",
       " ['on a set of large linear chromosomes',\n",
       "  'on what chromosomes answer linear',\n",
       "  0.10138766566247959,\n",
       "  0.01856626756489277],\n",
       " ['a nucleosome', 'nucleosome', 1.6629979633364655e-06, 0.08660229295492172],\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save to file\n",
    "with open(\"diff_validation_qa_eval_results_validation_l1.json\", \"w\") as f:\n",
    "    json.dump(analysis_validation, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = np.array(analysis_validation)[:,-1].astype(float)\n",
    "sum(tt<=0.05)+sum(tt>0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "bin_edges = [np.array([0, 0.05, 0.1, 0.5, 1]), np.array([0, 0.05, 0.1, 0.5,  1])]\n",
    "\n",
    "num = 50\n",
    "\n",
    "hist,_,_= np.histogram2d( np.array(analysis_validation)[:,-1].astype(float),  np.array(analysis_validation)[:,-2].astype(float), bins=bin_edges)\n",
    "\n",
    "hist = hist/num\n",
    "# Assuming `mean_hist_norm` and `var_hist_norm` are your 2D arrays of the same shape\n",
    "# If you prefer to show std instead of variance in the annotation, convert variance to std:\n",
    "\n",
    "\n",
    "# Create annotation labels with both mean and std (formatted as strings)\n",
    "annot_array = np.empty_like(hist, dtype=object)\n",
    "\n",
    "\n",
    "for i in range(hist.shape[0]):\n",
    "    for j in range(hist.shape[1]):\n",
    "        annot_array[i, j] = f\"{hist[i, j]:.1f}\"#\\n(±{std_hist_norm[i, j]:.1f})\"  # mean ± std\n",
    "\n",
    "# # Define tick positions for edges of bins (for correct labeling)\n",
    "x_edges = np.array([0, 0.05, 0.1, 0.5, 1])\n",
    "y_edges = np.array([0, 0.05, 0.1, 0.5, 1])\n",
    "xtick_positions = np.arange(len(x_edges) - 1) + 1.0  # Right edges\n",
    "ytick_positions = np.arange(len(y_edges) - 1) + 1.0  # Top edges\n",
    "\n",
    "# Plot heatmap for mean values (color intensity)\n",
    "plt.figure(figsize=(6, 6))\n",
    "ax = sns.heatmap(hist.T, annot=annot_array.T, cmap=sns.color_palette(\"coolwarm\"), \n",
    "                  annot_kws={\"size\":18}, cbar=False, vmin=5, vmax=70,fmt=\"\")\n",
    "\n",
    "# Adjust tick positions for x and y axes (move them to the edges)\n",
    "ax.set_xticks(xtick_positions)\n",
    "ax.set_yticks(ytick_positions)\n",
    "\n",
    "# Set the labels for ticks (x and y edges)\n",
    "ax.set_xticklabels(x_edges[1:])\n",
    "ax.set_yticklabels(y_edges[1:])\n",
    "\n",
    "# # Make the tick labels bold\n",
    "ax.tick_params(axis='x', labelsize=14)  # Bold x-axis labels\n",
    "ax.tick_params(axis='y', labelsize=14)  # Bold y-axis labels\n",
    "\n",
    "\n",
    "# Invert y-axis to align with typical heatmap style\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(r\"Distinct Token Attention\", fontweight=\"bold\", fontsize=16)\n",
    "plt.ylabel(r\"Relevant Token Probability\", fontweight=\"bold\", fontsize=16)\n",
    "#plt.title(\"Mean (±Std Dev) Heatmap\")\n",
    "\n",
    "# # Save the figure\n",
    "plt.savefig(\"faster_qk_10_times_squad_validation_l1.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on SQuAD examples...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|███████████████████████████████████████| 5000/5000 [08:10<00:00, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Total Questions: 5000\n",
      "Exact Match Score: 28.96%\n",
      "F1 Score: 45.99%\n",
      "\n",
      "Detailed Statistics:\n",
      "Questions with EM = 1: 1448 (29.0%)\n",
      "Questions with F1 > 0.5: 2201 (44.0%)\n",
      "Average F1 for non-zero scores: 0.750\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Perfect Matches (EM=1): 1448 examples\n",
      "  1. Predicted: 'Álvaro de Saavedra cerón' | True: 'Álvaro de Saavedra Cerón'\n",
      "  2. Predicted: 'the Maluku Islands' | True: 'the Maluku Islands'\n",
      "  3. Predicted: 'Enewetak or Bikini Atoll' | True: 'Enewetak or Bikini Atoll'\n",
      "\n",
      "Partial Matches (EM=0, F1>0.3): 1337 examples\n",
      "  1. Predicted: 'Los J' | True: 'Los Pintados' | F1: 0.500\n",
      "  2. Predicted: 'Los J' | True: 'Los Jardines' | F1: 0.500\n",
      "  3. Predicted: 'Los Jard' | True: 'Los Jardines' | F1: 0.500\n",
      "\n",
      "No Matches (F1=0): 1935 examples\n",
      "  1. Predicted: 'San Bartolome' | True: 'Taongi'\n",
      "  2. Predicted: 'Los Barbés' | True: 'Mejit'\n",
      "  3. Predicted: '\"' | True: 'Corrales'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics tracking\n",
    "count = 0\n",
    "exact_match_scores = []\n",
    "f1_scores = []\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "print(\"Evaluating model on SQuAD examples...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test on training examples\n",
    "for example in tqdm(squad_validation):\n",
    "    # Generate prediction\n",
    "    predicted_answer = generate_answer(example['context'], example['question'])\n",
    "    true_answers = example['answers']['text']  # List of all valid answers\n",
    "    \n",
    "    # Take the best score across all possible answers (SQuAD style)\n",
    "    em_score = max(compute_exact_match(true_ans, predicted_answer) for true_ans in true_answers)\n",
    "    f1_score = max(compute_f1(true_ans, predicted_answer) for true_ans in true_answers)\n",
    "    \n",
    "    # Store scores\n",
    "    exact_match_scores.append(em_score)\n",
    "    f1_scores.append(f1_score)\n",
    "    predictions.append(predicted_answer)\n",
    "    ground_truths.append(true_answers[0])  # First answer for display\n",
    "    \n",
    "    # Display results\n",
    "    #print(f\"Question {count + 1}:\")\n",
    "    #print(f\"Q: {example['question']}\")\n",
    "    #print(f\"Predicted A: {predicted_answer}\")\n",
    "    #print(f\"True A: {true_answers[0]}\")\n",
    "    #print(f\"EM Score: {em_score} | F1 Score: {f1_score:.3f}\")\n",
    "    #print(\"=\" * 60)\n",
    "    \n",
    "    # count += 1\n",
    "    # if count > 5000:\n",
    "    #     break\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_em = sum(exact_match_scores) / len(exact_match_scores) * 100\n",
    "overall_f1 = sum(f1_scores) / len(f1_scores) * 100\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Questions: {len(exact_match_scores)}\")\n",
    "print(f\"Exact Match Score: {overall_em:.2f}%\")\n",
    "print(f\"F1 Score: {overall_f1:.2f}%\")\n",
    "\n",
    "# Additional statistics\n",
    "print(f\"\\nDetailed Statistics:\")\n",
    "print(f\"Questions with EM = 1: {sum(exact_match_scores)} ({sum(exact_match_scores)/len(exact_match_scores)*100:.1f}%)\")\n",
    "print(f\"Questions with F1 > 0.5: {sum(1 for f1 in f1_scores if f1 > 0.5)} ({sum(1 for f1 in f1_scores if f1 > 0.5)/len(f1_scores)*100:.1f}%)\")\n",
    "print(f\"Average F1 for non-zero scores: {sum(f1 for f1 in f1_scores if f1 > 0) / max(1, sum(1 for f1 in f1_scores if f1 > 0)):.3f}\")\n",
    "\n",
    "# Show some examples of different performance levels\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find examples with different performance levels\n",
    "perfect_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if em == 1]\n",
    "partial_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if em == 0 and f1 > 0.3]\n",
    "no_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if f1 == 0]\n",
    "\n",
    "if perfect_matches:\n",
    "    print(f\"\\nPerfect Matches (EM=1): {len(perfect_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(perfect_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}'\")\n",
    "\n",
    "if partial_matches:\n",
    "    print(f\"\\nPartial Matches (EM=0, F1>0.3): {len(partial_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(partial_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}' | F1: {f1_scores[idx]:.3f}\")\n",
    "\n",
    "if no_matches:\n",
    "    print(f\"\\nNo Matches (F1=0): {len(no_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(no_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}'\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model and GPT Pretrained Model Weight and Output Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import GPT2LMHeadModel, GPT2Config\n",
    "# from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2Model, GPT2PreTrainedModel\n",
    "# from transformers.modeling_utils import Conv1D\n",
    "\n",
    "# # --------- Custom Attention Module using Conv1D ----------\n",
    "# class CustomGPT2Attention(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = config.hidden_size\n",
    "#         self.num_heads = config.num_attention_heads\n",
    "#         self.head_dim = self.embed_dim // self.num_heads\n",
    "#         assert self.head_dim * self.num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "#         self.q_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "#         self.k_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "#         self.v_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "#         self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "\n",
    "#         self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "#         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "#         self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "#         # Register causal mask buffer (same as original GPT2)\n",
    "#         max_positions = config.max_position_embeddings\n",
    "#         self.register_buffer(\n",
    "#             \"bias\",\n",
    "#             torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "#                 1, 1, max_positions, max_positions\n",
    "#             ),\n",
    "#         )\n",
    "#         self.register_buffer(\"masked_bias\", torch.tensor(-1e4))\n",
    "\n",
    "#     def _split_heads(self, x):\n",
    "#         batch_size, seq_len, embed_dim = x.size()\n",
    "#         # embed_dim should equal num_heads * head_dim\n",
    "#         assert embed_dim == self.num_heads * self.head_dim, f\"Embed dim {embed_dim} != num_heads * head_dim {self.num_heads * self.head_dim}\"\n",
    "        \n",
    "#         # reshape to (batch_size, seq_len, num_heads, head_dim)\n",
    "#         x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "#         # permute to (batch_size, num_heads, seq_len, head_dim)\n",
    "#         return x.permute(0, 2, 1, 3)\n",
    "\n",
    "#     def _merge_heads(self, x):\n",
    "#         x = x.permute(0, 2, 1, 3).contiguous()\n",
    "#         new_shape = x.size()[:-2] + (self.embed_dim,)\n",
    "#         return x.view(*new_shape)\n",
    "\n",
    "#     def forward(self, hidden_states, layer_past=None, attention_mask=None,\n",
    "#                 head_mask=None, use_cache=False, output_attentions=False):\n",
    "    \n",
    "#         # Project to Q, K, V\n",
    "#         query = self.q_proj(hidden_states)\n",
    "#         key = self.k_proj(hidden_states)\n",
    "#         value = self.v_proj(hidden_states)\n",
    "\n",
    "#         # Split heads\n",
    "#         query = self._split_heads(query)\n",
    "#         key = self._split_heads(key)\n",
    "#         value = self._split_heads(value)\n",
    "\n",
    "#         # Handle past keys/values for generation\n",
    "#         if layer_past is not None:\n",
    "#             past_key, past_value = layer_past\n",
    "#             key = torch.cat((past_key, key), dim=-2)\n",
    "#             value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "#         if use_cache:\n",
    "#             present = (key, value)\n",
    "#         else:\n",
    "#             present = None\n",
    "\n",
    "#         # Compute attention weights\n",
    "#         attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "#         attn_weights = attn_weights * self.scale\n",
    "\n",
    "#         # Apply causal mask\n",
    "#         query_length, key_length = query.size(-2), key.size(-2)\n",
    "#         causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "#         mask_value = torch.finfo(attn_weights.dtype).min\n",
    "#         mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
    "#         attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
    "\n",
    "#         # Apply attention mask if provided\n",
    "#         if attention_mask is not None:\n",
    "#             attn_weights = attn_weights + attention_mask\n",
    "\n",
    "#         # Softmax\n",
    "#         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "#         attn_weights = attn_weights.type(value.dtype)\n",
    "#         attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "#         # Apply head mask if provided\n",
    "#         if head_mask is not None:\n",
    "#             attn_weights = attn_weights * head_mask\n",
    "\n",
    "#         # Apply attention to values\n",
    "#         attn_output = torch.matmul(attn_weights, value)\n",
    "#         attn_output = self._merge_heads(attn_output)\n",
    "\n",
    "#         # Final projection\n",
    "#         attn_output = self.c_proj(attn_output)\n",
    "#         attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "#         outputs = (attn_output, present)\n",
    "#         if output_attentions:\n",
    "#             outputs += (attn_weights,)\n",
    "\n",
    "#         return outputs\n",
    "# # --------- Custom GPT2 Block ----------\n",
    "# class CustomGPT2Block(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.ln_1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "#         self.attn = CustomGPT2Attention(config)\n",
    "#         self.ln_2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "#         self.mlp = GPT2Block(config).mlp\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         hidden_states,\n",
    "#         layer_past=None,\n",
    "#         attention_mask=None,\n",
    "#         head_mask=None,\n",
    "#         encoder_hidden_states=None,\n",
    "#         encoder_attention_mask=None,\n",
    "#         use_cache=False,\n",
    "#         output_attentions=False,\n",
    "#     ):\n",
    "#         residual = hidden_states\n",
    "#         hidden_states = self.ln_1(hidden_states)\n",
    "\n",
    "#         attn_outputs = self.attn(\n",
    "#             hidden_states,\n",
    "#             layer_past=layer_past,\n",
    "#             attention_mask=attention_mask,\n",
    "#             head_mask=head_mask,\n",
    "#             use_cache=use_cache,\n",
    "#             output_attentions=output_attentions,\n",
    "#         )\n",
    "\n",
    "#         attn_output = attn_outputs[0]\n",
    "#         outputs = attn_outputs[1:]\n",
    "\n",
    "#         hidden_states = residual + attn_output\n",
    "\n",
    "#         residual = hidden_states\n",
    "#         hidden_states = self.ln_2(hidden_states)\n",
    "#         feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "#         hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "#         return (hidden_states,) + outputs\n",
    "\n",
    "# # --------- Custom GPT2 Model ----------\n",
    "# class CustomGPT2Model(GPT2Model):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.h = nn.ModuleList([CustomGPT2Block(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "# # --------- Custom GPT2 LM Model ----------\n",
    "# class CustomGPT2LMHeadModel(GPT2PreTrainedModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.transformer = CustomGPT2Model(config)\n",
    "#         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def forward(self, input_ids=None, **kwargs):\n",
    "#         transformer_outputs = self.transformer(input_ids=input_ids, **kwargs)\n",
    "#         hidden_states = transformer_outputs[0]\n",
    "#         lm_logits = self.lm_head(hidden_states)\n",
    "#         return lm_logits\n",
    "\n",
    "# # --------- Copy Weights from Original GPT2 Model ----------\n",
    "# def copy_weights(original_model, custom_model):\n",
    "#     orig_state_dict = original_model.state_dict()\n",
    "#     custom_state_dict = custom_model.state_dict()\n",
    "\n",
    "#     for name, param in orig_state_dict.items():\n",
    "#         if \"attn.c_attn.weight\" in name:\n",
    "#             layer_num = int(name.split('.')[2])\n",
    "#             prefix = f'transformer.h.{layer_num}.attn.'\n",
    "            \n",
    "#             # Original c_attn weight shape: (embed_dim, 3 * embed_dim)\n",
    "#             # Need to split along dim=1 (the 3 * embed_dim dimension)\n",
    "#             embed_dim = param.shape[0]\n",
    "#             q_weight, k_weight, v_weight = torch.split(param, embed_dim, dim=1)\n",
    "            \n",
    "#             # Conv1D weight shape is (input_dim, output_dim), no transpose needed\n",
    "#             custom_state_dict[f'{prefix}q_proj.weight'].copy_(q_weight)\n",
    "#             custom_state_dict[f'{prefix}k_proj.weight'].copy_(k_weight)  \n",
    "#             custom_state_dict[f'{prefix}v_proj.weight'].copy_(v_weight)\n",
    "\n",
    "#         elif \"attn.c_attn.bias\" in name:\n",
    "#             layer_num = int(name.split('.')[2])\n",
    "#             prefix = f'transformer.h.{layer_num}.attn.'\n",
    "#             hidden_size = param.shape[0] // 3\n",
    "\n",
    "#             q_bias, k_bias, v_bias = torch.split(param, hidden_size)\n",
    "#             custom_state_dict[f'{prefix}q_proj.bias'].copy_(q_bias)\n",
    "#             custom_state_dict[f'{prefix}k_proj.bias'].copy_(k_bias)\n",
    "#             custom_state_dict[f'{prefix}v_proj.bias'].copy_(v_bias)\n",
    "\n",
    "#         elif \"attn.c_proj.weight\" in name:\n",
    "#             # Copy c_proj weights directly\n",
    "#             layer_num = int(name.split('.')[2])\n",
    "#             prefix = f'transformer.h.{layer_num}.attn.'\n",
    "#             custom_state_dict[f'{prefix}c_proj.weight'].copy_(param)\n",
    "            \n",
    "#         else:\n",
    "#             if name in custom_state_dict:\n",
    "#                 custom_state_dict[name].copy_(param)\n",
    "\n",
    "#     custom_model.load_state_dict(custom_state_dict)\n",
    "\n",
    "# # --------- Parameter Comparison ----------\n",
    "# def compare_model_parameters(original_model, custom_model):\n",
    "#     orig_params = dict(original_model.named_parameters())\n",
    "#     cust_params = dict(custom_model.named_parameters())\n",
    "\n",
    "#     print(\"--- Comparing Non-Split Parameters ---\")\n",
    "#     for name, orig_param in orig_params.items():\n",
    "#         if \"attn.c_attn\" in name or \"attn.c_proj.weight\" in name:\n",
    "#             continue\n",
    "#         if name not in cust_params:\n",
    "#             print(f\"Parameter {name} missing in custom model\")\n",
    "#             continue\n",
    "#         cust_param = cust_params[name]\n",
    "#         if torch.allclose(orig_param, cust_param, atol=1e-6):\n",
    "#             c = 1 #print(f\"Parameter {name} matches.\")\n",
    "#         else:\n",
    "#             print(f\"Parameter {name} differs!\")\n",
    "\n",
    "#     print(\"\\n--- Comparing Split QKV Parameters ---\")\n",
    "#     for i in range(original_model.config.num_hidden_layers):\n",
    "#         prefix = f'transformer.h.{i}.'\n",
    "#         orig_w = orig_params[f'{prefix}attn.c_attn.weight']\n",
    "#         orig_b = orig_params[f'{prefix}attn.c_attn.bias']\n",
    "#         orig_proj_w = orig_params[f'{prefix}attn.c_proj.weight']\n",
    "        \n",
    "#         hidden_size = orig_w.shape[0]\n",
    "\n",
    "#         # Split the original concatenated weights\n",
    "#         orig_qw, orig_kw, orig_vw = torch.split(orig_w, hidden_size, dim=1)\n",
    "#         orig_qb, orig_kb, orig_vb = torch.split(orig_b, hidden_size)\n",
    "\n",
    "#         # Get custom model parameters\n",
    "#         cust_qw = cust_params[f'{prefix}attn.q_proj.weight']\n",
    "#         cust_qb = cust_params[f'{prefix}attn.q_proj.bias']\n",
    "#         cust_kw = cust_params[f'{prefix}attn.k_proj.weight']\n",
    "#         cust_kb = cust_params[f'{prefix}attn.k_proj.bias']\n",
    "#         cust_vw = cust_params[f'{prefix}attn.v_proj.weight']\n",
    "#         cust_vb = cust_params[f'{prefix}attn.v_proj.bias']\n",
    "#         cust_proj_w = cust_params[f'{prefix}attn.c_proj.weight']\n",
    "\n",
    "#         # Compare weights (no transpose needed for Conv1D)\n",
    "#         if torch.allclose(orig_qw, cust_qw, atol=1e-6): c = 1#print(f\"Layer {i} Q weight matches.\")\n",
    "#         else: print(f\"Layer {i} Q weight differs!\")\n",
    "        \n",
    "#         if torch.allclose(orig_kw, cust_kw, atol=1e-6): c =1 #print(f\"Layer {i} K weight matches.\")\n",
    "#         else: print(f\"Layer {i} K weight differs!\")\n",
    "        \n",
    "#         if torch.allclose(orig_vw, cust_vw, atol=1e-6): c =1 #print(f\"Layer {i} V weight matches.\")\n",
    "#         else: print(f\"Layer {i} V weight differs!\")\n",
    "        \n",
    "#         if torch.allclose(orig_proj_w, cust_proj_w, atol=1e-6): c= 1#print(f\"Layer {i} c_proj weight matches.\")\n",
    "#         else: print(f\"Layer {i} c_proj weight differs!\")\n",
    "\n",
    "#         # Compare biases (no transpose needed)\n",
    "#         if torch.allclose(orig_qb, cust_qb, atol=1e-6): c= 1#print(f\"Layer {i} Q bias matches.\")\n",
    "#         else: print(f\"Layer {i} Q bias differs!\")\n",
    "        \n",
    "#         if torch.allclose(orig_kb, cust_kb, atol=1e-6): c =1 #print(f\"Layer {i} K bias matches.\")\n",
    "#         else: print(f\"Layer {i} K bias differs!\")\n",
    "        \n",
    "#         if torch.allclose(orig_vb, cust_vb, atol=1e-6): c = 1#print(f\"Layer {i} V bias matches.\")\n",
    "#         else: print(f\"Layer {i} V bias differs!\")\n",
    "\n",
    "# # --------- Output Comparison ----------\n",
    "# def check_outputs_identical(original_model, custom_model):\n",
    "#     original_model.eval()\n",
    "#     custom_model.eval()\n",
    "\n",
    "#     batch_size = 2\n",
    "#     seq_len = 16\n",
    "#     vocab_size = original_model.config.vocab_size\n",
    "#     input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         orig_logits = original_model(input_ids).logits\n",
    "#         cust_logits = custom_model(input_ids)\n",
    "\n",
    "#     max_diff = torch.max(torch.abs(orig_logits - cust_logits)).item()\n",
    "#     print(f\"\\n--- Comparing Model Outputs ---\")\n",
    "#     print(f\"Max absolute difference between outputs: {max_diff:.10f}\")# Additional debugging steps to minimize differences\n",
    "\n",
    "# def debug_attention_step_by_step(original_model, custom_model, input_ids):\n",
    "#     \"\"\"Compare intermediate outputs in the first attention layer\"\"\"\n",
    "#     original_model.eval()\n",
    "#     custom_model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Get embeddings (should be identical)\n",
    "#         orig_embeds = original_model.transformer.wte(input_ids) + original_model.transformer.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "#         cust_embeds = custom_model.transformer.wte(input_ids) + custom_model.transformer.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "        \n",
    "#         print(\"Embedding difference:\", torch.max(torch.abs(orig_embeds - cust_embeds)).item())\n",
    "        \n",
    "#         # First layer norm\n",
    "#         orig_ln1 = original_model.transformer.h[0].ln_1(orig_embeds)\n",
    "#         cust_ln1 = custom_model.transformer.h[0].ln_1(cust_embeds)\n",
    "        \n",
    "#         print(\"First LayerNorm difference:\", torch.max(torch.abs(orig_ln1 - cust_ln1)).item())\n",
    "        \n",
    "#         # QKV projections\n",
    "#         orig_qkv = original_model.transformer.h[0].attn.c_attn(orig_ln1)\n",
    "#         orig_q, orig_k, orig_v = orig_qkv.split(original_model.config.hidden_size, dim=2)\n",
    "        \n",
    "#         cust_q = custom_model.transformer.h[0].attn.q_proj(cust_ln1)\n",
    "#         cust_k = custom_model.transformer.h[0].attn.k_proj(cust_ln1)\n",
    "#         cust_v = custom_model.transformer.h[0].attn.v_proj(cust_ln1)\n",
    "        \n",
    "#         print(\"Q projection difference:\", torch.max(torch.abs(orig_q - cust_q)).item())\n",
    "#         print(\"K projection difference:\", torch.max(torch.abs(orig_k - cust_k)).item())\n",
    "#         print(\"V projection difference:\", torch.max(torch.abs(orig_v - cust_v)).item())\n",
    "\n",
    "# def ensure_identical_initialization():\n",
    "#     \"\"\"Ensure both models have identical random initialization states\"\"\"\n",
    "#     torch.manual_seed(42)\n",
    "#     torch.cuda.manual_seed_all(42)\n",
    "#     # Set deterministic behavior\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# # Alternative: Use double precision for comparison\n",
    "# def check_outputs_double_precision(original_model, custom_model):\n",
    "#     \"\"\"Check outputs using double precision for more accurate comparison\"\"\"\n",
    "#     original_model.eval()\n",
    "#     custom_model.eval()\n",
    "    \n",
    "#     # Convert to double precision\n",
    "#     original_model = original_model.double()\n",
    "#     custom_model = custom_model.double()\n",
    "    \n",
    "#     batch_size = 2\n",
    "#     seq_len = 16\n",
    "#     vocab_size = original_model.config.vocab_size\n",
    "#     input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         orig_logits = original_model(input_ids).logits\n",
    "#         cust_logits = custom_model(input_ids)\n",
    "\n",
    "#     max_diff = torch.max(torch.abs(orig_logits - cust_logits)).item()\n",
    "#     print(f\"Double precision max difference: {max_diff:.15f}\")\n",
    "#     return max_diff\n",
    "#     if max_diff < 1e-4:\n",
    "#         print(\"Success! Outputs match within tolerance.\")\n",
    "#     else:\n",
    "#         print(\"Failure! Outputs still differ.\")\n",
    "\n",
    "\n",
    "# config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "\n",
    "# print(\"Loading original GPT-2 model...\")\n",
    "# original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# print(\"Creating custom GPT-2 model with split Q,K,V...\")\n",
    "# custom_model = CustomGPT2LMHeadModel(config)\n",
    "\n",
    "# print(\"Copying weights...\")\n",
    "# copy_weights(original_model, custom_model)\n",
    "\n",
    "# print(\"\\nComparing parameters...\")\n",
    "# compare_model_parameters(original_model, custom_model)\n",
    "\n",
    "# check_outputs_identical(original_model, custom_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional debugging steps to minimize differences\n",
    "\n",
    "# def debug_attention_step_by_step(original_model, custom_model, input_ids):\n",
    "#     \"\"\"Compare intermediate outputs in the first attention layer\"\"\"\n",
    "#     original_model.eval()\n",
    "#     custom_model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Get embeddings (should be identical)\n",
    "#         orig_embeds = original_model.transformer.wte(input_ids) + original_model.transformer.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "#         cust_embeds = custom_model.transformer.wte(input_ids) + custom_model.transformer.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "        \n",
    "#         print(\"Embedding difference:\", torch.max(torch.abs(orig_embeds - cust_embeds)).item())\n",
    "        \n",
    "#         # First layer norm\n",
    "#         orig_ln1 = original_model.transformer.h[0].ln_1(orig_embeds)\n",
    "#         cust_ln1 = custom_model.transformer.h[0].ln_1(cust_embeds)\n",
    "        \n",
    "#         print(\"First LayerNorm difference:\", torch.max(torch.abs(orig_ln1 - cust_ln1)).item())\n",
    "        \n",
    "#         # QKV projections\n",
    "#         orig_qkv = original_model.transformer.h[0].attn.c_attn(orig_ln1)\n",
    "#         orig_q, orig_k, orig_v = orig_qkv.split(original_model.config.hidden_size, dim=2)\n",
    "        \n",
    "#         cust_q = custom_model.transformer.h[0].attn.q_proj(cust_ln1)\n",
    "#         cust_k = custom_model.transformer.h[0].attn.k_proj(cust_ln1)\n",
    "#         cust_v = custom_model.transformer.h[0].attn.v_proj(cust_ln1)\n",
    "        \n",
    "#         print(\"Q projection difference:\", torch.max(torch.abs(orig_q - cust_q)).item())\n",
    "#         print(\"K projection difference:\", torch.max(torch.abs(orig_k - cust_k)).item())\n",
    "#         print(\"V projection difference:\", torch.max(torch.abs(orig_v - cust_v)).item())\n",
    "\n",
    "# def ensure_identical_initialization():\n",
    "#     \"\"\"Ensure both models have identical random initialization states\"\"\"\n",
    "#     torch.manual_seed(42)\n",
    "#     torch.cuda.manual_seed_all(42)\n",
    "#     # Set deterministic behavior\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# # Alternative: Use double precision for comparison\n",
    "# def check_outputs_double_precision(original_model, custom_model):\n",
    "#     \"\"\"Check outputs using double precision for more accurate comparison\"\"\"\n",
    "#     original_model.eval()\n",
    "#     custom_model.eval()\n",
    "    \n",
    "#     # Convert to double precision\n",
    "#     original_model = original_model.double()\n",
    "#     custom_model = custom_model.double()\n",
    "    \n",
    "#     batch_size = 2\n",
    "#     seq_len = 16\n",
    "#     vocab_size = original_model.config.vocab_size\n",
    "#     input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         orig_logits = original_model(input_ids).logits\n",
    "#         cust_logits = custom_model(input_ids)\n",
    "\n",
    "#     max_diff = torch.max(torch.abs(orig_logits - cust_logits)).item()\n",
    "#     print(f\"Double precision max difference: {max_diff:.15f}\")\n",
    "    \n",
    "#     # Convert back to float\n",
    "#     original_model = original_model.float()\n",
    "#     custom_model = custom_model.float()\n",
    "    \n",
    "#     return max_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = original_model.config.vocab_size\n",
    "# input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# debug_attention_step_by_step(original_model, custom_model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_outputs_double_precision(original_model, custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
