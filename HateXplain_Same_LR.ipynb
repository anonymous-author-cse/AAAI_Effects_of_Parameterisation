{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:23:46.662753Z",
     "iopub.status.busy": "2025-06-14T15:23:46.662044Z",
     "iopub.status.idle": "2025-06-14T15:23:46.669143Z",
     "shell.execute_reply": "2025-06-14T15:23:46.668549Z",
     "shell.execute_reply.started": "2025-06-14T15:23:46.662711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "count = 0\n",
    "for root, folders, filenames in os.walk('../input'):\n",
    "   print(root, folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:24:43.337015Z",
     "iopub.status.busy": "2025-06-14T15:24:43.336336Z",
     "iopub.status.idle": "2025-06-14T15:24:43.342809Z",
     "shell.execute_reply": "2025-06-14T15:24:43.342189Z",
     "shell.execute_reply.started": "2025-06-14T15:24:43.336990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current folder: /kaggle/input/\n",
      "Subfolders: []\n",
      "Files: ['post_id_divisions.json', 'classes.npy', 'dataset.json']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for root, folders, files in os.walk('/kaggle/input/'):\n",
    "    print(\"Current folder:\", root)\n",
    "    print(\"Subfolders:\", folders)\n",
    "    print(\"Files:\", files)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:25:23.932688Z",
     "iopub.status.busy": "2025-06-14T15:25:23.932174Z",
     "iopub.status.idle": "2025-06-14T15:27:40.943503Z",
     "shell.execute_reply": "2025-06-14T15:27:40.942642Z",
     "shell.execute_reply.started": "2025-06-14T15:25:23.932663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Colab notebook\n",
      "Collecting git+https://github.com/neelnanda-io/Easy-Transformer.git@clean-transformer-demo\n",
      "  Cloning https://github.com/neelnanda-io/Easy-Transformer.git (to revision clean-transformer-demo) to /tmp/pip-req-build-f1g32xr3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/Easy-Transformer.git /tmp/pip-req-build-f1g32xr3\n",
      "  Running command git checkout -b clean-transformer-demo --track origin/clean-transformer-demo\n",
      "  Switched to a new branch 'clean-transformer-demo'\n",
      "  Branch 'clean-transformer-demo' set up to track remote branch 'clean-transformer-demo' from 'origin'.\n",
      "  Resolved https://github.com/neelnanda-io/Easy-Transformer.git to commit 1f25219e631aeb478d17075d47274db32c874e88\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from easy_transformer==0.1.0) (0.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easy_transformer==0.1.0) (1.26.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easy_transformer==0.1.0) (2.5.1+cu124)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from easy_transformer==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from easy_transformer==0.1.0) (4.51.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from easy_transformer==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from easy_transformer==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from easy_transformer==0.1.0) (0.19.6)\n",
      "Collecting fancy_einsum (from easy_transformer==0.1.0)\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->easy_transformer==0.1.0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->easy_transformer==0.1.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->easy_transformer==0.1.0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->easy_transformer==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->easy_transformer==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->easy_transformer==0.1.0) (0.70.16)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->easy_transformer==0.1.0)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->easy_transformer==0.1.0) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->easy_transformer==0.1.0) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->easy_transformer==0.1.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->easy_transformer==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->easy_transformer==0.1.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->easy_transformer==0.1.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->easy_transformer==0.1.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->easy_transformer==0.1.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->easy_transformer==0.1.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->easy_transformer==0.1.0) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->easy_transformer==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->easy_transformer==0.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->easy_transformer==0.1.0) (2025.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->easy_transformer==0.1.0) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easy_transformer==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easy_transformer==0.1.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easy_transformer==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easy_transformer==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easy_transformer==0.1.0) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->easy_transformer==0.1.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->easy_transformer==0.1.0)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->easy_transformer==0.1.0)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->easy_transformer==0.1.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->easy_transformer==0.1.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->easy_transformer==0.1.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easy_transformer==0.1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easy_transformer==0.1.0) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->easy_transformer==0.1.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->easy_transformer==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easy_transformer==0.1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easy_transformer==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->easy_transformer==0.1.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->easy_transformer==0.1.0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->easy_transformer==0.1.0) (0.5.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->easy_transformer==0.1.0) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->easy_transformer==0.1.0) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->easy_transformer==0.1.0) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->easy_transformer==0.1.0) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->easy_transformer==0.1.0) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->easy_transformer==0.1.0) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb->easy_transformer==0.1.0) (2.11.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->easy_transformer==0.1.0) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->easy_transformer==0.1.0) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->easy_transformer==0.1.0) (75.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb->easy_transformer==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (1.19.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->easy_transformer==0.1.0) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->easy_transformer==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->easy_transformer==0.1.0) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->easy_transformer==0.1.0) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->easy_transformer==0.1.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->easy_transformer==0.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->easy_transformer==0.1.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->easy_transformer==0.1.0) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easy_transformer==0.1.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->easy_transformer==0.1.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->easy_transformer==0.1.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->easy_transformer==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->easy_transformer==0.1.0) (2024.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->easy_transformer==0.1.0) (5.0.2)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->easy_transformer==0.1.0) (2024.2.0)\n",
      "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: easy_transformer\n",
      "  Building wheel for easy_transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for easy_transformer: filename=easy_transformer-0.1.0-py3-none-any.whl size=55600 sha256=9e01da59139c53229fa1900e2ffa9c4314c3411db3872392dbd59a1837908c76\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-w9ezyzpi/wheels/bf/56/b1/e601264be47cea8ab7d082bfd13e38f6a652e3790a0a225496\n",
      "Successfully built easy_transformer\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, fancy_einsum, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, easy_transformer\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed easy_transformer-0.1.0 fancy_einsum-0.0.3 fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[31m================================================================================\u001b[m\n",
      "\u001b[31m================================================================================\u001b[m\n",
      "\n",
      "  \u001b[1m\u001b[33m                            DEPRECATION WARNING                            \u001b[m\n",
      "\n",
      "    \u001b[1m\u001b[4m Node.js 16.x is no longer actively supported!\u001b[m\n",
      "\n",
      "  \u001b[1mYou will not receive security or critical stability updates\u001b[m for this version.\n",
      "\n",
      "  You should migrate to a supported version of Node.js as soon as possible.\n",
      "  Use the installation script that corresponds to the version of Node.js you\n",
      "  wish to install. e.g.\n",
      "  \n",
      "   * \u001b[31mhttps://deb.nodesource.com/setup_16.x — Node.js 16 \"Gallium\" \u001b[1m(deprecated)\u001b[m\n",
      "   * \u001b[32mhttps://deb.nodesource.com/setup_18.x — Node.js 18 \"Hydrogen\" (Maintenance)\u001b[m\n",
      "   * \u001b[31mhttps://deb.nodesource.com/setup_19.x — Node.js 19 \"Nineteen\" \u001b[1m(deprecated)\u001b[m\n",
      "   * \u001b[1m\u001b[32mhttps://deb.nodesource.com/setup_20.x — Node.js 20 LTS \"Iron\" (recommended)\u001b[m\n",
      "   * \u001b[32mhttps://deb.nodesource.com/setup_21.x — Node.js 21 \"Iron\" (current)\u001b[m\n",
      "   \n",
      "\n",
      "\n",
      "  Please see \u001b[1mhttps://github.com/nodejs/Release\u001b[m for details about which\n",
      "  version may be appropriate for you.\n",
      "\n",
      "  The \u001b[32m\u001b[1mNodeSource\u001b[m Node.js distributions repository contains\n",
      "  information both about supported versions of Node.js and supported Linux\n",
      "  distributions. To learn more about usage, see the repository:\n",
      "   \u001b[4m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n",
      "\n",
      "\u001b[31m================================================================================\u001b[m\n",
      "\u001b[31m================================================================================\u001b[m\n",
      "\n",
      "Continuing in 10 seconds ...\n",
      "\n",
      "\u001b[38;5;79m2025-06-14 15:26:56 - Installing pre-requisites\u001b[0m\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease                          \n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]       \n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,776 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]         \n",
      "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]            \n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]       \n",
      "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,037 kB]   \n",
      "Get:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,986 kB]\n",
      "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,296 kB]\n",
      "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
      "Get:14 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [77.5 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,249 kB]\n",
      "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,750 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,476 kB]\n",
      "Get:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,630 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,556 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
      "Get:25 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.8 kB]\n",
      "Get:26 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.6 kB]\n",
      "Get:27 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.1 kB]\n",
      "Fetched 32.7 MB in 3s (12.4 MB/s)                            \n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ca-certificates is already the newest version (20240203~22.04.1).\n",
      "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
      "The following additional packages will be installed:\n",
      "  dirmngr gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server\n",
      "  gpgconf gpgsm gpgv\n",
      "Suggested packages:\n",
      "  pinentry-gnome3 tor parcimonie xloadimage scdaemon\n",
      "The following NEW packages will be installed:\n",
      "  apt-transport-https\n",
      "The following packages will be upgraded:\n",
      "  dirmngr gnupg gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client\n",
      "  gpg-wks-server gpgconf gpgsm gpgv\n",
      "11 upgraded, 1 newly installed, 0 to remove and 226 not upgraded.\n",
      "Need to get 2,250 kB of archives.\n",
      "After this operation, 170 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-wks-client amd64 2.2.27-3ubuntu2.3 [62.7 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dirmngr amd64 2.2.27-3ubuntu2.3 [293 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-wks-server amd64 2.2.27-3ubuntu2.3 [57.6 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg-utils amd64 2.2.27-3ubuntu2.3 [309 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-agent amd64 2.2.27-3ubuntu2.3 [209 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg amd64 2.2.27-3ubuntu2.3 [519 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpgconf amd64 2.2.27-3ubuntu2.3 [94.4 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg-l10n all 2.2.27-3ubuntu2.3 [54.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg all 2.2.27-3ubuntu2.3 [315 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpgsm amd64 2.2.27-3ubuntu2.3 [198 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpgv amd64 2.2.27-3ubuntu2.3 [137 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 apt-transport-https all 2.4.14 [1,510 B]\n",
      "Fetched 2,250 kB in 1s (1,579 kB/s)\n",
      "(Reading database ... 128691 files and directories currently installed.)\n",
      "Preparing to unpack .../00-gpg-wks-client_2.2.27-3ubuntu2.3_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../01-dirmngr_2.2.27-3ubuntu2.3_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../02-gpg-wks-server_2.2.27-3ubuntu2.3_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../03-gnupg-utils_2.2.27-3ubuntu2.3_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../04-gpg-agent_2.2.27-3ubuntu2.3_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../05-gpg_2.2.27-3ubuntu2.3_amd64.deb ...\n",
      "Unpacking gpg (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../06-gpgconf_2.2.27-3ubuntu2.3_amd64.deb ...\n",
      "Unpacking gpgconf (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../07-gnupg-l10n_2.2.27-3ubuntu2.3_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../08-gnupg_2.2.27-3ubuntu2.3_all.deb ...\n",
      "Unpacking gnupg (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../09-gpgsm_2.2.27-3ubuntu2.3_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../10-gpgv_2.2.27-3ubuntu2.3_amd64.deb ...\n",
      "Unpacking gpgv (2.2.27-3ubuntu2.3) over (2.2.27-3ubuntu2.1) ...\n",
      "Setting up gpgv (2.2.27-3ubuntu2.3) ...\n",
      "Selecting previously unselected package apt-transport-https.\n",
      "(Reading database ... 128691 files and directories currently installed.)\n",
      "Preparing to unpack .../apt-transport-https_2.4.14_all.deb ...\n",
      "Unpacking apt-transport-https (2.4.14) ...\n",
      "Setting up apt-transport-https (2.4.14) ...\n",
      "Setting up gnupg-l10n (2.2.27-3ubuntu2.3) ...\n",
      "Setting up gpgconf (2.2.27-3ubuntu2.3) ...\n",
      "Setting up gpg (2.2.27-3ubuntu2.3) ...\n",
      "Setting up gnupg-utils (2.2.27-3ubuntu2.3) ...\n",
      "Setting up gpg-agent (2.2.27-3ubuntu2.3) ...\n",
      "Setting up gpgsm (2.2.27-3ubuntu2.3) ...\n",
      "Setting up dirmngr (2.2.27-3ubuntu2.3) ...\n",
      "Setting up gpg-wks-server (2.2.27-3ubuntu2.3) ...\n",
      "Setting up gpg-wks-client (2.2.27-3ubuntu2.3) ...\n",
      "Setting up gnupg (2.2.27-3ubuntu2.3) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Get:3 https://deb.nodesource.com/node_16.x nodistro InRelease [12.1 kB]         \n",
      "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease                \n",
      "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease                      \n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease                          \n",
      "Get:7 https://deb.nodesource.com/node_16.x nodistro/main amd64 Packages [7,253 B]\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                  \n",
      "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease                \n",
      "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease   \n",
      "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Fetched 19.4 kB in 1s (14.5 kB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "\u001b[1;32m2025-06-14 15:27:15 - Repository configured successfully. To install Node.js, run: apt-get install nodejs -y\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  nodejs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 226 not upgraded.\n",
      "Need to get 27.5 MB of archives.\n",
      "After this operation, 128 MB of additional disk space will be used.\n",
      "Get:1 https://deb.nodesource.com/node_16.x nodistro/main amd64 nodejs amd64 16.20.2-1nodesource1 [27.5 MB]\n",
      "Fetched 27.5 MB in 0s (65.9 MB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Selecting previously unselected package nodejs.\n",
      "(Reading database ... 128695 files and directories currently installed.)\n",
      "Preparing to unpack .../nodejs_16.20.2-1nodesource1_amd64.deb ...\n",
      "Unpacking nodejs (16.20.2-1nodesource1) ...\n",
      "Setting up nodejs (16.20.2-1nodesource1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Collecting git+https://github.com/neelnanda-io/PySvelte.git\n",
      "  Cloning https://github.com/neelnanda-io/PySvelte.git to /tmp/pip-req-build-77y2yhpm\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/PySvelte.git /tmp/pip-req-build-77y2yhpm\n",
      "  Resolved https://github.com/neelnanda-io/PySvelte.git to commit 582d85ff708947e72b35cfcca05641332b44f5f5\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from PySvelte==1.0.0) (0.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from PySvelte==1.0.0) (1.26.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from PySvelte==1.0.0) (2.5.1+cu124)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from PySvelte==1.0.0) (3.5.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from PySvelte==1.0.0) (4.51.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from PySvelte==1.0.0) (4.67.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from PySvelte==1.0.0) (2.2.3)\n",
      "Collecting typeguard~=2.0 (from PySvelte==1.0.0)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->PySvelte==1.0.0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->PySvelte==1.0.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->PySvelte==1.0.0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->PySvelte==1.0.0) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->PySvelte==1.0.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->PySvelte==1.0.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->PySvelte==1.0.0) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->PySvelte==1.0.0) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->PySvelte==1.0.0) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->PySvelte==1.0.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->PySvelte==1.0.0) (6.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->PySvelte==1.0.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->PySvelte==1.0.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->PySvelte==1.0.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->PySvelte==1.0.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->PySvelte==1.0.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->PySvelte==1.0.0) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->PySvelte==1.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->PySvelte==1.0.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->PySvelte==1.0.0) (2025.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->PySvelte==1.0.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->PySvelte==1.0.0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->PySvelte==1.0.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->PySvelte==1.0.0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->PySvelte==1.0.0) (0.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->PySvelte==1.0.0) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->PySvelte==1.0.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->PySvelte==1.0.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->PySvelte==1.0.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->PySvelte==1.0.0) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->PySvelte==1.0.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->PySvelte==1.0.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->PySvelte==1.0.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->PySvelte==1.0.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->PySvelte==1.0.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->PySvelte==1.0.0) (2024.2.0)\n",
      "Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: PySvelte\n",
      "  Building wheel for PySvelte (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PySvelte: filename=PySvelte-1.0.0-py3-none-any.whl size=158318 sha256=841d6f8c17d7ccc32b3c4ec89e731effe19331babc2a72d68d7c2c26b32f4445\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-idy4i5tx/wheels/40/95/dc/bd1a06dc2ca83b8d30d7b25b683b2e3833dc1f7f1e90a33273\n",
      "Successfully built PySvelte\n",
      "Installing collected packages: typeguard, PySvelte\n",
      "  Attempting uninstall: typeguard\n",
      "    Found existing installation: typeguard 4.4.1\n",
      "    Uninstalling typeguard-4.4.1:\n",
      "      Successfully uninstalled typeguard-4.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.16.1 requires typeguard<5,>=3, but you have typeguard 2.13.3 which is incompatible.\n",
      "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PySvelte-1.0.0 typeguard-2.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: fancy_einsum in /usr/local/lib/python3.11/dist-packages (0.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting ekphrasis\n",
      "  Downloading ekphrasis-0.5.4-py3-none-any.whl.metadata (610 bytes)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (2.5.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (4.67.1)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (0.4.6)\n",
      "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (5.10.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (3.7.5)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (3.9.1)\n",
      "Collecting ftfy (from ekphrasis)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ekphrasis) (1.26.4)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->ekphrasis) (0.2.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ekphrasis) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->ekphrasis) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->ekphrasis) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->ekphrasis) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->ekphrasis) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->ekphrasis) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->ekphrasis) (2.4.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->ekphrasis) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->ekphrasis) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->ekphrasis) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ekphrasis) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ekphrasis) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->ekphrasis) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->ekphrasis) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->ekphrasis) (2024.2.0)\n",
      "Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ftfy, ekphrasis\n",
      "Successfully installed ekphrasis-0.5.4 ftfy-6.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  print(\"Running as a Colab notebook\")\n",
    "  %pip install git+https://github.com/neelnanda-io/Easy-Transformer.git@clean-transformer-demo\n",
    "  # Install another version of node that makes PySvelte work way faster\n",
    "  !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "  %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "  %pip install fancy_einsum\n",
    "  %pip install einops\n",
    "  %pip install ekphrasis\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print(\"Running as a Jupyter notebook - intended for develop6ment only!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-14T15:27:40.945504Z",
     "iopub.status.busy": "2025-06-14T15:27:40.945232Z",
     "iopub.status.idle": "2025-06-14T15:27:52.653758Z",
     "shell.execute_reply": "2025-06-14T15:27:52.653068Z",
     "shell.execute_reply.started": "2025-06-14T15:27:40.945479Z"
    }
   },
   "outputs": [],
   "source": [
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "from easy_transformer import EasyTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from easy_transformer.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "import plotly.express as px\n",
    "import pysvelte\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:27:52.654629Z",
     "iopub.status.busy": "2025-06-14T15:27:52.654422Z",
     "iopub.status.idle": "2025-06-14T15:27:52.877798Z",
     "shell.execute_reply": "2025-06-14T15:27:52.877175Z",
     "shell.execute_reply.started": "2025-06-14T15:27:52.654612Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:27:52.879681Z",
     "iopub.status.busy": "2025-06-14T15:27:52.879211Z",
     "iopub.status.idle": "2025-06-14T15:27:56.794846Z",
     "shell.execute_reply": "2025-06-14T15:27:56.794250Z",
     "shell.execute_reply.started": "2025-06-14T15:27:52.879663Z"
    }
   },
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "#from transformers import BertTokenizer\n",
    "import string \n",
    "import re\n",
    "import spacy\n",
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "from spacy.symbols import ORTH,NORM,LEMMA\n",
    "import string \n",
    "from spacy.lang.char_classes import LIST_PUNCT, LIST_ELLIPSES, LIST_QUOTES, LIST_CURRENCY\n",
    "from spacy.lang.char_classes import LIST_ICONS, HYPHENS, CURRENCY, UNITS\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, ALPHA_LOWER, ALPHA_UPPER, ALPHA, PUNCT\n",
    "from spacy.util import compile_infix_regex, compile_prefix_regex, compile_suffix_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:27:56.796196Z",
     "iopub.status.busy": "2025-06-14T15:27:56.795587Z",
     "iopub.status.idle": "2025-06-14T15:28:19.275500Z",
     "shell.execute_reply": "2025-06-14T15:28:19.274605Z",
     "shell.execute_reply.started": "2025-06-14T15:27:56.796167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64020901fa748bab66c02f8c4038f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 15:28:02.974468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749914883.167501      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749914883.222663      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e80a35a03a4cbb98cedf7e33ef35b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab750d2629df437793718fcd9944380d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d99649eb55b4e6fb78b4bb3bebb4c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a90da9d044441990b1a99c302d174a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c295d7d8a32a4d5b8ed6a1ea8ae23051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284fed9e172c4b509259d24aa528e7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:19.277563Z",
     "iopub.status.busy": "2025-06-14T15:28:19.276955Z",
     "iopub.status.idle": "2025-06-14T15:28:19.283671Z",
     "shell.execute_reply": "2025-06-14T15:28:19.282738Z",
     "shell.execute_reply.started": "2025-06-14T15:28:19.277544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12, n_classes=3)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "    n_classes: int = 3\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:19.284751Z",
     "iopub.status.busy": "2025-06-14T15:28:19.284516Z",
     "iopub.status.idle": "2025-06-14T15:28:19.300483Z",
     "shell.execute_reply": "2025-06-14T15:28:19.299939Z",
     "shell.execute_reply.started": "2025-06-14T15:28:19.284726Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "\n",
    "        residual = residual - einops.reduce(residual, \"batch position d_model -> batch position 1\",reduction=\"mean\")\n",
    "        scale = (einops.reduce(residual.pow(2),\"batch position d_model -> batch position 1\",reduction=\"mean\" ) + + cfg.layer_norm_eps).sqrt()\n",
    "\n",
    "        outputs = residual/scale\n",
    "\n",
    "        outputs = outputs*self.w + self.b\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:19.301629Z",
     "iopub.status.busy": "2025-06-14T15:28:19.301354Z",
     "iopub.status.idle": "2025-06-14T15:28:19.317462Z",
     "shell.execute_reply": "2025-06-14T15:28:19.316806Z",
     "shell.execute_reply.started": "2025-06-14T15:28:19.301608Z"
    }
   },
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        \"YOUR CODE HERE\"\n",
    "        if self.cfg.debug: print(\"Tokens:\", tokens.shape)\n",
    "        embeddings = self.W_E[tokens,:]\n",
    "        if self.cfg.debug: print(\"Embeddings:\", embeddings.shape)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:19.318456Z",
     "iopub.status.busy": "2025-06-14T15:28:19.318226Z",
     "iopub.status.idle": "2025-06-14T15:28:19.331066Z",
     "shell.execute_reply": "2025-06-14T15:28:19.330339Z",
     "shell.execute_reply.started": "2025-06-14T15:28:19.318429Z"
    }
   },
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"YOUR CODE HERE\"\n",
    "        if self.cfg.debug: print(\"Tokens:\", tokens.shape)\n",
    "        pos_embed = self.W_pos[:tokens.size(1), :] # [position, d_model]\n",
    "        pos_embed = einops.repeat(pos_embed, \"position d_model -> batch position d_model\", batch=tokens.size(0))\n",
    "        if self.cfg.debug: print(\"POS Embeddings:\", pos_embeddings.shape)\n",
    "        return pos_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:19.333704Z",
     "iopub.status.busy": "2025-06-14T15:28:19.333507Z",
     "iopub.status.idle": "2025-06-14T15:28:33.258515Z",
     "shell.execute_reply": "2025-06-14T15:28:33.257650Z",
     "shell.execute_reply.started": "2025-06-14T15:28:19.333690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n",
      "pysvelte components appear to be unbuilt or stale\n",
      "Running npm install...\n",
      "\n",
      "added 642 packages, and audited 643 packages in 10s\n",
      "\n",
      "12 packages are looking for funding\n",
      "  run `npm fund` for details\n",
      "\n",
      "47 vulnerabilities (4 low, 5 moderate, 33 high, 5 critical)\n",
      "\n",
      "To address issues that do not require attention, run:\n",
      "  npm audit fix\n",
      "\n",
      "To address all issues (including breaking changes), run:\n",
      "  npm audit fix --force\n",
      "\n",
      "Run `npm audit` for details.\n",
      "Building pysvelte components with webpack...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "npm notice \n",
      "npm notice New major version of npm available! 8.19.4 -> 11.4.2\n",
      "npm notice Changelog: <https://github.com/npm/cli/releases/tag/v11.4.2>\n",
      "npm notice Run `npm install -g npm@11.4.2` to update!\n",
      "npm notice \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> pysvelte@1.0.0 webpack\n",
      "> node ./node_modules/.bin/webpack\n",
      "\n",
      "entry: {\"loader\":\"./src/loader.js\",\"AttentionMulti\":\"./src/AttentionMulti.svelte\",\"Hello\":\"./src/Hello.svelte\",\"TextSingle\":\"./src/TextSingle.svelte\",\"TopKTable\":\"./src/TopKTable.svelte\"}\n",
      "asset AttentionMulti.js 40.2 KiB [emitted] [minimized] (name: AttentionMulti)\n",
      "asset TopKTable.js 32.2 KiB [emitted] [minimized] (name: TopKTable)\n",
      "asset loader.js 32.1 KiB [emitted] [minimized] (name: loader)\n",
      "asset TextSingle.js 23.7 KiB [emitted] [minimized] (name: TextSingle)\n",
      "asset Hello.js 19.9 KiB [emitted] [minimized] (name: Hello)\n",
      "runtime modules 2.18 KiB 11 modules\n",
      "orphan modules 72.9 KiB [orphan] 6 modules\n",
      "cacheable modules 437 KiB\n",
      "  modules by path ./node_modules/ 116 KiB\n",
      "    modules by path ./node_modules/pako/lib/ 102 KiB 12 modules\n",
      "    ./node_modules/numpy-parser/dist/main.js 3.63 KiB [built] [code generated]\n",
      "    ./node_modules/ndarray/ndarray.js 9.62 KiB [built] [code generated]\n",
      "    ./node_modules/iota-array/iota.js 150 bytes [built] [code generated]\n",
      "    ./node_modules/is-buffer/index.js 698 bytes [built] [code generated]\n",
      "  modules by path ./src/ 321 KiB\n",
      "    modules by path ./src/*.svelte + 1 modules 144 KiB 2 modules\n",
      "    ./src/loader.js 2.84 KiB [built] [code generated]\n",
      "    ./src/AttentionMulti.svelte + 6 modules 110 KiB [built] [code generated]\n",
      "    ./src/TextSingle.svelte + 2 modules 64.5 KiB [built] [code generated]\n",
      "webpack 5.16.0 compiled successfully in 1487 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>var loader;loader=(()=>{var Ze={907:k=>{\"use strict\";function I(E){for(var T=new Array(E),n=0;n<E;++n)T[n]=n;return T}k.exports=I},738:k=>{/*!\n",
       " * Determine if an object is a Buffer\n",
       " *\n",
       " * @author   Feross Aboukhadijeh <https://feross.org>\n",
       " * @license  MIT\n",
       " */k.exports=function(T){return T!=null&&(I(T)||E(T)||!!T._isBuffer)};function I(T){return!!T.constructor&&typeof T.constructor.isBuffer==\"function\"&&T.constructor.isBuffer(T)}function E(T){return typeof T.readFloatLE==\"function\"&&typeof T.slice==\"function\"&&I(T.slice(0,0))}},861:(k,I,E)=>{var T=E(907),n=E(738),g=typeof Float64Array!=\"undefined\";function s(l,h){return l[0]-h[0]}function m(){var l=this.stride,h=new Array(l.length),c;for(c=0;c<h.length;++c)h[c]=[Math.abs(l[c]),c];h.sort(s);var N=new Array(h.length);for(c=0;c<N.length;++c)N[c]=h[c][1];return N}function _(l,h){var c=[\"View\",h,\"d\",l].join(\"\");h<0&&(c=\"View_Nil\"+l);var N=l===\"generic\";if(h===-1){var f=\"function \"+c+\"(a){this.data=a;};var proto=\"+c+\".prototype;proto.dtype='\"+l+\"';proto.index=function(){return -1};proto.size=0;proto.dimension=-1;proto.shape=proto.stride=proto.order=[];proto.lo=proto.hi=proto.transpose=proto.step=function(){return new \"+c+\"(this.data);};proto.get=proto.set=function(){};proto.pick=function(){return null};return function construct_\"+c+\"(a){return new \"+c+\"(a);}\",O=new Function(f);return O()}else if(h===0){var f=\"function \"+c+\"(a,d) {this.data = a;this.offset = d};var proto=\"+c+\".prototype;proto.dtype='\"+l+\"';proto.index=function(){return this.offset};proto.dimension=0;proto.size=1;proto.shape=proto.stride=proto.order=[];proto.lo=proto.hi=proto.transpose=proto.step=function \"+c+\"_copy() {return new \"+c+\"(this.data,this.offset)};proto.pick=function \"+c+\"_pick(){return TrivialArray(this.data);};proto.valueOf=proto.get=function \"+c+\"_get(){return \"+(N?\"this.data.get(this.offset)\":\"this.data[this.offset]\")+\"};proto.set=function \"+c+\"_set(v){return \"+(N?\"this.data.set(this.offset,v)\":\"this.data[this.offset]=v\")+\"};return function construct_\"+c+\"(a,b,c,d){return new \"+c+\"(a,d)}\",O=new Function(\"TrivialArray\",f);return O(u[l][0])}var f=[\"'use strict'\"],d=T(h),M=d.map(function(p){return\"i\"+p}),L=\"this.offset+\"+d.map(function(p){return\"this.stride[\"+p+\"]*i\"+p}).join(\"+\"),j=d.map(function(p){return\"b\"+p}).join(\",\"),D=d.map(function(p){return\"c\"+p}).join(\",\");f.push(\"function \"+c+\"(a,\"+j+\",\"+D+\",d){this.data=a\",\"this.shape=[\"+j+\"]\",\"this.stride=[\"+D+\"]\",\"this.offset=d|0}\",\"var proto=\"+c+\".prototype\",\"proto.dtype='\"+l+\"'\",\"proto.dimension=\"+h),f.push(\"Object.defineProperty(proto,'size',{get:function \"+c+\"_size(){return \"+d.map(function(p){return\"this.shape[\"+p+\"]\"}).join(\"*\"),\"}})\"),h===1?f.push(\"proto.order=[0]\"):(f.push(\"Object.defineProperty(proto,'order',{get:\"),h<4?(f.push(\"function \"+c+\"_order(){\"),h===2?f.push(\"return (Math.abs(this.stride[0])>Math.abs(this.stride[1]))?[1,0]:[0,1]}})\"):h===3&&f.push(\"var s0=Math.abs(this.stride[0]),s1=Math.abs(this.stride[1]),s2=Math.abs(this.stride[2]);if(s0>s1){if(s1>s2){return [2,1,0];}else if(s0>s2){return [1,2,0];}else{return [1,0,2];}}else if(s0>s2){return [2,0,1];}else if(s2>s1){return [0,1,2];}else{return [0,2,1];}}})\")):f.push(\"ORDER})\")),f.push(\"proto.set=function \"+c+\"_set(\"+M.join(\",\")+\",v){\"),N?f.push(\"return this.data.set(\"+L+\",v)}\"):f.push(\"return this.data[\"+L+\"]=v}\"),f.push(\"proto.get=function \"+c+\"_get(\"+M.join(\",\")+\"){\"),N?f.push(\"return this.data.get(\"+L+\")}\"):f.push(\"return this.data[\"+L+\"]}\"),f.push(\"proto.index=function \"+c+\"_index(\",M.join(),\"){return \"+L+\"}\"),f.push(\"proto.hi=function \"+c+\"_hi(\"+M.join(\",\")+\"){return new \"+c+\"(this.data,\"+d.map(function(p){return[\"(typeof i\",p,\"!=='number'||i\",p,\"<0)?this.shape[\",p,\"]:i\",p,\"|0\"].join(\"\")}).join(\",\")+\",\"+d.map(function(p){return\"this.stride[\"+p+\"]\"}).join(\",\")+\",this.offset)}\");var A=d.map(function(p){return\"a\"+p+\"=this.shape[\"+p+\"]\"}),o=d.map(function(p){return\"c\"+p+\"=this.stride[\"+p+\"]\"});f.push(\"proto.lo=function \"+c+\"_lo(\"+M.join(\",\")+\"){var b=this.offset,d=0,\"+A.join(\",\")+\",\"+o.join(\",\"));for(var i=0;i<h;++i)f.push(\"if(typeof i\"+i+\"==='number'&&i\"+i+\">=0){d=i\"+i+\"|0;b+=c\"+i+\"*d;a\"+i+\"-=d}\");f.push(\"return new \"+c+\"(this.data,\"+d.map(function(p){return\"a\"+p}).join(\",\")+\",\"+d.map(function(p){return\"c\"+p}).join(\",\")+\",b)}\"),f.push(\"proto.step=function \"+c+\"_step(\"+M.join(\",\")+\"){var \"+d.map(function(p){return\"a\"+p+\"=this.shape[\"+p+\"]\"}).join(\",\")+\",\"+d.map(function(p){return\"b\"+p+\"=this.stride[\"+p+\"]\"}).join(\",\")+\",c=this.offset,d=0,ceil=Math.ceil\");for(var i=0;i<h;++i)f.push(\"if(typeof i\"+i+\"==='number'){d=i\"+i+\"|0;if(d<0){c+=b\"+i+\"*(a\"+i+\"-1);a\"+i+\"=ceil(-a\"+i+\"/d)}else{a\"+i+\"=ceil(a\"+i+\"/d)}b\"+i+\"*=d}\");f.push(\"return new \"+c+\"(this.data,\"+d.map(function(p){return\"a\"+p}).join(\",\")+\",\"+d.map(function(p){return\"b\"+p}).join(\",\")+\",c)}\");for(var Z=new Array(h),R=new Array(h),i=0;i<h;++i)Z[i]=\"a[i\"+i+\"]\",R[i]=\"b[i\"+i+\"]\";f.push(\"proto.transpose=function \"+c+\"_transpose(\"+M+\"){\"+M.map(function(p,U){return p+\"=(\"+p+\"===undefined?\"+U+\":\"+p+\"|0)\"}).join(\";\"),\"var a=this.shape,b=this.stride;return new \"+c+\"(this.data,\"+Z.join(\",\")+\",\"+R.join(\",\")+\",this.offset)}\"),f.push(\"proto.pick=function \"+c+\"_pick(\"+M+\"){var a=[],b=[],c=this.offset\");for(var i=0;i<h;++i)f.push(\"if(typeof i\"+i+\"==='number'&&i\"+i+\">=0){c=(c+this.stride[\"+i+\"]*i\"+i+\")|0}else{a.push(this.shape[\"+i+\"]);b.push(this.stride[\"+i+\"])}\");f.push(\"var ctor=CTOR_LIST[a.length+1];return ctor(this.data,a,b,c)}\"),f.push(\"return function construct_\"+c+\"(data,shape,stride,offset){return new \"+c+\"(data,\"+d.map(function(p){return\"shape[\"+p+\"]\"}).join(\",\")+\",\"+d.map(function(p){return\"stride[\"+p+\"]\"}).join(\",\")+\",offset)}\");var O=new Function(\"CTOR_LIST\",\"ORDER\",f.join(`\n",
       "`));return O(u[l],m)}function C(l){if(n(l))return\"buffer\";if(g)switch(Object.prototype.toString.call(l)){case\"[object Float64Array]\":return\"float64\";case\"[object Float32Array]\":return\"float32\";case\"[object Int8Array]\":return\"int8\";case\"[object Int16Array]\":return\"int16\";case\"[object Int32Array]\":return\"int32\";case\"[object Uint8Array]\":return\"uint8\";case\"[object Uint16Array]\":return\"uint16\";case\"[object Uint32Array]\":return\"uint32\";case\"[object Uint8ClampedArray]\":return\"uint8_clamped\";case\"[object BigInt64Array]\":return\"bigint64\";case\"[object BigUint64Array]\":return\"biguint64\"}return Array.isArray(l)?\"array\":\"generic\"}var u={float32:[],float64:[],int8:[],int16:[],int32:[],uint8:[],uint16:[],uint32:[],array:[],uint8_clamped:[],bigint64:[],biguint64:[],buffer:[],generic:[]};function v(l,h,c,N){if(l===void 0){var D=u.array[0];return D([])}else typeof l==\"number\"&&(l=[l]);h===void 0&&(h=[l.length]);var f=h.length;if(c===void 0){c=new Array(f);for(var d=f-1,M=1;d>=0;--d)c[d]=M,M*=h[d]}if(N===void 0){N=0;for(var d=0;d<f;++d)c[d]<0&&(N-=(h[d]-1)*c[d])}for(var L=C(l),j=u[L];j.length<=f+1;)j.push(_(L,j.length-1));var D=j[f+1];return D(l,h,c,N)}k.exports=v},829:(k,I)=>{\"use strict\";var E;E={value:!0},I.g=m;function T(u,v){if(!(u instanceof v))throw new TypeError(\"Cannot call a class as a function\")}function n(u,v){for(var l,h=0;h<v.length;h++)l=v[h],l.enumerable=l.enumerable||!1,l.configurable=!0,\"value\"in l&&(l.writable=!0),Object.defineProperty(u,l.key,l)}function g(u,v,l){return v&&n(u.prototype,v),l&&n(u,l),u}var s=function(){function u(v){T(this,u),v instanceof DataView?this.dataView=v:v instanceof ArrayBuffer&&(this.dataView=new DataView(v)),this.offset=0}return g(u,[{key:\"readBytes\",value:function(l){var h=new DataView(this.dataView.buffer,this.offset,l);return this.offset+=l,h}},{key:\"readAndASCIIDecodeBytes\",value:function(l){var h=new Uint8Array(this.dataView.buffer,this.offset,l);return this.offset+=l,this._decodeASCIIByteArray(h)}},{key:\"readUint8\",value:function(){var l=0<arguments.length&&arguments[0]!==void 0&&arguments[0],h=this.dataView.getUint8(this.offset,l);return this.offset+=Uint8Array.BYTES_PER_ELEMENT,h}},{key:\"readUint16\",value:function(){var l=0<arguments.length&&arguments[0]!==void 0&&arguments[0],h=this.dataView.getUint16(this.offset,l);return this.offset+=Uint16Array.BYTES_PER_ELEMENT,h}},{key:\"readUint32\",value:function(){var l=0<arguments.length&&arguments[0]!==void 0&&arguments[0],h=this.dataView.getUint32(this.offset,l);return this.offset+=Uint32Array.BYTES_PER_ELEMENT,h}},{key:\"_decodeASCIIByteArray\",value:function(l){var h=String.fromCharCode,c=[],N=!0,f=!1,d=void 0;try{for(var M,L=l[Symbol.iterator]();!(N=(M=L.next()).done);N=!0){var j=M.value,D=h(j);c.push(D)}}catch(A){f=!0,d=A}finally{try{N||L.return==null||L.return()}finally{if(f)throw d}}return c.join(\"\")}}]),u}();function m(u){if(!u instanceof ArrayBuffer)throw new Error(\"Argument must be an ArrayBuffer.\");var v=new s(u),l=v.readUint8(),h=v.readAndASCIIDecodeBytes(5);if(l!=147||h!=\"NUMPY\")throw new Error('unknown file type: \"'.concat(l).concat(h,'\"'));var c,N=v.readUint8(),f=v.readUint8();c=1>=N?v.readUint16(!0):v.readUint32(!0);var d=10+c;d%16!=0&&console.warn(\"NPY file header is incorrectly padded. (\".concat(d,\" is not evenly divisible by 16.)\"));var M=v.readAndASCIIDecodeBytes(c),L=_(M);if(L.fortran_order)throw new Error(\"NPY file is written in Fortran byte order, support for this byte order is not yet implemented.\");var j=C(L.descr),D=new j(u,v.offset);return{data:D,shape:L.shape}}function _(u){var v=u.toLowerCase().replace(\"(\",\"[\").replace(\"),\",\"]\").replace(\"[,\",\"[1,]\").replace(\",]\",\",1]\").replace(/'/g,'\"');return JSON.parse(v)}function C(u){switch(u){case\"|u1\":return Uint8Array;case\"<u2\":return Uint16Array;case\"<u4\":return Uint32Array;case\"<u8\":throw new Error(\"Because JavaScript doesn't currently include standard support for 64-bit unsigned integer values, support for this dtype is not yet implemented.\");case\"|i1\":return Int8Array;case\"<i2\":return Int16Array;case\"<i4\":return Int32Array;case\"<i8\":throw new Error(\"Because JavaScript doesn't currently include standard support for 64-bit integer values, support for this dtype is not yet implemented.\");case\"<f2\":throw new Error(\"Because JavaScript doesn't currently include standard support for 16-bit floating point values, support for this dtype is not yet implemented.\");case\"<f4\":return Float32Array;case\"<f8\":return Float64Array;default:throw new Error(\"Unknown or not yet implemented numpy dtype description: \"+dtype)}}},843:(k,I,E)=>{\"use strict\";var T;const n=E(948),g=E(236),s=E(373),m=E(898),_=E(292),C=E(401),u=Object.prototype.toString,{Z_NO_FLUSH:v,Z_FINISH:l,Z_OK:h,Z_STREAM_END:c,Z_NEED_DICT:N,Z_STREAM_ERROR:f,Z_DATA_ERROR:d,Z_MEM_ERROR:M}=E(619);function L(A){this.options=g.assign({chunkSize:65536,windowBits:15,to:\"\"},A||{});const o=this.options;o.raw&&o.windowBits>=0&&o.windowBits<16&&(o.windowBits=-o.windowBits,o.windowBits===0&&(o.windowBits=-15)),o.windowBits>=0&&o.windowBits<16&&!(A&&A.windowBits)&&(o.windowBits+=32),o.windowBits>15&&o.windowBits<48&&(o.windowBits&15)===0&&(o.windowBits|=15),this.err=0,this.msg=\"\",this.ended=!1,this.chunks=[],this.strm=new _,this.strm.avail_out=0;let i=n.inflateInit2(this.strm,o.windowBits);if(i!==h)throw new Error(m[i]);if(this.header=new C,n.inflateGetHeader(this.strm,this.header),o.dictionary&&(typeof o.dictionary==\"string\"?o.dictionary=s.string2buf(o.dictionary):u.call(o.dictionary)===\"[object ArrayBuffer]\"&&(o.dictionary=new Uint8Array(o.dictionary)),o.raw&&(i=n.inflateSetDictionary(this.strm,o.dictionary),i!==h)))throw new Error(m[i])}L.prototype.push=function(A,o){const i=this.strm,Z=this.options.chunkSize,R=this.options.dictionary;let O,p,U;if(this.ended)return!1;for(o===~~o?p=o:p=o===!0?l:v,u.call(A)===\"[object ArrayBuffer]\"?i.input=new Uint8Array(A):i.input=A,i.next_in=0,i.avail_in=i.input.length;;){for(i.avail_out===0&&(i.output=new Uint8Array(Z),i.next_out=0,i.avail_out=Z),O=n.inflate(i,p),O===N&&R&&(O=n.inflateSetDictionary(i,R),O===h?O=n.inflate(i,p):O===d&&(O=N));i.avail_in>0&&O===c&&i.state.wrap>0&&A[i.next_in]!==0;)n.inflateReset(i),O=n.inflate(i,p);switch(O){case f:case d:case N:case M:return this.onEnd(O),this.ended=!0,!1}if(U=i.avail_out,i.next_out&&(i.avail_out===0||O===c))if(this.options.to===\"string\"){let B=s.utf8border(i.output,i.next_out),K=i.next_out-B,V=s.buf2string(i.output,B);i.next_out=K,i.avail_out=Z-K,K&&i.output.set(i.output.subarray(B,B+K),0),this.onData(V)}else this.onData(i.output.length===i.next_out?i.output:i.output.subarray(0,i.next_out));if(!(O===h&&U===0)){if(O===c)return O=n.inflateEnd(this.strm),this.onEnd(O),this.ended=!0,!0;if(i.avail_in===0)break}}return!0},L.prototype.onData=function(A){this.chunks.push(A)},L.prototype.onEnd=function(A){A===h&&(this.options.to===\"string\"?this.result=this.chunks.join(\"\"):this.result=g.flattenChunks(this.chunks)),this.chunks=[],this.err=A,this.msg=this.strm.msg};function j(A,o){const i=new L(o);if(i.push(A),i.err)throw i.msg||m[i.err];return i.result}function D(A,o){return o=o||{},o.raw=!0,j(A,o)}T=L,k.exports.rr=j,T=D,T=j,E(619)},236:k=>{\"use strict\";const I=(E,T)=>Object.prototype.hasOwnProperty.call(E,T);k.exports.assign=function(E){const T=Array.prototype.slice.call(arguments,1);for(;T.length;){const n=T.shift();if(!!n){if(typeof n!=\"object\")throw new TypeError(n+\"must be non-object\");for(const g in n)I(n,g)&&(E[g]=n[g])}}return E},k.exports.flattenChunks=E=>{let T=0;for(let g=0,s=E.length;g<s;g++)T+=E[g].length;const n=new Uint8Array(T);for(let g=0,s=0,m=E.length;g<m;g++){let _=E[g];n.set(_,s),s+=_.length}return n}},373:k=>{\"use strict\";let I=!0;try{String.fromCharCode.apply(null,new Uint8Array(1))}catch(n){I=!1}const E=new Uint8Array(256);for(let n=0;n<256;n++)E[n]=n>=252?6:n>=248?5:n>=240?4:n>=224?3:n>=192?2:1;E[254]=E[254]=1,k.exports.string2buf=n=>{let g,s,m,_,C,u=n.length,v=0;for(_=0;_<u;_++)s=n.charCodeAt(_),(s&64512)===55296&&_+1<u&&(m=n.charCodeAt(_+1),(m&64512)===56320&&(s=65536+(s-55296<<10)+(m-56320),_++)),v+=s<128?1:s<2048?2:s<65536?3:4;for(g=new Uint8Array(v),C=0,_=0;C<v;_++)s=n.charCodeAt(_),(s&64512)===55296&&_+1<u&&(m=n.charCodeAt(_+1),(m&64512)===56320&&(s=65536+(s-55296<<10)+(m-56320),_++)),s<128?g[C++]=s:s<2048?(g[C++]=192|s>>>6,g[C++]=128|s&63):s<65536?(g[C++]=224|s>>>12,g[C++]=128|s>>>6&63,g[C++]=128|s&63):(g[C++]=240|s>>>18,g[C++]=128|s>>>12&63,g[C++]=128|s>>>6&63,g[C++]=128|s&63);return g};const T=(n,g)=>{if(g<65534&&n.subarray&&I)return String.fromCharCode.apply(null,n.length===g?n:n.subarray(0,g));let s=\"\";for(let m=0;m<g;m++)s+=String.fromCharCode(n[m]);return s};k.exports.buf2string=(n,g)=>{let s,m;const _=g||n.length,C=new Array(_*2);for(m=0,s=0;s<_;){let u=n[s++];if(u<128){C[m++]=u;continue}let v=E[u];if(v>4){C[m++]=65533,s+=v-1;continue}for(u&=v===2?31:v===3?15:7;v>1&&s<_;)u=u<<6|n[s++]&63,v--;if(v>1){C[m++]=65533;continue}u<65536?C[m++]=u:(u-=65536,C[m++]=55296|u>>10&1023,C[m++]=56320|u&1023)}return T(C,m)},k.exports.utf8border=(n,g)=>{g=g||n.length,g>n.length&&(g=n.length);let s=g-1;for(;s>=0&&(n[s]&192)===128;)s--;return s<0||s===0?g:s+E[n[s]]>g?s:g}},69:k=>{\"use strict\";const I=(E,T,n,g)=>{let s=E&65535|0,m=E>>>16&65535|0,_=0;for(;n!==0;){_=n>2e3?2e3:n,n-=_;do s=s+T[g++]|0,m=m+s|0;while(--_);s%=65521,m%=65521}return s|m<<16|0};k.exports=I},619:k=>{\"use strict\";k.exports={Z_NO_FLUSH:0,Z_PARTIAL_FLUSH:1,Z_SYNC_FLUSH:2,Z_FULL_FLUSH:3,Z_FINISH:4,Z_BLOCK:5,Z_TREES:6,Z_OK:0,Z_STREAM_END:1,Z_NEED_DICT:2,Z_ERRNO:-1,Z_STREAM_ERROR:-2,Z_DATA_ERROR:-3,Z_MEM_ERROR:-4,Z_BUF_ERROR:-5,Z_NO_COMPRESSION:0,Z_BEST_SPEED:1,Z_BEST_COMPRESSION:9,Z_DEFAULT_COMPRESSION:-1,Z_FILTERED:1,Z_HUFFMAN_ONLY:2,Z_RLE:3,Z_FIXED:4,Z_DEFAULT_STRATEGY:0,Z_BINARY:0,Z_TEXT:1,Z_UNKNOWN:2,Z_DEFLATED:8}},869:k=>{\"use strict\";const I=()=>{let n,g=[];for(var s=0;s<256;s++){n=s;for(var m=0;m<8;m++)n=n&1?3988292384^n>>>1:n>>>1;g[s]=n}return g},E=new Uint32Array(I()),T=(n,g,s,m)=>{const _=E,C=m+s;n^=-1;for(let u=m;u<C;u++)n=n>>>8^_[(n^g[u])&255];return n^-1};k.exports=T},401:k=>{\"use strict\";function I(){this.text=0,this.time=0,this.xflags=0,this.os=0,this.extra=null,this.extra_len=0,this.name=\"\",this.comment=\"\",this.hcrc=0,this.done=!1}k.exports=I},264:k=>{\"use strict\";k.exports=function(n,g){let s,m,_,C,u,v,l,h,c,N,f,d,M,L,j,D,A,o,i,Z,R,O,p,U;const B=n.state;s=n.next_in,p=n.input,m=s+(n.avail_in-5),_=n.next_out,U=n.output,C=_-(g-n.avail_out),u=_+(n.avail_out-257),v=B.dmax,l=B.wsize,h=B.whave,c=B.wnext,N=B.window,f=B.hold,d=B.bits,M=B.lencode,L=B.distcode,j=(1<<B.lenbits)-1,D=(1<<B.distbits)-1;e:do{d<15&&(f+=p[s++]<<d,d+=8,f+=p[s++]<<d,d+=8),A=M[f&j];t:for(;;){if(o=A>>>24,f>>>=o,d-=o,o=A>>>16&255,o===0)U[_++]=A&65535;else if(o&16){i=A&65535,o&=15,o&&(d<o&&(f+=p[s++]<<d,d+=8),i+=f&(1<<o)-1,f>>>=o,d-=o),d<15&&(f+=p[s++]<<d,d+=8,f+=p[s++]<<d,d+=8),A=L[f&D];i:for(;;){if(o=A>>>24,f>>>=o,d-=o,o=A>>>16&255,o&16){if(Z=A&65535,o&=15,d<o&&(f+=p[s++]<<d,d+=8,d<o&&(f+=p[s++]<<d,d+=8)),Z+=f&(1<<o)-1,Z>v){n.msg=\"invalid distance too far back\",B.mode=30;break e}if(f>>>=o,d-=o,o=_-C,Z>o){if(o=Z-o,o>h&&B.sane){n.msg=\"invalid distance too far back\",B.mode=30;break e}if(R=0,O=N,c===0){if(R+=l-o,o<i){i-=o;do U[_++]=N[R++];while(--o);R=_-Z,O=U}}else if(c<o){if(R+=l+c-o,o-=c,o<i){i-=o;do U[_++]=N[R++];while(--o);if(R=0,c<i){o=c,i-=o;do U[_++]=N[R++];while(--o);R=_-Z,O=U}}}else if(R+=c-o,o<i){i-=o;do U[_++]=N[R++];while(--o);R=_-Z,O=U}for(;i>2;)U[_++]=O[R++],U[_++]=O[R++],U[_++]=O[R++],i-=3;i&&(U[_++]=O[R++],i>1&&(U[_++]=O[R++]))}else{R=_-Z;do U[_++]=U[R++],U[_++]=U[R++],U[_++]=U[R++],i-=3;while(i>2);i&&(U[_++]=U[R++],i>1&&(U[_++]=U[R++]))}}else if((o&64)===0){A=L[(A&65535)+(f&(1<<o)-1)];continue i}else{n.msg=\"invalid distance code\",B.mode=30;break e}break}}else if((o&64)===0){A=M[(A&65535)+(f&(1<<o)-1)];continue t}else if(o&32){B.mode=12;break e}else{n.msg=\"invalid literal/length code\",B.mode=30;break e}break}}while(s<m&&_<u);i=d>>3,s-=i,d-=i<<3,f&=(1<<d)-1,n.next_in=s,n.next_out=_,n.avail_in=s<m?5+(m-s):5-(s-m),n.avail_out=_<u?257+(u-_):257-(_-u),B.hold=f,B.bits=d}},948:(k,I,E)=>{\"use strict\";const T=E(69),n=E(869),g=E(264),s=E(241),m=0,_=1,C=2,{Z_FINISH:u,Z_BLOCK:v,Z_TREES:l,Z_OK:h,Z_STREAM_END:c,Z_NEED_DICT:N,Z_STREAM_ERROR:f,Z_DATA_ERROR:d,Z_MEM_ERROR:M,Z_BUF_ERROR:L,Z_DEFLATED:j}=E(619),D=1,A=2,o=3,i=4,Z=5,R=6,O=7,p=8,U=9,B=10,K=11,V=12,fe=13,de=14,ne=15,ce=16,pe=17,le=18,te=19,re=20,ae=21,we=22,_e=23,ue=24,he=25,Te=26,ke=27,Oe=28,De=29,F=30,Ee=31,Pe=32,Fe=852,je=592,He=15,Re=t=>(t>>>24&255)+(t>>>8&65280)+((t&65280)<<8)+((t&255)<<24);function ze(){this.mode=0,this.last=!1,this.wrap=0,this.havedict=!1,this.flags=0,this.dmax=0,this.check=0,this.total=0,this.head=null,this.wbits=0,this.wsize=0,this.whave=0,this.wnext=0,this.window=null,this.hold=0,this.bits=0,this.length=0,this.offset=0,this.extra=0,this.lencode=null,this.distcode=null,this.lenbits=0,this.distbits=0,this.ncode=0,this.nlen=0,this.ndist=0,this.have=0,this.next=null,this.lens=new Uint16Array(320),this.work=new Uint16Array(288),this.lendyn=null,this.distdyn=null,this.sane=0,this.back=0,this.was=0}const Ie=t=>{if(!t||!t.state)return f;const x=t.state;return t.total_in=t.total_out=x.total=0,t.msg=\"\",x.wrap&&(t.adler=x.wrap&1),x.mode=D,x.last=0,x.havedict=0,x.dmax=32768,x.head=null,x.hold=0,x.bits=0,x.lencode=x.lendyn=new Int32Array(Fe),x.distcode=x.distdyn=new Int32Array(je),x.sane=1,x.back=-1,h},Ue=t=>{if(!t||!t.state)return f;const x=t.state;return x.wsize=0,x.whave=0,x.wnext=0,Ie(t)},Ce=(t,x)=>{let e;if(!t||!t.state)return f;const y=t.state;return x<0?(e=0,x=-x):(e=(x>>4)+1,x<48&&(x&=15)),x&&(x<8||x>15)?f:(y.window!==null&&y.wbits!==x&&(y.window=null),y.wrap=e,y.wbits=x,Ue(t))},Ne=(t,x)=>{if(!t)return f;const e=new ze;t.state=e,e.window=null;const y=Ce(t,x);return y!==h&&(t.state=null),y},Ye=t=>Ne(t,He);let Be=!0,Ae,Se;const Ge=t=>{if(Be){Ae=new Int32Array(512),Se=new Int32Array(32);let x=0;for(;x<144;)t.lens[x++]=8;for(;x<256;)t.lens[x++]=9;for(;x<280;)t.lens[x++]=7;for(;x<288;)t.lens[x++]=8;for(s(_,t.lens,0,288,Ae,0,t.work,{bits:9}),x=0;x<32;)t.lens[x++]=5;s(C,t.lens,0,32,Se,0,t.work,{bits:5}),Be=!1}t.lencode=Ae,t.lenbits=9,t.distcode=Se,t.distbits=5},Me=(t,x,e,y)=>{let H;const w=t.state;return w.window===null&&(w.wsize=1<<w.wbits,w.wnext=0,w.whave=0,w.window=new Uint8Array(w.wsize)),y>=w.wsize?(w.window.set(x.subarray(e-w.wsize,e),0),w.wnext=0,w.whave=w.wsize):(H=w.wsize-w.wnext,H>y&&(H=y),w.window.set(x.subarray(e-y,e-y+H),w.wnext),y-=H,y?(w.window.set(x.subarray(e-y,e),0),w.wnext=y,w.whave=w.wsize):(w.wnext+=H,w.wnext===w.wsize&&(w.wnext=0),w.whave<w.wsize&&(w.whave+=H))),0},Xe=(t,x)=>{let e,y,H,w,q,b,G,a,r,xe,z,S,be,me,Y=0,P,J,$,Q,ve,ge,X,ee;const W=new Uint8Array(4);let oe,ie;const Le=new Uint8Array([16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15]);if(!t||!t.state||!t.output||!t.input&&t.avail_in!==0)return f;e=t.state,e.mode===V&&(e.mode=fe),q=t.next_out,H=t.output,G=t.avail_out,w=t.next_in,y=t.input,b=t.avail_in,a=e.hold,r=e.bits,xe=b,z=G,ee=h;e:for(;;)switch(e.mode){case D:if(e.wrap===0){e.mode=fe;break}for(;r<16;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(e.wrap&2&&a===35615){e.check=0,W[0]=a&255,W[1]=a>>>8&255,e.check=n(e.check,W,2,0),a=0,r=0,e.mode=A;break}if(e.flags=0,e.head&&(e.head.done=!1),!(e.wrap&1)||(((a&255)<<8)+(a>>8))%31){t.msg=\"incorrect header check\",e.mode=F;break}if((a&15)!==j){t.msg=\"unknown compression method\",e.mode=F;break}if(a>>>=4,r-=4,X=(a&15)+8,e.wbits===0)e.wbits=X;else if(X>e.wbits){t.msg=\"invalid window size\",e.mode=F;break}e.dmax=1<<e.wbits,t.adler=e.check=1,e.mode=a&512?B:V,a=0,r=0;break;case A:for(;r<16;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(e.flags=a,(e.flags&255)!==j){t.msg=\"unknown compression method\",e.mode=F;break}if(e.flags&57344){t.msg=\"unknown header flags set\",e.mode=F;break}e.head&&(e.head.text=a>>8&1),e.flags&512&&(W[0]=a&255,W[1]=a>>>8&255,e.check=n(e.check,W,2,0)),a=0,r=0,e.mode=o;case o:for(;r<32;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.head&&(e.head.time=a),e.flags&512&&(W[0]=a&255,W[1]=a>>>8&255,W[2]=a>>>16&255,W[3]=a>>>24&255,e.check=n(e.check,W,4,0)),a=0,r=0,e.mode=i;case i:for(;r<16;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.head&&(e.head.xflags=a&255,e.head.os=a>>8),e.flags&512&&(W[0]=a&255,W[1]=a>>>8&255,e.check=n(e.check,W,2,0)),a=0,r=0,e.mode=Z;case Z:if(e.flags&1024){for(;r<16;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.length=a,e.head&&(e.head.extra_len=a),e.flags&512&&(W[0]=a&255,W[1]=a>>>8&255,e.check=n(e.check,W,2,0)),a=0,r=0}else e.head&&(e.head.extra=null);e.mode=R;case R:if(e.flags&1024&&(S=e.length,S>b&&(S=b),S&&(e.head&&(X=e.head.extra_len-e.length,e.head.extra||(e.head.extra=new Uint8Array(e.head.extra_len)),e.head.extra.set(y.subarray(w,w+S),X)),e.flags&512&&(e.check=n(e.check,y,S,w)),b-=S,w+=S,e.length-=S),e.length))break e;e.length=0,e.mode=O;case O:if(e.flags&2048){if(b===0)break e;S=0;do X=y[w+S++],e.head&&X&&e.length<65536&&(e.head.name+=String.fromCharCode(X));while(X&&S<b);if(e.flags&512&&(e.check=n(e.check,y,S,w)),b-=S,w+=S,X)break e}else e.head&&(e.head.name=null);e.length=0,e.mode=p;case p:if(e.flags&4096){if(b===0)break e;S=0;do X=y[w+S++],e.head&&X&&e.length<65536&&(e.head.comment+=String.fromCharCode(X));while(X&&S<b);if(e.flags&512&&(e.check=n(e.check,y,S,w)),b-=S,w+=S,X)break e}else e.head&&(e.head.comment=null);e.mode=U;case U:if(e.flags&512){for(;r<16;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(a!==(e.check&65535)){t.msg=\"header crc mismatch\",e.mode=F;break}a=0,r=0}e.head&&(e.head.hcrc=e.flags>>9&1,e.head.done=!0),t.adler=e.check=0,e.mode=V;break;case B:for(;r<32;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}t.adler=e.check=Re(a),a=0,r=0,e.mode=K;case K:if(e.havedict===0)return t.next_out=q,t.avail_out=G,t.next_in=w,t.avail_in=b,e.hold=a,e.bits=r,N;t.adler=e.check=1,e.mode=V;case V:if(x===v||x===l)break e;case fe:if(e.last){a>>>=r&7,r-=r&7,e.mode=ke;break}for(;r<3;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}switch(e.last=a&1,a>>>=1,r-=1,a&3){case 0:e.mode=de;break;case 1:if(Ge(e),e.mode=re,x===l){a>>>=2,r-=2;break e}break;case 2:e.mode=pe;break;case 3:t.msg=\"invalid block type\",e.mode=F}a>>>=2,r-=2;break;case de:for(a>>>=r&7,r-=r&7;r<32;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if((a&65535)!==(a>>>16^65535)){t.msg=\"invalid stored block lengths\",e.mode=F;break}if(e.length=a&65535,a=0,r=0,e.mode=ne,x===l)break e;case ne:e.mode=ce;case ce:if(S=e.length,S){if(S>b&&(S=b),S>G&&(S=G),S===0)break e;H.set(y.subarray(w,w+S),q),b-=S,w+=S,G-=S,q+=S,e.length-=S;break}e.mode=V;break;case pe:for(;r<14;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(e.nlen=(a&31)+257,a>>>=5,r-=5,e.ndist=(a&31)+1,a>>>=5,r-=5,e.ncode=(a&15)+4,a>>>=4,r-=4,e.nlen>286||e.ndist>30){t.msg=\"too many length or distance symbols\",e.mode=F;break}e.have=0,e.mode=le;case le:for(;e.have<e.ncode;){for(;r<3;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.lens[Le[e.have++]]=a&7,a>>>=3,r-=3}for(;e.have<19;)e.lens[Le[e.have++]]=0;if(e.lencode=e.lendyn,e.lenbits=7,oe={bits:e.lenbits},ee=s(m,e.lens,0,19,e.lencode,0,e.work,oe),e.lenbits=oe.bits,ee){t.msg=\"invalid code lengths set\",e.mode=F;break}e.have=0,e.mode=te;case te:for(;e.have<e.nlen+e.ndist;){for(;Y=e.lencode[a&(1<<e.lenbits)-1],P=Y>>>24,J=Y>>>16&255,$=Y&65535,!(P<=r);){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if($<16)a>>>=P,r-=P,e.lens[e.have++]=$;else{if($===16){for(ie=P+2;r<ie;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(a>>>=P,r-=P,e.have===0){t.msg=\"invalid bit length repeat\",e.mode=F;break}X=e.lens[e.have-1],S=3+(a&3),a>>>=2,r-=2}else if($===17){for(ie=P+3;r<ie;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}a>>>=P,r-=P,X=0,S=3+(a&7),a>>>=3,r-=3}else{for(ie=P+7;r<ie;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}a>>>=P,r-=P,X=0,S=11+(a&127),a>>>=7,r-=7}if(e.have+S>e.nlen+e.ndist){t.msg=\"invalid bit length repeat\",e.mode=F;break}for(;S--;)e.lens[e.have++]=X}}if(e.mode===F)break;if(e.lens[256]===0){t.msg=\"invalid code -- missing end-of-block\",e.mode=F;break}if(e.lenbits=9,oe={bits:e.lenbits},ee=s(_,e.lens,0,e.nlen,e.lencode,0,e.work,oe),e.lenbits=oe.bits,ee){t.msg=\"invalid literal/lengths set\",e.mode=F;break}if(e.distbits=6,e.distcode=e.distdyn,oe={bits:e.distbits},ee=s(C,e.lens,e.nlen,e.ndist,e.distcode,0,e.work,oe),e.distbits=oe.bits,ee){t.msg=\"invalid distances set\",e.mode=F;break}if(e.mode=re,x===l)break e;case re:e.mode=ae;case ae:if(b>=6&&G>=258){t.next_out=q,t.avail_out=G,t.next_in=w,t.avail_in=b,e.hold=a,e.bits=r,g(t,z),q=t.next_out,H=t.output,G=t.avail_out,w=t.next_in,y=t.input,b=t.avail_in,a=e.hold,r=e.bits,e.mode===V&&(e.back=-1);break}for(e.back=0;Y=e.lencode[a&(1<<e.lenbits)-1],P=Y>>>24,J=Y>>>16&255,$=Y&65535,!(P<=r);){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(J&&(J&240)===0){for(Q=P,ve=J,ge=$;Y=e.lencode[ge+((a&(1<<Q+ve)-1)>>Q)],P=Y>>>24,J=Y>>>16&255,$=Y&65535,!(Q+P<=r);){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}a>>>=Q,r-=Q,e.back+=Q}if(a>>>=P,r-=P,e.back+=P,e.length=$,J===0){e.mode=Te;break}if(J&32){e.back=-1,e.mode=V;break}if(J&64){t.msg=\"invalid literal/length code\",e.mode=F;break}e.extra=J&15,e.mode=we;case we:if(e.extra){for(ie=e.extra;r<ie;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.length+=a&(1<<e.extra)-1,a>>>=e.extra,r-=e.extra,e.back+=e.extra}e.was=e.length,e.mode=_e;case _e:for(;Y=e.distcode[a&(1<<e.distbits)-1],P=Y>>>24,J=Y>>>16&255,$=Y&65535,!(P<=r);){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if((J&240)===0){for(Q=P,ve=J,ge=$;Y=e.distcode[ge+((a&(1<<Q+ve)-1)>>Q)],P=Y>>>24,J=Y>>>16&255,$=Y&65535,!(Q+P<=r);){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}a>>>=Q,r-=Q,e.back+=Q}if(a>>>=P,r-=P,e.back+=P,J&64){t.msg=\"invalid distance code\",e.mode=F;break}e.offset=$,e.extra=J&15,e.mode=ue;case ue:if(e.extra){for(ie=e.extra;r<ie;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.offset+=a&(1<<e.extra)-1,a>>>=e.extra,r-=e.extra,e.back+=e.extra}if(e.offset>e.dmax){t.msg=\"invalid distance too far back\",e.mode=F;break}e.mode=he;case he:if(G===0)break e;if(S=z-G,e.offset>S){if(S=e.offset-S,S>e.whave&&e.sane){t.msg=\"invalid distance too far back\",e.mode=F;break}S>e.wnext?(S-=e.wnext,be=e.wsize-S):be=e.wnext-S,S>e.length&&(S=e.length),me=e.window}else me=H,be=q-e.offset,S=e.length;S>G&&(S=G),G-=S,e.length-=S;do H[q++]=me[be++];while(--S);e.length===0&&(e.mode=ae);break;case Te:if(G===0)break e;H[q++]=e.length,G--,e.mode=ae;break;case ke:if(e.wrap){for(;r<32;){if(b===0)break e;b--,a|=y[w++]<<r,r+=8}if(z-=G,t.total_out+=z,e.total+=z,z&&(t.adler=e.check=e.flags?n(e.check,H,z,q-z):T(e.check,H,z,q-z)),z=G,(e.flags?a:Re(a))!==e.check){t.msg=\"incorrect data check\",e.mode=F;break}a=0,r=0}e.mode=Oe;case Oe:if(e.wrap&&e.flags){for(;r<32;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(a!==(e.total&4294967295)){t.msg=\"incorrect length check\",e.mode=F;break}a=0,r=0}e.mode=De;case De:ee=c;break e;case F:ee=d;break e;case Ee:return M;case Pe:default:return f}return t.next_out=q,t.avail_out=G,t.next_in=w,t.avail_in=b,e.hold=a,e.bits=r,(e.wsize||z!==t.avail_out&&e.mode<F&&(e.mode<ke||x!==u))&&Me(t,t.output,t.next_out,z-t.avail_out)?(e.mode=Ee,M):(xe-=t.avail_in,z-=t.avail_out,t.total_in+=xe,t.total_out+=z,e.total+=z,e.wrap&&z&&(t.adler=e.check=e.flags?n(e.check,H,z,t.next_out-z):T(e.check,H,z,t.next_out-z)),t.data_type=e.bits+(e.last?64:0)+(e.mode===V?128:0)+(e.mode===re||e.mode===ne?256:0),(xe===0&&z===0||x===u)&&ee===h&&(ee=L),ee)},Ke=t=>{if(!t||!t.state)return f;let x=t.state;return x.window&&(x.window=null),t.state=null,h},Ve=(t,x)=>{if(!t||!t.state)return f;const e=t.state;return(e.wrap&2)===0?f:(e.head=x,x.done=!1,h)},We=(t,x)=>{const e=x.length;let y,H,w;return!t||!t.state||(y=t.state,y.wrap!==0&&y.mode!==K)?f:y.mode===K&&(H=1,H=T(H,x,e,0),H!==y.check)?d:(w=Me(t,x,e,e),w?(y.mode=Ee,M):(y.havedict=1,h))};k.exports.inflateReset=Ue,k.exports.inflateReset2=Ce,k.exports.inflateResetKeep=Ie,k.exports.inflateInit=Ye,k.exports.inflateInit2=Ne,k.exports.inflate=Xe,k.exports.inflateEnd=Ke,k.exports.inflateGetHeader=Ve,k.exports.inflateSetDictionary=We,k.exports.inflateInfo=\"pako inflate (from Nodeca project)\"},241:k=>{\"use strict\";const m=new Uint16Array([3,4,5,6,7,8,9,10,11,13,15,17,19,23,27,31,35,43,51,59,67,83,99,115,131,163,195,227,258,0,0]),_=new Uint8Array([16,16,16,16,16,16,16,16,17,17,17,17,18,18,18,18,19,19,19,19,20,20,20,20,21,21,21,21,16,72,78]),C=new Uint16Array([1,2,3,4,5,7,9,13,17,25,33,49,65,97,129,193,257,385,513,769,1025,1537,2049,3073,4097,6145,8193,12289,16385,24577,0,0]),u=new Uint8Array([16,16,16,16,17,17,18,18,19,19,20,20,21,21,22,22,23,23,24,24,25,25,26,26,27,27,28,28,29,29,64,64]),v=(l,h,c,N,f,d,M,L)=>{const j=L.bits;let D=0,A=0,o=0,i=0,Z=0,R=0,O=0,p=0,U=0,B=0,K,V,fe,de,ne,ce=null,pe=0,le;const te=new Uint16Array(15+1),re=new Uint16Array(15+1);let ae=null,we=0,_e,ue,he;for(D=0;D<=15;D++)te[D]=0;for(A=0;A<N;A++)te[h[c+A]]++;for(Z=j,i=15;i>=1&&te[i]===0;i--);if(Z>i&&(Z=i),i===0)return f[d++]=1<<24|64<<16|0,f[d++]=1<<24|64<<16|0,L.bits=1,0;for(o=1;o<i&&te[o]===0;o++);for(Z<o&&(Z=o),p=1,D=1;D<=15;D++)if(p<<=1,p-=te[D],p<0)return-1;if(p>0&&(l===0||i!==1))return-1;for(re[1]=0,D=1;D<15;D++)re[D+1]=re[D]+te[D];for(A=0;A<N;A++)h[c+A]!==0&&(M[re[h[c+A]]++]=A);if(l===0?(ce=ae=M,le=19):l===1?(ce=m,pe-=257,ae=_,we-=257,le=256):(ce=C,ae=u,le=-1),B=0,A=0,D=o,ne=d,R=Z,O=0,fe=-1,U=1<<Z,de=U-1,l===1&&U>852||l===2&&U>592)return 1;for(;;){_e=D-O,M[A]<le?(ue=0,he=M[A]):M[A]>le?(ue=ae[we+M[A]],he=ce[pe+M[A]]):(ue=32+64,he=0),K=1<<D-O,V=1<<R,o=V;do V-=K,f[ne+(B>>O)+V]=_e<<24|ue<<16|he|0;while(V!==0);for(K=1<<D-1;B&K;)K>>=1;if(K!==0?(B&=K-1,B+=K):B=0,A++,--te[D]===0){if(D===i)break;D=h[c+M[A]]}if(D>Z&&(B&de)!==fe){for(O===0&&(O=Z),ne+=o,R=D-O,p=1<<R;R+O<i&&(p-=te[R+O],!(p<=0));)R++,p<<=1;if(U+=1<<R,l===1&&U>852||l===2&&U>592)return 1;fe=B&de,f[fe]=Z<<24|R<<16|ne-d|0}}return B!==0&&(f[ne+B]=D-O<<24|64<<16|0),L.bits=Z,0};k.exports=v},898:k=>{\"use strict\";k.exports={2:\"need dictionary\",1:\"stream end\",0:\"\",\"-1\":\"file error\",\"-2\":\"stream error\",\"-3\":\"data error\",\"-4\":\"insufficient memory\",\"-5\":\"buffer error\",\"-6\":\"incompatible version\"}},292:k=>{\"use strict\";function I(){this.input=null,this.next_in=0,this.avail_in=0,this.total_in=0,this.output=null,this.next_out=0,this.avail_out=0,this.total_out=0,this.msg=\"\",this.state=null,this.data_type=2,this.adler=0}k.exports=I},330:(k,I,E)=>{\"use strict\";E.d(I,{default:()=>C});var T=E(829),n=E(861),g=E.n(n),s=E(843);function m(u){if(u.__type__==\"npy\"){var v;if(window.obj=u,u.hasOwnProperty(\"zdata\")){const f=Uint8Array.from(window.atob(u.zdata),d=>d.charCodeAt(0));v=(0,s.rr)(f)}else v=Uint8Array.from(window.atob(u.data),f=>f.charCodeAt(0));var l=(0,T.g)(v.buffer);if(l=n(l.data,l.shape),u.hasOwnProperty(\"min\")){let f=l.dtype===\"uint8\"?255:65535;for(var h=1,c=0;c<l.shape.length;c++)h=h*l.shape[c];for(var N=n(new Float32Array(h),l.shape),c=0;c<l.data.length;c++)N.data[c]=u.min+(u.max-u.min)*l.data[c]/f;return N}else return l}else return{}}function _(u){if(Array.isArray(u)){var v=[];for(var l of u)v.push(_(l));return v}else if(u instanceof Object){if(u.hasOwnProperty(\"__type__\"))return m(u);var v={};for(var h of Object.keys(u))v[h]=_(u[h]);return v}else return u}const C=loader={unpack_obj:_}}},ye={};function se(k){if(ye[k])return ye[k].exports;var I=ye[k]={exports:{}};return Ze[k](I,I.exports,se),I.exports}return se.n=k=>{var I=k&&k.__esModule?()=>k.default:()=>k;return se.d(I,{a:I}),I},se.d=(k,I)=>{for(var E in I)se.o(I,E)&&!se.o(k,E)&&Object.defineProperty(k,E,{enumerable:!0,get:I[E]})},se.o=(k,I)=>Object.prototype.hasOwnProperty.call(k,I),se(330)})().default;\n",
       "</script>\n",
       "<script>var AttentionMulti;AttentionMulti=(()=>{\"use strict\";var St={143:(x,B,te)=>{te.d(B,{default:()=>mn});function P(){}const me=e=>e;function jt(e,t){for(const n in t)e[n]=t[n];return e}function Mt(e){return e&&typeof e==\"object\"&&typeof e.then==\"function\"}function En(e,t,n,i,o){e.__svelte_meta={loc:{file:t,line:n,column:i,char:o}}}function Ge(e){return e()}function Se(){return Object.create(null)}function H(e){e.forEach(Ge)}function fe(e){return typeof e==\"function\"}function je(e,t){return e!=e?t==t:e!==t||e&&typeof e==\"object\"||typeof e==\"function\"}function Sn(e,t){return e!=e?t==t:e!==t}function Ke(e){return Object.keys(e).length===0}function jn(e,t){if(e!=null&&typeof e.subscribe!=\"function\")throw new Error(`'${t}' is not a store with a 'subscribe' method`)}function Qe(e,...t){if(e==null)return P;const n=e.subscribe(...t);return n.unsubscribe?()=>n.unsubscribe():n}function Mn(e){let t;return Qe(e,n=>t=n)(),t}function An(e,t,n){e.$$.on_destroy.push(Qe(t,n))}function At(e,t,n,i){if(e){const o=Me(e,t,n,i);return e[0](o)}}function Me(e,t,n,i){return e[1]&&i?jt(n.ctx.slice(),e[1](i(t))):n.ctx}function Ze(e,t,n,i){if(e[2]&&i){const o=e[2](i(n));if(t.dirty===void 0)return o;if(typeof o==\"object\"){const l=[],s=Math.max(t.dirty.length,o.length);for(let r=0;r<s;r+=1)l[r]=t.dirty[r]|o[r];return l}return t.dirty|o}return t.dirty}function Ct(e,t,n,i,o,l,s){const r=Ze(t,i,o,l);if(r){const f=Me(t,n,i,s);e.p(f,r)}}function Cn(e,t,n,i,o,l,s,r){const f=s(o)|Ze(t,i,o,l);if(f){const a=Me(t,n,i,r);e.p(a,f)}}function Dn(e){const t={};for(const n in e)n[0]!==\"$\"&&(t[n]=e[n]);return t}function On(e,t){const n={};t=new Set(t);for(const i in e)!t.has(i)&&i[0]!==\"$\"&&(n[i]=e[i]);return n}function Tn(e){const t={};for(const n in e)t[n]=!0;return t}function Ln(e){let t=!1;return function(...n){t||(t=!0,e.call(this,...n))}}function Nn(e){return e==null?\"\":e}function Pn(e,t,n=t){return e.set(n),t}const Dt=(e,t)=>Object.prototype.hasOwnProperty.call(e,t);function Rn(e){return e&&fe(e.destroy)?e.destroy:P}const Bn=typeof window!=\"undefined\";let ae=null,ge=null;function Hn(e){ae=e}function xn(e){ge=e}const K=new Set;function $e(e){K.forEach(t=>{t.c(e)||(K.delete(t),t.f())}),K.size!==0&&ge($e)}function zn(){K.clear()}function ve(e){let t;return K.size===0&&ge($e),{promise:new Promise(n=>{K.add(t={c:e,f:n})}),abort(){K.delete(t)}}}function E(e,t){e.appendChild(t)}function C(e,t,n){e.insertBefore(t,n||null)}function j(e){e.parentNode.removeChild(e)}function et(e,t){for(let n=0;n<e.length;n+=1)e[n]&&e[n].d(t)}function F(e){return document.createElement(e)}function In(e,t){return document.createElement(e,{is:t})}function Vn(e,t){const n={};for(const i in e)Dt(e,i)&&t.indexOf(i)===-1&&(n[i]=e[i]);return n}function Ot(e){return document.createElementNS(\"http://www.w3.org/2000/svg\",e)}function R(e){return document.createTextNode(e)}function T(){return R(\" \")}function tt(){return R(\"\")}function z(e,t,n,i){return e.addEventListener(t,n,i),()=>e.removeEventListener(t,n,i)}function Wn(e){return function(t){return t.preventDefault(),e.call(this,t)}}function Yn(e){return function(t){return t.stopPropagation(),e.call(this,t)}}function Un(e){return function(t){t.target===this&&e.call(this,t)}}function y(e,t,n){n==null?e.removeAttribute(t):e.getAttribute(t)!==n&&e.setAttribute(t,n)}function Jn(e,t){const n=Object.getOwnPropertyDescriptors(e.__proto__);for(const i in t)t[i]==null?e.removeAttribute(i):i===\"style\"?e.style.cssText=t[i]:i===\"__value\"?e.value=e[i]=t[i]:n[i]&&n[i].set?e[i]=t[i]:y(e,i,t[i])}function Xn(e,t){for(const n in t)y(e,n,t[n])}function Gn(e,t,n){t in e?e[t]=n:y(e,t,n)}function Kn(e,t,n){e.setAttributeNS(\"http://www.w3.org/1999/xlink\",t,n)}function Qn(e,t,n){const i=new Set;for(let o=0;o<e.length;o+=1)e[o].checked&&i.add(e[o].__value);return n||i.delete(t),Array.from(i)}function Zn(e){return e===\"\"?null:+e}function $n(e){const t=[];for(let n=0;n<e.length;n+=1)t.push({start:e.start(n),end:e.end(n)});return t}function Tt(e){return Array.from(e.childNodes)}function ei(e,t,n,i){for(let o=0;o<e.length;o+=1){const l=e[o];if(l.nodeName===t){let s=0;const r=[];for(;s<l.attributes.length;){const f=l.attributes[s++];n[f.name]||r.push(f.name)}for(let f=0;f<r.length;f++)l.removeAttribute(r[f]);return e.splice(o,1)[0]}}return i?Ot(t):F(t)}function Lt(e,t){for(let n=0;n<e.length;n+=1){const i=e[n];if(i.nodeType===3)return i.data=\"\"+t,e.splice(n,1)[0]}return R(t)}function ti(e){return Lt(e,\" \")}function ke(e,t){t=\"\"+t,e.wholeText!==t&&(e.data=t)}function ni(e,t){e.value=t==null?\"\":t}function ii(e,t){try{e.type=t}catch(n){}}function S(e,t,n,i){e.style.setProperty(t,n,i?\"important\":\"\")}function oi(e,t){for(let n=0;n<e.options.length;n+=1){const i=e.options[n];if(i.__value===t){i.selected=!0;return}}}function li(e,t){for(let n=0;n<e.options.length;n+=1){const i=e.options[n];i.selected=~t.indexOf(i.__value)}}function si(e){const t=e.querySelector(\":checked\")||e.options[0];return t&&t.__value}function ri(e){return[].map.call(e.querySelectorAll(\":checked\"),t=>t.__value)}let be;function Nt(){if(be===void 0){be=!1;try{typeof window!=\"undefined\"&&window.parent&&window.parent.document}catch(e){be=!0}}return be}function ui(e,t){getComputedStyle(e).position===\"static\"&&(e.style.position=\"relative\");const i=F(\"iframe\");i.setAttribute(\"style\",\"display: block; position: absolute; top: 0; left: 0; width: 100%; height: 100%; overflow: hidden; border: 0; opacity: 0; pointer-events: none; z-index: -1;\"),i.setAttribute(\"aria-hidden\",\"true\"),i.tabIndex=-1;const o=Nt();let l;return o?(i.src=\"data:text/html,<script>onresize=function(){parent.postMessage(0,'*')}<\\/script>\",l=z(window,\"message\",s=>{s.source===i.contentWindow&&t()})):(i.src=\"about:blank\",i.onload=()=>{l=z(i.contentWindow,\"resize\",t)}),E(e,i),()=>{(o||l&&i.contentWindow)&&l(),j(i)}}function fi(e,t,n){e.classList[n?\"add\":\"remove\"](t)}function Ae(e,t){const n=document.createEvent(\"CustomEvent\");return n.initCustomEvent(e,!1,!1,t),n}function ai(e,t=document.body){return Array.from(t.querySelectorAll(e))}class ci{constructor(t=null){this.a=t,this.e=this.n=null}m(t,n,i=null){this.e||(this.e=F(n.nodeName),this.t=n,this.h(t)),this.i(i)}h(t){this.e.innerHTML=t,this.n=Array.from(this.e.childNodes)}i(t){for(let n=0;n<this.n.length;n+=1)C(this.t,this.n[n],t)}p(t){this.d(),this.h(t),this.i(this.a)}d(){this.n.forEach(j)}}function di(e){const t={};for(const n of e)t[n.name]=n.value;return t}function _i(e){const t={};return e.childNodes.forEach(n=>{t[n.slot||\"default\"]=!0}),t}const Ce=new Set;let ye=0;function Pt(e){let t=5381,n=e.length;for(;n--;)t=(t<<5)-t^e.charCodeAt(n);return t>>>0}function ce(e,t,n,i,o,l,s,r=0){const f=16.666/i;let a=`{\n",
       "`;for(let u=0;u<=1;u+=f){const _=t+(n-t)*l(u);a+=u*100+`%{${s(_,1-_)}}\n",
       "`}const c=a+`100% {${s(n,1-n)}}\n",
       "}`,d=`__svelte_${Pt(c)}_${r}`,h=e.ownerDocument;Ce.add(h);const p=h.__svelte_stylesheet||(h.__svelte_stylesheet=h.head.appendChild(F(\"style\")).sheet),m=h.__svelte_rules||(h.__svelte_rules={});m[d]||(m[d]=!0,p.insertRule(`@keyframes ${d} ${c}`,p.cssRules.length));const g=e.style.animation||\"\";return e.style.animation=`${g?`${g}, `:\"\"}${d} ${i}ms linear ${o}ms 1 both`,ye+=1,d}function de(e,t){const n=(e.style.animation||\"\").split(\", \"),i=n.filter(t?l=>l.indexOf(t)<0:l=>l.indexOf(\"__svelte\")===-1),o=n.length-i.length;o&&(e.style.animation=i.join(\", \"),ye-=o,ye||Rt())}function Rt(){ge(()=>{ye||(Ce.forEach(e=>{const t=e.__svelte_stylesheet;let n=t.cssRules.length;for(;n--;)t.deleteRule(n);e.__svelte_rules={}}),Ce.clear())})}function hi(e,t,n,i){if(!t)return P;const o=e.getBoundingClientRect();if(t.left===o.left&&t.right===o.right&&t.top===o.top&&t.bottom===o.bottom)return P;const{delay:l=0,duration:s=300,easing:r=me,start:f=ae()+l,end:a=f+s,tick:c=P,css:d}=n(e,{from:t,to:o},i);let h=!0,p=!1,m;function g(){d&&(m=ce(e,0,1,s,l,r,d)),l||(p=!0)}function u(){d&&de(e,m),h=!1}return ve(_=>{if(!p&&_>=f&&(p=!0),p&&_>=a&&(c(1,0),u()),!h)return!1;if(p){const k=_-f,w=0+1*r(k/s);c(w,1-w)}return!0}),g(),c(0,1),u}function pi(e){const t=getComputedStyle(e);if(t.position!==\"absolute\"&&t.position!==\"fixed\"){const{width:n,height:i}=t,o=e.getBoundingClientRect();e.style.position=\"absolute\",e.style.width=n,e.style.height=i,Bt(e,o)}}function Bt(e,t){const n=e.getBoundingClientRect();if(t.left!==n.left||t.top!==n.top){const i=getComputedStyle(e),o=i.transform===\"none\"?\"\":i.transform;e.style.transform=`${o} translate(${t.left-n.left}px, ${t.top-n.top}px)`}}let _e;function I(e){_e=e}function U(){if(!_e)throw new Error(\"Function called outside component initialization\");return _e}function mi(e){U().$$.before_update.push(e)}function Ht(e){U().$$.on_mount.push(e)}function gi(e){U().$$.after_update.push(e)}function vi(e){U().$$.on_destroy.push(e)}function ki(){const e=U();return(t,n)=>{const i=e.$$.callbacks[t];if(i){const o=Ae(t,n);i.slice().forEach(l=>{l.call(e,o)})}}}function bi(e,t){U().$$.context.set(e,t)}function yi(e){return U().$$.context.get(e)}function wi(e){return U().$$.context.has(e)}function Fi(e,t){const n=e.$$.callbacks[t.type];n&&n.slice().forEach(i=>i(t))}const he=[],qi={enabled:!1},ne=[],we=[],De=[],nt=Promise.resolve();let Oe=!1;function it(){Oe||(Oe=!0,nt.then(Ne))}function Ei(){return it(),nt}function ie(e){we.push(e)}function ot(e){De.push(e)}let Te=!1;const Le=new Set;function Ne(){if(!Te){Te=!0;do{for(let e=0;e<he.length;e+=1){const t=he[e];I(t),xt(t.$$)}for(I(null),he.length=0;ne.length;)ne.pop()();for(let e=0;e<we.length;e+=1){const t=we[e];Le.has(t)||(Le.add(t),t())}we.length=0}while(he.length);for(;De.length;)De.pop()();Oe=!1,Te=!1,Le.clear()}}function xt(e){if(e.fragment!==null){e.update(),H(e.before_update);const t=e.dirty;e.dirty=[-1],e.fragment&&e.fragment.p(e.ctx,t),e.after_update.forEach(ie)}}let pe;function Pe(){return pe||(pe=Promise.resolve(),pe.then(()=>{pe=null})),pe}function Q(e,t,n){e.dispatchEvent(Ae(`${t?\"intro\":\"outro\"}${n}`))}const Fe=new Set;let V;function oe(){V={r:0,c:[],p:V}}function le(){V.r||H(V.c),V=V.p}function O(e,t){e&&e.i&&(Fe.delete(e),e.i(t))}function L(e,t,n,i){if(e&&e.o){if(Fe.has(e))return;Fe.add(e),V.c.push(()=>{Fe.delete(e),i&&(n&&e.d(1),i())}),e.o(t)}}const Re={duration:0};function Si(e,t,n){let i=t(e,n),o=!1,l,s,r=0;function f(){l&&de(e,l)}function a(){const{delay:d=0,duration:h=300,easing:p=me,tick:m=P,css:g}=i||Re;g&&(l=ce(e,0,1,h,d,p,g,r++)),m(0,1);const u=ae()+d,_=u+h;s&&s.abort(),o=!0,ie(()=>Q(e,!0,\"start\")),s=ve(k=>{if(o){if(k>=_)return m(1,0),Q(e,!0,\"end\"),f(),o=!1;if(k>=u){const w=p((k-u)/h);m(w,1-w)}}return o})}let c=!1;return{start(){c||(de(e),fe(i)?(i=i(),Pe().then(a)):a())},invalidate(){c=!1},end(){o&&(f(),o=!1)}}}function ji(e,t,n){let i=t(e,n),o=!0,l;const s=V;s.r+=1;function r(){const{delay:f=0,duration:a=300,easing:c=me,tick:d=P,css:h}=i||Re;h&&(l=ce(e,1,0,a,f,c,h));const p=ae()+f,m=p+a;ie(()=>Q(e,!1,\"start\")),ve(g=>{if(o){if(g>=m)return d(0,1),Q(e,!1,\"end\"),--s.r||H(s.c),!1;if(g>=p){const u=c((g-p)/a);d(1-u,u)}}return o})}return fe(i)?Pe().then(()=>{i=i(),r()}):r(),{end(f){f&&i.tick&&i.tick(1,0),o&&(l&&de(e,l),o=!1)}}}function Mi(e,t,n,i){let o=t(e,n),l=i?0:1,s=null,r=null,f=null;function a(){f&&de(e,f)}function c(h,p){const m=h.b-l;return p*=Math.abs(m),{a:l,b:h.b,d:m,duration:p,start:h.start,end:h.start+p,group:h.group}}function d(h){const{delay:p=0,duration:m=300,easing:g=me,tick:u=P,css:_}=o||Re,k={start:ae()+p,b:h};h||(k.group=V,V.r+=1),s||r?r=k:(_&&(a(),f=ce(e,l,h,m,p,g,_)),h&&u(0,1),s=c(k,m),ie(()=>Q(e,h,\"start\")),ve(w=>{if(r&&w>r.start&&(s=c(r,m),r=null,Q(e,s.b,\"start\"),_&&(a(),f=ce(e,l,s.b,s.duration,0,g,o.css))),s){if(w>=s.end)u(l=s.b,1-l),Q(e,s.b,\"end\"),r||(s.b?a():--s.group.r||H(s.group.c)),s=null;else if(w>=s.start){const M=w-s.start;l=s.a+s.d*g(M/s.duration),u(l,1-l)}}return!!(s||r)}))}return{run(h){fe(o)?Pe().then(()=>{o=o(),d(h)}):d(h)},end(){a(),s=r=null}}}function Ai(e,t){const n=t.token={};function i(o,l,s,r){if(t.token!==n)return;t.resolved=r;let f=t.ctx;s!==void 0&&(f=f.slice(),f[s]=r);const a=o&&(t.current=o)(f);let c=!1;t.block&&(t.blocks?t.blocks.forEach((d,h)=>{h!==l&&d&&(oe(),L(d,1,1,()=>{t.blocks[h]===d&&(t.blocks[h]=null)}),le())}):t.block.d(1),a.c(),O(a,1),a.m(t.mount(),t.anchor),c=!0),t.block=a,t.blocks&&(t.blocks[l]=a),c&&Ne()}if(Mt(e)){const o=U();if(e.then(l=>{I(o),i(t.then,1,t.value,l),I(null)},l=>{if(I(o),i(t.catch,2,t.error,l),I(null),!t.hasCatch)throw l}),t.current!==t.pending)return i(t.pending,0),!0}else{if(t.current!==t.then)return i(t.then,1,t.value,e),!0;t.resolved=e}}const Ci=typeof window!=\"undefined\"?window:typeof globalThis!=\"undefined\"?globalThis:global;function zt(e,t){e.d(1),t.delete(e.key)}function It(e,t){L(e,1,1,()=>{t.delete(e.key)})}function Di(e,t){e.f(),zt(e,t)}function Oi(e,t){e.f(),It(e,t)}function Ti(e,t,n,i,o,l,s,r,f,a,c,d){let h=e.length,p=l.length,m=h;const g={};for(;m--;)g[e[m].key]=m;const u=[],_=new Map,k=new Map;for(m=p;m--;){const b=d(o,l,m),v=n(b);let q=s.get(v);q?i&&q.p(b,t):(q=a(v,b),q.c()),_.set(v,u[m]=q),v in g&&k.set(v,Math.abs(m-g[v]))}const w=new Set,M=new Set;function N(b){O(b,1),b.m(r,c),s.set(b.key,b),c=b.first,p--}for(;h&&p;){const b=u[p-1],v=e[h-1],q=b.key,D=v.key;b===v?(c=b.first,h--,p--):_.has(D)?!s.has(q)||w.has(q)?N(b):M.has(D)?h--:k.get(q)>k.get(D)?(M.add(q),N(b)):(w.add(D),h--):(f(v,s),h--)}for(;h--;){const b=e[h];_.has(b.key)||f(b,s)}for(;p;)N(u[p-1]);return u}function Li(e,t,n,i){const o=new Set;for(let l=0;l<t.length;l++){const s=i(n(e,t,l));if(o.has(s))throw new Error(\"Cannot have duplicate keys in a keyed each\");o.add(s)}}function Ni(e,t){const n={},i={},o={$$scope:1};let l=e.length;for(;l--;){const s=e[l],r=t[l];if(r){for(const f in s)f in r||(i[f]=1);for(const f in r)o[f]||(n[f]=r[f],o[f]=1);e[l]=r}else for(const f in s)o[f]=1}for(const s in i)s in n||(n[s]=void 0);return n}function Pi(e){return typeof e==\"object\"&&e!==null?e:{}}const Vt=new Set([\"allowfullscreen\",\"allowpaymentrequest\",\"async\",\"autofocus\",\"autoplay\",\"checked\",\"controls\",\"default\",\"defer\",\"disabled\",\"formnovalidate\",\"hidden\",\"ismap\",\"loop\",\"multiple\",\"muted\",\"nomodule\",\"novalidate\",\"open\",\"playsinline\",\"readonly\",\"required\",\"reversed\",\"selected\"]),Wt=/[\\s'\">/=\\u{FDD0}-\\u{FDEF}\\u{FFFE}\\u{FFFF}\\u{1FFFE}\\u{1FFFF}\\u{2FFFE}\\u{2FFFF}\\u{3FFFE}\\u{3FFFF}\\u{4FFFE}\\u{4FFFF}\\u{5FFFE}\\u{5FFFF}\\u{6FFFE}\\u{6FFFF}\\u{7FFFE}\\u{7FFFF}\\u{8FFFE}\\u{8FFFF}\\u{9FFFE}\\u{9FFFF}\\u{AFFFE}\\u{AFFFF}\\u{BFFFE}\\u{BFFFF}\\u{CFFFE}\\u{CFFFF}\\u{DFFFE}\\u{DFFFF}\\u{EFFFE}\\u{EFFFF}\\u{FFFFE}\\u{FFFFF}\\u{10FFFE}\\u{10FFFF}]/u;function Ri(e,t){const n=Object.assign({},...e);t&&(n.class==null?n.class=t:n.class+=\" \"+t);let i=\"\";return Object.keys(n).forEach(o=>{if(Wt.test(o))return;const l=n[o];l===!0?i+=\" \"+o:Vt.has(o.toLowerCase())?l&&(i+=\" \"+o):l!=null&&(i+=` ${o}=\"${String(l).replace(/\"/g,\"&#34;\").replace(/'/g,\"&#39;\")}\"`)}),i}const Yt={'\"':\"&quot;\",\"'\":\"&#39;\",\"&\":\"&amp;\",\"<\":\"&lt;\",\">\":\"&gt;\"};function Ut(e){return String(e).replace(/[\"'&<>]/g,t=>Yt[t])}function Bi(e,t){let n=\"\";for(let i=0;i<e.length;i+=1)n+=t(e[i],i);return n}const Hi={$$render:()=>\"\"};function xi(e,t){if(!e||!e.$$render)throw t===\"svelte:component\"&&(t+=\" this={...}\"),new Error(`<${t}> is not a valid SSR component. You may need to review your build config to ensure that dependencies are compiled, rather than imported as pre-compiled modules`);return e}function zi(e,t,n,i){return console.log(`{@debug} ${e?e+\" \":\"\"}(${t}:${n})`),console.log(i),\"\"}let Be;function Ii(e){function t(n,i,o,l){const s=_e,r={on_destroy:Be,context:new Map(s?s.$$.context:[]),on_mount:[],before_update:[],after_update:[],callbacks:Se()};I({$$:r});const f=e(n,i,o,l);return I(s),f}return{render:(n={},i={})=>{Be=[];const o={title:\"\",head:\"\",css:new Set},l=t(o,n,{},i);return H(Be),{html:l,css:{code:Array.from(o.css).map(s=>s.code).join(`\n",
       "`),map:null},head:o.title+o.head}},$$render:t}}function Vi(e,t,n){return t==null||n&&!t?\"\":` ${e}${t===!0?\"\":`=${typeof t==\"string\"?JSON.stringify(Ut(t)):`\"${t}\"`}`}`}function Wi(e){return e?` class=\"${e}\"`:\"\"}function lt(e,t,n){const i=e.$$.props[t];i!==void 0&&(e.$$.bound[i]=n,n(e.$$.ctx[i]))}function se(e){e&&e.c()}function Yi(e,t){e&&e.l(t)}function Z(e,t,n){const{fragment:i,on_mount:o,on_destroy:l,after_update:s}=e.$$;i&&i.m(t,n),ie(()=>{const r=o.map(Ge).filter(fe);l?l.push(...r):H(r),e.$$.on_mount=[]}),s.forEach(ie)}function X(e,t){const n=e.$$;n.fragment!==null&&(H(n.on_destroy),n.fragment&&n.fragment.d(t),n.on_destroy=n.fragment=null,n.ctx=[])}function Jt(e,t){e.$$.dirty[0]===-1&&(he.push(e),it(),e.$$.dirty.fill(0)),e.$$.dirty[t/31|0]|=1<<t%31}function He(e,t,n,i,o,l,s=[-1]){const r=_e;I(e);const f=t.props||{},a=e.$$={fragment:null,ctx:null,props:l,update:P,not_equal:o,bound:Se(),on_mount:[],on_destroy:[],before_update:[],after_update:[],context:new Map(r?r.$$.context:[]),callbacks:Se(),dirty:s,skip_bound:!1};let c=!1;if(a.ctx=n?n(e,f,(d,h,...p)=>{const m=p.length?p[0]:h;return a.ctx&&o(a.ctx[d],a.ctx[d]=m)&&(!a.skip_bound&&a.bound[d]&&a.bound[d](m),c&&Jt(e,d)),h}):[],a.update(),c=!0,H(a.before_update),a.fragment=i?i(a.ctx):!1,t.target){if(t.hydrate){const d=Tt(t.target);a.fragment&&a.fragment.l(d),d.forEach(j)}else a.fragment&&a.fragment.c();t.intro&&O(e.$$.fragment),Z(e,t.target,t.anchor),Ne()}I(r)}let Xt;typeof HTMLElement==\"function\"&&(Xt=class extends HTMLElement{constructor(){super(),this.attachShadow({mode:\"open\"})}connectedCallback(){for(const e in this.$$.slotted)this.appendChild(this.$$.slotted[e])}attributeChangedCallback(e,t,n){this[e]=n}$destroy(){X(this,1),this.$destroy=P}$on(e,t){const n=this.$$.callbacks[e]||(this.$$.callbacks[e]=[]);return n.push(t),()=>{const i=n.indexOf(t);i!==-1&&n.splice(i,1)}}$set(e){this.$$set&&!Ke(e)&&(this.$$.skip_bound=!0,this.$$set(e),this.$$.skip_bound=!1)}});class xe{$destroy(){X(this,1),this.$destroy=P}$on(t,n){const i=this.$$.callbacks[t]||(this.$$.callbacks[t]=[]);return i.push(n),()=>{const o=i.indexOf(n);o!==-1&&i.splice(o,1)}}$set(t){this.$$set&&!Ke(t)&&(this.$$.skip_bound=!0,this.$$set(t),this.$$.skip_bound=!1)}}function W(e,t){document.dispatchEvent(Ae(e,Object.assign({version:\"3.31.2\"},t)))}function Ui(e,t){W(\"SvelteDOMInsert\",{target:e,node:t}),E(e,t)}function Ji(e,t,n){W(\"SvelteDOMInsert\",{target:e,node:t,anchor:n}),C(e,t,n)}function ze(e){W(\"SvelteDOMRemove\",{node:e}),j(e)}function Xi(e,t){for(;e.nextSibling&&e.nextSibling!==t;)ze(e.nextSibling)}function Gi(e){for(;e.previousSibling;)ze(e.previousSibling)}function Ki(e){for(;e.nextSibling;)ze(e.nextSibling)}function Qi(e,t,n,i,o,l){const s=i===!0?[\"capture\"]:i?Array.from(Object.keys(i)):[];o&&s.push(\"preventDefault\"),l&&s.push(\"stopPropagation\"),W(\"SvelteDOMAddEventListener\",{node:e,event:t,handler:n,modifiers:s});const r=z(e,t,n,i);return()=>{W(\"SvelteDOMRemoveEventListener\",{node:e,event:t,handler:n,modifiers:s}),r()}}function Zi(e,t,n){y(e,t,n),n==null?W(\"SvelteDOMRemoveAttribute\",{node:e,attribute:t}):W(\"SvelteDOMSetAttribute\",{node:e,attribute:t,value:n})}function $i(e,t,n){e[t]=n,W(\"SvelteDOMSetProperty\",{node:e,property:t,value:n})}function eo(e,t,n){e.dataset[t]=n,W(\"SvelteDOMSetDataset\",{node:e,property:t,value:n})}function to(e,t){t=\"\"+t,e.wholeText!==t&&(W(\"SvelteDOMSetData\",{node:e,data:t}),e.data=t)}function no(e){if(typeof e!=\"string\"&&!(e&&typeof e==\"object\"&&\"length\"in e)){let t=\"{#each} only iterates over array-like objects.\";throw typeof Symbol==\"function\"&&e&&Symbol.iterator in e&&(t+=\" You can use a spread to convert this iterable into an array.\"),new Error(t)}}function io(e,t,n){for(const i of Object.keys(t))~n.indexOf(i)||console.warn(`<${e}> received an unexpected slot \"${i}\".`)}class oo extends null{constructor(t){if(!t||!t.target&&!t.$$inline)throw new Error(\"'target' is a required option\");super()}$destroy(){super.$destroy(),this.$destroy=()=>{console.warn(\"Component was already destroyed\")}}$capture_state(){}$inject_state(){}}class lo extends null{constructor(t){super(t)}}function so(e){const t=Date.now();return()=>{if(Date.now()-t>e)throw new Error(\"Infinite loop detected\")}}function ro(e,t){return e.map((n,i)=>n+t[i])}function uo(e,t){return t.map(n=>e*n)}function Gt(e,t,n){return e.map((i,o)=>(1-n)*i+n*t[o])}function st(e,t=2){if(!(\"length\"in e))return Kt(e,t);for(var n=0,i=0;i<e.length;i++)n+=Math.pow(Math.abs(e[i]),t);return Math.pow(n,1/t)}function Kt(e,t=2){for(var n=0,i=0;i<e.shape[0];i++)n+=Math.pow(Math.abs(e.get(i)),t);return Math.pow(n,1/t)}function Ie(e,t=2){var n=st(e,t);return e.map(i=>i/(1e-4+n))}function Qt(e){for(var t=[[1,0,0],[1,1,0],[0,1,0],[0,1,1],[0,0,1],[1,0,1]].map(s=>Ie(s,1));e<0;)e+=360;e=e%360;var n=360/t.length,i=Math.floor(e/n),o=(e-i*n)/n,l=Gt(t[i],t[(i+1)%t.length],o);return l=Ie(l,1),l}const Ve=[];function Zt(e){if(e in Ve)return Ve[e];let t=[];for(let n=0;n<e;n++){const i=360*n/e;t.push(Qt(i))}return Ve[e]=t,t}function rt(e){return`rgb(${255*e[0]}, ${255*e[1]}, ${255*e[2]})`}function ut(e,t=[.98,.98,.98],n=void 0,i=void 0){const o=\"length\"in e?e.length:e.shape[0],l=\"length\"in e?h=>e[h]:h=>e.get(h);if(i==null&&(i=Zt(o)),console.log(\"Hues\",i),n==null){for(var s=[0,0,0],r=0;r<o;r++){const h=l(r);if(h!=0)for(var f=i[r],a=0;a<3;a++)s[a]+=h*f[a]}s=Ie(s,1);for(var c=st(e,2),c=Math.max(0,Math.min(1,c)),a=0;a<3;a++)s[a]=c*s[a]+(1-c)*t[a];return s}else{for(var r=n,d=i[r],s=[0,0,0],a=0;a<3;a++)s[a]=l(r)*d[a]+(1-l(r))*t[a];return s}}function We(e,t=[.98,.98,.98],n=void 0,i=void 0){var o=ut(e,t,n,i);return rt(o)}function $t(e,t=[.98,.98,.98]){if(e>=0)for(var n=[0,.7,0],i=[0,0,0],o=0;o<3;o++)i[o]=e*n[o]+(1-e)*t[o];else for(var n=[1,0,0],i=[0,0,0],o=0;o<3;o++)i[o]=-e*n[o]+(1+e)*t[o];return console.log(\"Neuron color\",e,i),i}function fo(e,t=[.98,.98,.98]){var n=$t(e,t);return rt(n)}function en(){var e=F(\"style\");e.id=\"svelte-1fjpmsy-style\",e.textContent=\".container.svelte-1fjpmsy.svelte-1fjpmsy{position:relative;border:1px solid #aaa}.container.svelte-1fjpmsy>.svelte-1fjpmsy{position:absolute}.container.svelte-1fjpmsy canvas.svelte-1fjpmsy{left:0px;top:0px;width:100%;height:100%;image-rendering:pixelated}.container.svelte-1fjpmsy .focus-top.svelte-1fjpmsy,.container.svelte-1fjpmsy .focus-bottom.svelte-1fjpmsy{left:0px;width:100%;background:#aaa;opacity:0.3}.container.svelte-1fjpmsy .focus-top.svelte-1fjpmsy{top:0px}.container.svelte-1fjpmsy .focus-bottom.svelte-1fjpmsy{bottom:0px}.container.svelte-1fjpmsy .focus-left.svelte-1fjpmsy,.container.svelte-1fjpmsy .focus-right.svelte-1fjpmsy{top:0px;height:100%;background:#aaa;opacity:0.3}.container.svelte-1fjpmsy .focus-left.svelte-1fjpmsy{left:0px}.container.svelte-1fjpmsy .focus-right.svelte-1fjpmsy{right:0px}\",E(document.head,e)}function ft(e){let t,n,i,o=e[3]!=null&&at(e);return{c(){t=F(\"div\"),n=F(\"canvas\"),i=T(),o&&o.c(),S(n,\"width\",e[1]+\"px\"),S(n,\"height\",e[2]+\"px\"),y(n,\"class\",\"svelte-1fjpmsy\"),y(t,\"class\",\"container svelte-1fjpmsy\"),S(t,\"width\",e[1]+\"px\"),S(t,\"height\",e[2]+\"px\")},m(l,s){C(l,t,s),E(t,n),e[9](n),E(t,i),o&&o.m(t,null)},p(l,s){s&2&&S(n,\"width\",l[1]+\"px\"),s&4&&S(n,\"height\",l[2]+\"px\"),l[3]!=null?o?o.p(l,s):(o=at(l),o.c(),o.m(t,null)):o&&(o.d(1),o=null),s&2&&S(t,\"width\",l[1]+\"px\"),s&4&&S(t,\"height\",l[2]+\"px\")},d(l){l&&j(t),e[9](null),o&&o.d()}}}function at(e){let t,n,i;return{c(){t=F(\"div\"),n=T(),i=F(\"div\"),y(t,\"class\",\"focus-top svelte-1fjpmsy\"),S(t,\"height\",e[2]*e[3]/e[0].shape[0]+\"px\"),y(i,\"class\",\"focus-bottom svelte-1fjpmsy\"),S(i,\"height\",e[2]*(1-(e[3]+1)/e[0].shape[0])+\"px\")},m(o,l){C(o,t,l),C(o,n,l),C(o,i,l)},p(o,l){l&13&&S(t,\"height\",o[2]*o[3]/o[0].shape[0]+\"px\"),l&13&&S(i,\"height\",o[2]*(1-(o[3]+1)/o[0].shape[0])+\"px\")},d(o){o&&j(t),o&&j(n),o&&j(i)}}}function ct(e){let t,n,i,o=e[3]!=null&&dt(e);return{c(){t=F(\"div\"),n=F(\"canvas\"),i=T(),o&&o.c(),S(n,\"width\",e[1]+\"px\"),S(n,\"height\",e[2]+\"px\"),y(n,\"class\",\"svelte-1fjpmsy\"),y(t,\"class\",\"container svelte-1fjpmsy\"),S(t,\"width\",e[1]+\"px\"),S(t,\"height\",e[2]+\"px\")},m(l,s){C(l,t,s),E(t,n),e[10](n),E(t,i),o&&o.m(t,null)},p(l,s){s&2&&S(n,\"width\",l[1]+\"px\"),s&4&&S(n,\"height\",l[2]+\"px\"),l[3]!=null?o?o.p(l,s):(o=dt(l),o.c(),o.m(t,null)):o&&(o.d(1),o=null),s&2&&S(t,\"width\",l[1]+\"px\"),s&4&&S(t,\"height\",l[2]+\"px\")},d(l){l&&j(t),e[10](null),o&&o.d()}}}function dt(e){let t,n,i;return{c(){t=F(\"div\"),n=T(),i=F(\"div\"),y(t,\"class\",\"focus-left svelte-1fjpmsy\"),S(t,\"width\",e[1]*e[3]/e[0].shape[1]+\"px\"),y(i,\"class\",\"focus-right svelte-1fjpmsy\"),S(i,\"width\",e[1]*(1-(e[3]+1)/e[0].shape[1])+\"px\")},m(o,l){C(o,t,l),C(o,n,l),C(o,i,l)},p(o,l){l&11&&S(t,\"width\",o[1]*o[3]/o[0].shape[1]+\"px\"),l&11&&S(i,\"width\",o[1]*(1-(o[3]+1)/o[0].shape[1])+\"px\")},d(o){o&&j(t),o&&j(n),o&&j(i)}}}function tn(e){let t,n,i=!e[4]&&ft(e),o=e[4]&&ct(e);return{c(){i&&i.c(),t=T(),o&&o.c(),n=tt()},m(l,s){i&&i.m(l,s),C(l,t,s),o&&o.m(l,s),C(l,n,s)},p(l,[s]){l[4]?i&&(i.d(1),i=null):i?i.p(l,s):(i=ft(l),i.c(),i.m(t.parentNode,t)),l[4]?o?o.p(l,s):(o=ct(l),o.c(),o.m(n.parentNode,n)):o&&(o.d(1),o=null)},i:P,o:P,d(l){i&&i.d(l),l&&j(t),o&&o.d(l),l&&j(n)}}}function nn(e,t,n){let{array:i}=t,{width:o}=t,{height:l}=t,{hues:s}=t,{focus_token:r}=t,{isolate_channel:f=void 0}=t,{hover_token_is_target:a=!1}=t,{color_map:c=ut}=t,d;function h(u,_,k,w=void 0,M=void 0){if(_<k)return[255,255,255];var N=u.pick(_,k,null),b=c(N,void 0,w,M);return b.map(v=>255*v)}function p(u,_,k=void 0,w=void 0){if(!(u==null||_==null)){u.width=_.shape[0],u.height=_.shape[1];for(var M=u.getContext(\"2d\"),N=M.getImageData(0,0,u.width,u.height),b=0;b<u.width;b++)for(var v=0;v<u.height;v++){for(var q=b*u.width+v,D=h(_,b,v,k,w=w),J=0;J<3;J++)N.data[4*q+J]=D[J];N.data[4*q+3]=255}M.putImageData(N,0,0)}}Ht(()=>p(d,i,f));function m(u){ne[u?\"unshift\":\"push\"](()=>{d=u,n(5,d)})}function g(u){ne[u?\"unshift\":\"push\"](()=>{d=u,n(5,d)})}return e.$$set=u=>{\"array\"in u&&n(0,i=u.array),\"width\"in u&&n(1,o=u.width),\"height\"in u&&n(2,l=u.height),\"hues\"in u&&n(6,s=u.hues),\"focus_token\"in u&&n(3,r=u.focus_token),\"isolate_channel\"in u&&n(7,f=u.isolate_channel),\"hover_token_is_target\"in u&&n(4,a=u.hover_token_is_target),\"color_map\"in u&&n(8,c=u.color_map)},e.$$.update=()=>{if(e.$$.dirty&225){e:p(d,i,f,s)}},[i,o,l,r,a,d,s,f,c,m,g]}class on extends xe{constructor(t){super(),document.getElementById(\"svelte-1fjpmsy-style\")||en(),He(this,t,nn,tn,je,{array:0,width:1,height:2,hues:6,focus_token:3,isolate_channel:7,hover_token_is_target:4,color_map:8})}}const qe=on;function _t(e,t){return e.mode==\"soft\"&&(e.value=t),e}function ln(e,t){if(e.mode==\"soft\")e.value=t,e.mode=\"hard\";else if(e.mode==\"hard\"&&e.value!=t)e.value=t;else return sn(e);return e}function sn(e){return e.value=void 0,e.mode=\"soft\",e}function rn(e){let t,n,i,o;const l=e[4].default,s=At(l,e,e[3],null);return{c(){t=F(\"div\"),s&&s.c(),y(t,\"style\",e[2])},m(r,f){C(r,t,f),s&&s.m(t,null),n=!0,i||(o=[z(t,\"mouseover\",e[5]),z(t,\"click\",e[6]),z(t,\"mouseout\",e[7])],i=!0)},p(r,[f]){s&&s.p&&f&8&&Ct(s,l,r,r[3],f,null,null),(!n||f&4)&&y(t,\"style\",r[2])},i(r){n||(O(s,r),n=!0)},o(r){L(s,r),n=!1},d(r){r&&j(t),s&&s.d(r),i=!1,H(o)}}}function un(e,t,n){let{$$slots:i={},$$scope:o}=t,{lock:l}=t,{set_value:s}=t,{style:r=\"\"}=t;const f=()=>{n(0,l=_t(l,s))},a=()=>{n(0,l=ln(l,s))},c=()=>{n(0,l=_t(l,void 0))};return e.$$set=d=>{\"lock\"in d&&n(0,l=d.lock),\"set_value\"in d&&n(1,s=d.set_value),\"style\"in d&&n(2,r=d.style),\"$$scope\"in d&&n(3,o=d.$$scope)},[l,s,r,o,i,f,a,c]}class fn extends xe{constructor(t){super(),He(this,t,un,rn,je,{lock:0,set_value:1,style:2})}}const ht=fn;function an(){var e=F(\"style\");e.id=\"svelte-xqk9oe-style\",e.textContent=\".attn-container.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{display:grid;grid-template-rows:[title] min-content [main] min-content;grid-template-columns:[big-attn] min-content [heads] minmax(min-content, 624px);gap:12px}.figcaption.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{color:#888;grid-row:title;white-space:nowrap}.tokens-container.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{display:grid;grid-template-rows:[title] min-content [main] min-content;grid-template-columns:[left] min-content [right] minmax(min-content, 800px) [end];gap:12px;margin-top:24px;color:white}.tokens.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{grid-row:main;grid-column-start:left;grid-column-end:end;cursor:pointer;height:min-content;line-height:110%}.tokens.svelte-xqk9oe .token.svelte-xqk9oe.svelte-xqk9oe{white-space:pre-wrap}.tokens.svelte-xqk9oe .selected.svelte-xqk9oe.svelte-xqk9oe{border:1px solid #999;z-index:10}.tokens.svelte-xqk9oe .token.svelte-xqk9oe.svelte-xqk9oe:not(.selected){z-index:0;padding:1px}.hover-mode.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.hover-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.info-mode.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.info-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{color:#888;grid-row:title;grid-column:settings;cursor:pointer}.hover-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.info-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{margin-right:8px}.heads.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{grid-column:heads;grid-row:main;display:flex;flex-direction:row;flex-wrap:wrap;gap:6px;height:min-content}.heads.svelte-xqk9oe .head-icon.svelte-xqk9oe.svelte-xqk9oe{position:relative;width:62px;height:62px}.heads.svelte-xqk9oe .head-icon.svelte-xqk9oe>.svelte-xqk9oe{position:absolute;right:0px;top:0px}.heads.svelte-xqk9oe .head-icon .head-label.svelte-xqk9oe.svelte-xqk9oe{background:#333;color:#eee;font-size:65%;padding:1px;border-bottom-left-radius:2px;padding-left:4px;padding-right:2px;min-width:14px;opacity:0.75}\",E(document.head,e)}function pt(e,t,n){const i=e.slice();return i[32]=t[n],i[34]=n,i}function mt(e,t,n){const i=e.slice();return i[35]=t[n],i}function gt(e){let t,n=e[15][e[10]]+\"\",i,o;return{c(){t=R(\"(\"),i=R(n),o=R(\")\")},m(l,s){C(l,t,s),C(l,i,s),C(l,o,s)},p(l,s){s[0]&33792&&n!==(n=l[15][l[10]]+\"\")&&ke(i,n)},d(l){l&&j(t),l&&j(i),l&&j(o)}}}function vt(e){let t,n,i,o;return n=new qe({props:{array:e[7],width:\"200\",height:\"200\",focus_token:e[9],hover_token_is_target:e[2],isolate_channel:e[10]}}),{c(){t=F(\"div\"),se(n.$$.fragment),y(t,\"style\",i=\"grid-column: big-attn; grid-row: main; \"+(e[11]?\"\":\"display:none;\"))},m(l,s){C(l,t,s),Z(n,t,null),o=!0},p(l,s){const r={};s[0]&128&&(r.array=l[7]),s[0]&512&&(r.focus_token=l[9]),s[0]&4&&(r.hover_token_is_target=l[2]),s[0]&1024&&(r.isolate_channel=l[10]),n.$set(r),(!o||s[0]&2048&&i!==(i=\"grid-column: big-attn; grid-row: main; \"+(l[11]?\"\":\"display:none;\")))&&y(t,\"style\",i)},i(l){o||(O(n.$$.fragment,l),o=!0)},o(l){L(n.$$.fragment,l),o=!1},d(l){l&&j(t),X(n)}}}function kt(e){let t,n,i,o,l=re(e[12].shape[2]),s=[];for(let f=0;f<l.length;f+=1)s[f]=bt(mt(e,l,f));const r=f=>L(s[f],1,1,()=>{s[f]=null});return{c(){t=F(\"div\"),t.textContent=\"Attention Heads (hover to focus, click to lock)\",n=T(),i=F(\"div\");for(let f=0;f<s.length;f+=1)s[f].c();y(t,\"class\",\"figcaption svelte-xqk9oe\"),S(t,\"grid-column\",\"heads\"),y(i,\"class\",\"heads svelte-xqk9oe\")},m(f,a){C(f,t,a),C(f,n,a),C(f,i,a);for(let c=0;c<s.length;c+=1)s[c].m(i,null);o=!0},p(f,a){if(a[0]&122566){l=re(f[12].shape[2]);let c;for(c=0;c<l.length;c+=1){const d=mt(f,l,c);s[c]?(s[c].p(d,a),O(s[c],1)):(s[c]=bt(d),s[c].c(),O(s[c],1),s[c].m(i,null))}for(oe(),c=l.length;c<s.length;c+=1)r(c);le()}},i(f){if(!o){for(let a=0;a<l.length;a+=1)O(s[a]);o=!0}},o(f){s=s.filter(Boolean);for(let a=0;a<s.length;a+=1)L(s[a]);o=!1},d(f){f&&j(t),f&&j(n),f&&j(i),et(s,f)}}}function cn(e){let t,n,i,o,l=(e[15][e[35]]!=null?e[15][e[35]]:\"&nbsp\")+\"\",s,r,f,a,c,d,h=(e[15][e[35]]!=null?e[15][e[35]]:\"&nbsp\")+\"\",p,m,g;return n=new qe({props:{array:e[7],width:\"60\",height:\"60\",isolate_channel:e[35]}}),a=new qe({props:{array:e[6],width:\"60\",height:\"60\",isolate_channel:e[35]}}),{c(){t=F(\"div\"),se(n.$$.fragment),i=T(),o=F(\"div\"),r=T(),f=F(\"div\"),se(a.$$.fragment),c=T(),d=F(\"div\"),m=T(),y(o,\"class\",\"head-label svelte-xqk9oe\"),S(o,\"background\",e[14][e[35]]),y(t,\"class\",\"head-icon svelte-xqk9oe\"),y(t,\"style\",s=\"opacity: \"+(e[10]!=null&&e[10]!=e[35]?\"0.2\":e[16](e[35],e[9],e[2]))+`;\n",
       "                        `+(e[11]?\"\":\"display:none;\")),y(d,\"class\",\"head-label svelte-xqk9oe\"),S(d,\"background\",e[14][e[35]]),y(f,\"class\",\"head-icon svelte-xqk9oe\"),y(f,\"style\",p=\"opacity: \"+(e[10]!=null&&e[10]!=e[35]?\"0.2\":e[16](e[35],e[9],e[2]))+`;\n",
       "                        `+(e[11]?\"display:none;\":\"\"))},m(u,_){C(u,t,_),Z(n,t,null),E(t,i),E(t,o),o.innerHTML=l,C(u,r,_),C(u,f,_),Z(a,f,null),E(f,c),E(f,d),d.innerHTML=h,C(u,m,_),g=!0},p(u,_){const k={};_[0]&128&&(k.array=u[7]),_[0]&4096&&(k.isolate_channel=u[35]),n.$set(k),(!g||_[0]&36864)&&l!==(l=(u[15][u[35]]!=null?u[15][u[35]]:\"&nbsp\")+\"\")&&(o.innerHTML=l),(!g||_[0]&20480)&&S(o,\"background\",u[14][u[35]]),(!g||_[0]&7684&&s!==(s=\"opacity: \"+(u[10]!=null&&u[10]!=u[35]?\"0.2\":u[16](u[35],u[9],u[2]))+`;\n",
       "                        `+(u[11]?\"\":\"display:none;\")))&&y(t,\"style\",s);const w={};_[0]&64&&(w.array=u[6]),_[0]&4096&&(w.isolate_channel=u[35]),a.$set(w),(!g||_[0]&36864)&&h!==(h=(u[15][u[35]]!=null?u[15][u[35]]:\"&nbsp\")+\"\")&&(d.innerHTML=h),(!g||_[0]&20480)&&S(d,\"background\",u[14][u[35]]),(!g||_[0]&7684&&p!==(p=\"opacity: \"+(u[10]!=null&&u[10]!=u[35]?\"0.2\":u[16](u[35],u[9],u[2]))+`;\n",
       "                        `+(u[11]?\"display:none;\":\"\")))&&y(f,\"style\",p)},i(u){g||(O(n.$$.fragment,u),O(a.$$.fragment,u),g=!0)},o(u){L(n.$$.fragment,u),L(a.$$.fragment,u),g=!1},d(u){u&&j(t),X(n),u&&j(r),u&&j(f),X(a),u&&j(m)}}}function bt(e){let t,n,i;function o(s){e[22].call(null,s)}let l={set_value:e[35],$$slots:{default:[cn]},$$scope:{ctx:e}};return e[1]!==void 0&&(l.lock=e[1]),t=new ht({props:l}),ne.push(()=>lt(t,\"lock\",o)),{c(){se(t.$$.fragment)},m(s,r){Z(t,s,r),i=!0},p(s,r){const f={};r[0]&4096&&(f.set_value=s[35]),r[0]&57028|r[1]&128&&(f.$$scope={dirty:r,ctx:s}),!n&&r[0]&2&&(n=!0,f.lock=s[1],ot(()=>n=!1)),t.$set(f)},i(s){i||(O(t.$$.fragment,s),i=!0)},o(s){L(t.$$.fragment,s),i=!1},d(s){X(t,s)}}}function yt(e){let t,n,i,o,l,s,r,f,a,c,d,h,p=e[2]?\"target\":\"source\",m,g,u,_,k,w=e[5],M=[];for(let v=0;v<w.length;v+=1)M[v]=wt(pt(e,w,v));const N=v=>L(M[v],1,1,()=>{M[v]=null});let b=e[7]!==void 0&&Ft(e);return{c(){t=F(\"div\"),n=F(\"div\"),n.textContent=\"Tokens (hover to focus, click to lock)\",i=T(),o=F(\"div\");for(let v=0;v<M.length;v+=1)M[v].c();l=T(),s=F(\"div\"),r=F(\"nobr\"),f=F(\"input\"),a=T(),c=F(\"span\"),d=R(`Selected is\n",
       "            `),h=F(\"b\"),m=R(p),g=T(),b&&b.c(),y(n,\"class\",\"figcaption svelte-xqk9oe\"),S(n,\"grid-column\",\"left\"),y(o,\"class\",\"tokens svelte-xqk9oe\"),y(f,\"class\",\"hover-mode svelte-xqk9oe\"),y(f,\"type\",\"checkbox\"),y(c,\"class\",\"hover-mode-text svelte-xqk9oe\"),S(c,\"white-space\",\"nowrap\"),y(s,\"class\",\"toggle\"),y(t,\"class\",\"tokens-container svelte-xqk9oe\")},m(v,q){C(v,t,q),E(t,n),E(t,i),E(t,o);for(let D=0;D<M.length;D+=1)M[D].m(o,null);E(t,l),E(t,s),E(s,r),E(r,f),f.checked=e[2],E(r,a),E(r,c),E(c,d),E(c,h),E(h,m),E(r,g),b&&b.m(r,null),u=!0,_||(k=[z(f,\"change\",e[24]),z(c,\"click\",e[25])],_=!0)},p(v,q){if(q[0]&561){w=v[5];let D;for(D=0;D<w.length;D+=1){const J=pt(v,w,D);M[D]?(M[D].p(J,q),O(M[D],1)):(M[D]=wt(J),M[D].c(),O(M[D],1),M[D].m(o,null))}for(oe(),D=w.length;D<M.length;D+=1)N(D);le()}q[0]&4&&(f.checked=v[2]),(!u||q[0]&4)&&p!==(p=v[2]?\"target\":\"source\")&&ke(m,p),v[7]!==void 0?b?b.p(v,q):(b=Ft(v),b.c(),b.m(r,null)):b&&(b.d(1),b=null)},i(v){if(!u){for(let q=0;q<w.length;q+=1)O(M[q]);u=!0}},o(v){M=M.filter(Boolean);for(let q=0;q<M.length;q+=1)L(M[q]);u=!1},d(v){v&&j(t),et(M,v),b&&b.d(),_=!1,H(k)}}}function dn(e){let t,n=e[32]+\"\",i,o;return{c(){t=F(\"span\"),i=R(n),y(t,\"class\",o=\"token \"+(e[34]==e[9]?\"selected\":\"\")+\" svelte-xqk9oe\"),S(t,\"background\",e[4][e[34]])},m(l,s){C(l,t,s),E(t,i)},p(l,s){s[0]&32&&n!==(n=l[32]+\"\")&&ke(i,n),s[0]&512&&o!==(o=\"token \"+(l[34]==l[9]?\"selected\":\"\")+\" svelte-xqk9oe\")&&y(t,\"class\",o),s[0]&16&&S(t,\"background\",l[4][l[34]])},d(l){l&&j(t)}}}function wt(e){let t,n,i;function o(s){e[23].call(null,s)}let l={set_value:e[34],style:\"display: inline\",$$slots:{default:[dn]},$$scope:{ctx:e}};return e[0]!==void 0&&(l.lock=e[0]),t=new ht({props:l}),ne.push(()=>lt(t,\"lock\",o)),{c(){se(t.$$.fragment)},m(s,r){Z(t,s,r),i=!0},p(s,r){const f={};r[0]&560|r[1]&128&&(f.$$scope={dirty:r,ctx:s}),!n&&r[0]&1&&(n=!0,f.lock=s[0],ot(()=>n=!1)),t.$set(f)},i(s){i||(O(t.$$.fragment,s),i=!0)},o(s){L(t.$$.fragment,s),i=!1},d(s){X(t,s)}}}function Ft(e){let t,n,i,o,l,s=e[3]?\"info-weighted\":\"unmodified\",r,f,a;return{c(){t=F(\"input\"),n=T(),i=F(\"span\"),o=R(`Attention is\n",
       "            `),l=F(\"b\"),r=R(s),y(t,\"class\",\"info-mode svelte-xqk9oe\"),y(t,\"type\",\"checkbox\"),y(i,\"class\",\"info-mode-text svelte-xqk9oe\"),S(i,\"white-space\",\"nowrap\")},m(c,d){C(c,t,d),t.checked=e[3],C(c,n,d),C(c,i,d),E(i,o),E(i,l),E(l,r),f||(a=[z(t,\"change\",e[26]),z(i,\"click\",e[27])],f=!0)},p(c,d){d[0]&8&&(t.checked=c[3]),d[0]&8&&s!==(s=c[3]?\"info-weighted\":\"unmodified\")&&ke(r,s)},d(c){c&&j(t),c&&j(n),c&&j(i),f=!1,H(a)}}}function _n(e){let t,n,i,o,l,s,r,f,a,c,d,h,p=e[10]!=null&&gt(e),m=e[7]!==void 0&&vt(e);r=new qe({props:{array:e[6],width:\"200\",height:\"200\",focus_token:e[9],hover_token_is_target:e[2],isolate_channel:e[10]}});let g=e[13]>1&&kt(e),u=e[8]&&yt(e);return{c(){t=F(\"div\"),n=F(\"div\"),i=R(`Attention Pattern\n",
       "        `),p&&p.c(),o=T(),m&&m.c(),l=T(),s=F(\"div\"),se(r.$$.fragment),a=T(),g&&g.c(),c=T(),u&&u.c(),d=tt(),y(n,\"class\",\"figcaption svelte-xqk9oe\"),S(n,\"grid-column\",\"big-attn\"),y(s,\"style\",f=\"grid-column: big-attn; grid-row: main; \"+(e[11]?\"display:none\":\"\")),y(t,\"class\",\"attn-container svelte-xqk9oe\")},m(_,k){C(_,t,k),E(t,n),E(n,i),p&&p.m(n,null),E(t,o),m&&m.m(t,null),E(t,l),E(t,s),Z(r,s,null),E(t,a),g&&g.m(t,null),C(_,c,k),u&&u.m(_,k),C(_,d,k),h=!0},p(_,k){_[10]!=null?p?p.p(_,k):(p=gt(_),p.c(),p.m(n,null)):p&&(p.d(1),p=null),_[7]!==void 0?m?(m.p(_,k),k[0]&128&&O(m,1)):(m=vt(_),m.c(),O(m,1),m.m(t,l)):m&&(oe(),L(m,1,1,()=>{m=null}),le());const w={};k[0]&64&&(w.array=_[6]),k[0]&512&&(w.focus_token=_[9]),k[0]&4&&(w.hover_token_is_target=_[2]),k[0]&1024&&(w.isolate_channel=_[10]),r.$set(w),(!h||k[0]&2048&&f!==(f=\"grid-column: big-attn; grid-row: main; \"+(_[11]?\"display:none\":\"\")))&&y(s,\"style\",f),_[13]>1?g?(g.p(_,k),k[0]&8192&&O(g,1)):(g=kt(_),g.c(),O(g,1),g.m(t,null)):g&&(oe(),L(g,1,1,()=>{g=null}),le()),_[8]?u?(u.p(_,k),k[0]&256&&O(u,1)):(u=yt(_),u.c(),O(u,1),u.m(d.parentNode,d)):u&&(oe(),L(u,1,1,()=>{u=null}),le())},i(_){h||(O(m),O(r.$$.fragment,_),O(g),O(u),h=!0)},o(_){L(m),L(r.$$.fragment,_),L(g),L(u),h=!1},d(_){_&&j(t),p&&p.d(),m&&m.d(),X(r),g&&g.d(),_&&j(c),u&&u.d(_),_&&j(d)}}}function re(e){return[...Array(e).keys()]}function qt(e){if(e!==void 0){for(var t=[],n=0;n<e.shape[0];n++){t.push([]);for(var i=0;i<e.shape[2];i++){for(var o=0,l=0;l<e.shape[1];l++)o=Math.max(o,e.pick(n,l,i));t[n].push(o)}}return t}}function Et(e){if(e!==void 0){for(var t=[],n=0;n<e.shape[0];n++){t.push([]);for(var i=0;i<e.shape[2];i++){for(var o=0,l=0;l<e.shape[1];l++)o=Math.max(o,e.pick(l,n,i));t[n].push(o)}}return t}}function hn(e,t,n){let i,o,l,s,r,f,a,c,d,h,p,m,g,{tokens:u}=t,{attention:_}=t,{info_weighted:k}=t,{head_labels:w}=t,{show_tokens:M=!0}=t,{focus_token_lock:N={value:void 0,mode:\"soft\"}}=t,{focus_head_lock:b={value:void 0,mode:\"soft\"}}=t,{hover_token_is_target:v=!1}=t,{_show_info_weighted:q=!1}=t;function D(A,Y,G,$=void 0){if(Y<G)return\"#FFF\";var ue=A.pick(Y,G,null);return We(ue,void 0,$)}function J(A,Y,G){if(Y==null)var $=1;else var ue=G?h:d,$=Math.max(0,Math.min(1,ue[Y][A]));return\"\"+$}function gn(A,Y,G,$=void 0,ue){if(Y==null){var qn=ue?h:d;return We(qn[G],void 0,$)}let Ue,Je;ue?(Ue=G,Je=Y):(Ue=Y,Je=G);let Xe=D(A,Ue,Je,$);return Xe===\"#FFF\"&&(Xe=\"#DDD\"),Xe}let{all_token_colors:Ye}=t;function vn(A){b=A,n(1,b)}function kn(A){N=A,n(0,N)}function bn(){v=this.checked,n(2,v)}const yn=()=>n(2,v=v^!0);function wn(){q=this.checked,n(3,q)}const Fn=()=>n(3,q=q^!0);return e.$$set=A=>{\"tokens\"in A&&n(5,u=A.tokens),\"attention\"in A&&n(6,_=A.attention),\"info_weighted\"in A&&n(7,k=A.info_weighted),\"head_labels\"in A&&n(17,w=A.head_labels),\"show_tokens\"in A&&n(8,M=A.show_tokens),\"focus_token_lock\"in A&&n(0,N=A.focus_token_lock),\"focus_head_lock\"in A&&n(1,b=A.focus_head_lock),\"hover_token_is_target\"in A&&n(2,v=A.hover_token_is_target),\"_show_info_weighted\"in A&&n(3,q=A._show_info_weighted),\"all_token_colors\"in A&&n(4,Ye=A.all_token_colors)},e.$$.update=()=>{if(e.$$.dirty[0]&1){e:n(9,i=N.value)}if(e.$$.dirty[0]&2){e:n(10,o=b.value)}if(e.$$.dirty[0]&136){e:n(11,l=q&&k!==void 0)}if(e.$$.dirty[0]&2240){e:n(12,s=l?k:_)}if(e.$$.dirty[0]&4096){e:window.attention_show=s}if(e.$$.dirty[0]&64){e:n(18,r=qt(_))}if(e.$$.dirty[0]&64){e:n(19,f=Et(_))}if(e.$$.dirty[0]&128){e:n(20,a=qt(k))}if(e.$$.dirty[0]&128){e:n(21,c=Et(k))}if(e.$$.dirty[0]&1312768){e:d=l?a:r}if(e.$$.dirty[0]&2623488){e:h=l?c:f}if(e.$$.dirty[0]&64){e:n(13,p=_.shape[2])}if(e.$$.dirty[0]&8192){e:n(14,m=re(p).map(A=>We(re(p).map(Y=>1),void 0,A)))}if(e.$$.dirty[0]&139264){e:n(15,g=w!=null?w:re(p))}if(e.$$.dirty[0]&5668){e:n(4,Ye=re(u.length).map(A=>gn(s,i,A,o,v)))}},[N,b,v,q,Ye,u,_,k,M,i,o,l,s,p,m,g,J,w,r,f,a,c,vn,kn,bn,yn,wn,Fn]}class pn extends xe{constructor(t){super(),document.getElementById(\"svelte-xqk9oe-style\")||an(),He(this,t,hn,_n,je,{tokens:5,attention:6,info_weighted:7,head_labels:17,show_tokens:8,focus_token_lock:0,focus_head_lock:1,hover_token_is_target:2,_show_info_weighted:3,all_token_colors:4},[-1,-1])}}const mn=pn}},Ee={};function ee(x){if(Ee[x])return Ee[x].exports;var B=Ee[x]={exports:{}};return St[x](B,B.exports,ee),B.exports}return ee.d=(x,B)=>{for(var te in B)ee.o(B,te)&&!ee.o(x,te)&&Object.defineProperty(x,te,{enumerable:!0,get:B[te]})},ee.o=(x,B)=>Object.prototype.hasOwnProperty.call(x,B),ee(143)})().default;\n",
       "</script>\n",
       "        \n",
       "        <div id=\"AttentionMulti_3ea3db3\"></div>\n",
       "        <script>\n",
       "        ( () => {\n",
       "            var data = {\n",
       "\"tokens\": [\n",
       "\"<|endoftext|>\",\n",
       "\"I\",\n",
       "\" am\",\n",
       "\" an\",\n",
       "\" amazing\",\n",
       "\" aut\",\n",
       "\"ore\",\n",
       "\"gressive\",\n",
       "\",\",\n",
       "\" dec\",\n",
       "\"oder\",\n",
       "\"-\",\n",
       "\"only\",\n",
       "\",\",\n",
       "\" G\",\n",
       "\"PT\",\n",
       "\"-\",\n",
       "\"2\",\n",
       "\" style\",\n",
       "\" transformer\",\n",
       "\".\",\n",
       "\" One\",\n",
       "\" day\",\n",
       "\" I\",\n",
       "\" will\",\n",
       "\" exceed\",\n",
       "\" human\",\n",
       "\" level\",\n",
       "\" intelligence\",\n",
       "\" and\",\n",
       "\" take\",\n",
       "\" over\",\n",
       "\" the\",\n",
       "\" world\",\n",
       "\"!\"\n",
       "],\n",
       "\"attention\": {\n",
       "\"__type__\": \"npy\",\n",
       "\"zdata\": \"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\",\n",
       "\"min\": 0.0,\n",
       "\"max\": 1.0\n",
       "}\n",
       "};\n",
       "            data = loader.unpack_obj(data);\n",
       "            window.AttentionMulti_data = data;\n",
       "            var AttentionMulti_inst = new AttentionMulti({\n",
       "                \"target\": document.getElementById(\"AttentionMulti_3ea3db3\"),\n",
       "                \"props\": data\n",
       "                });\n",
       "        })();\n",
       "        </script>\n",
       "        \n",
       "        "
      ],
      "text/plain": [
       "<pysvelte.html.Html at 0x7d87407690d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "tokens = tokens.cuda()\n",
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "print(logits.shape)\n",
    "pysvelte.AttentionMulti(tokens=reference_gpt2.to_str_tokens(reference_text), attention=cache['blocks.0.attn.hook_attn'][0].permute(1, 2, 0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:33.260736Z",
     "iopub.status.busy": "2025-06-14T15:28:33.259957Z",
     "iopub.status.idle": "2025-06-14T15:28:33.272247Z",
     "shell.execute_reply": "2025-06-14T15:28:33.271460Z",
     "shell.execute_reply.started": "2025-06-14T15:28:33.260711Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
    "\n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "\n",
    "        q = einsum(\"batch position d_model, n_heads d_model d_head -> batch position n_heads d_head\",normalized_resid_pre,self.W_Q) + self.b_Q\n",
    "\n",
    "        k = einsum(\"batch position d_model, n_heads d_model d_head -> batch position n_heads d_head\",normalized_resid_pre,self.W_K) + self.b_K\n",
    "        v = einsum(\"batch position d_model, n_heads d_model d_head -> batch position n_heads d_head\",normalized_resid_pre,self.W_V) + self.b_V\n",
    "\n",
    "        score = einsum(\"batch qposition n_heads d_head, batch kposition n_heads d_head -> batch n_heads qposition kposition\",q,k)\n",
    "        score = score / math.sqrt(self.cfg.d_head)\n",
    "        causal_score = self.apply_causal_mask(score)\n",
    "        attn = torch.nn.Softmax(dim=-1)(causal_score)\n",
    "\n",
    "        context = einsum(\"batch n_heads qposition kposition, batch kposition n_heads d_head -> batch qposition n_heads d_head\",attn,v)\n",
    "\n",
    "        context = einsum(\"batch position n_heads d_head, n_heads d_head d_model -> batch position d_model\",context,self.W_O) + self.b_O\n",
    "\n",
    "        return context,attn\n",
    "    def apply_causal_mask(self, attn_scores):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
    "        \"YOUR CODE HERE\"\n",
    "        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n",
    "\n",
    "        #print(mask,self.IGNORE)\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:33.273950Z",
     "iopub.status.busy": "2025-06-14T15:28:33.273520Z",
     "iopub.status.idle": "2025-06-14T15:28:33.334054Z",
     "shell.execute_reply": "2025-06-14T15:28:33.333174Z",
     "shell.execute_reply.started": "2025-06-14T15:28:33.273919Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "    def forward(self, normalized_resid_mid):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "        outputs = gelu_new(einsum(\"batch position d_model, d_model d_mlp -> batch position d_mlp\",normalized_resid_mid,self.W_in) + self.b_in)\n",
    "        outputs = einsum(\"batch position d_mlp, d_mlp d_model -> batch position d_model\",outputs,self.W_out) + self.b_out\n",
    "        return outputs\n",
    "\n",
    "# rand_float_test(MLP, [2, 4, 768])\n",
    "# load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"blocks.0.ln2.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:33.335277Z",
     "iopub.status.busy": "2025-06-14T15:28:33.334985Z",
     "iopub.status.idle": "2025-06-14T15:28:33.351617Z",
     "shell.execute_reply": "2025-06-14T15:28:33.351013Z",
     "shell.execute_reply.started": "2025-06-14T15:28:33.335251Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "        outputs,attn = self.attn(resid_pre) \n",
    "        outputs = outputs  #+ resid_pre  #self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        #outputs = self.mlp(outputs)+outputs #self.mlp(self.ln2(outputs))+outputs\n",
    "        return outputs,attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:33.352670Z",
     "iopub.status.busy": "2025-06-14T15:28:33.352397Z",
     "iopub.status.idle": "2025-06-14T15:28:33.366616Z",
     "shell.execute_reply": "2025-06-14T15:28:33.366086Z",
     "shell.execute_reply.started": "2025-06-14T15:28:33.352646Z"
    }
   },
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=True))\n",
    "\n",
    "    def forward(self, normalized_resid_final):\n",
    "        # normalized_resid_final [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "        outputs = einsum(\"batch position d_model, d_model d_vocab -> batch position d_vocab\", normalized_resid_final,self.W_U) + self.b_U\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:33.367711Z",
     "iopub.status.busy": "2025-06-14T15:28:33.367386Z",
     "iopub.status.idle": "2025-06-14T15:28:33.381842Z",
     "shell.execute_reply": "2025-06-14T15:28:33.381165Z",
     "shell.execute_reply.started": "2025-06-14T15:28:33.367687Z"
    }
   },
   "outputs": [],
   "source": [
    "class Classification_Head(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(torch.empty((cfg.d_model,cfg.n_classes)))\n",
    "        nn.init.normal_(self.W_U,std = self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((cfg.n_classes),requires_grad=True))\n",
    "    def forward(self, normalized_resid_final):\n",
    "        # normalized_resid_final [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "        outputs = einsum(\"batch d_model, d_model n_classes -> batch n_classes\", normalized_resid_final,self.W_U) + self.b_U\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder based Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:33.382944Z",
     "iopub.status.busy": "2025-06-14T15:28:33.382665Z",
     "iopub.status.idle": "2025-06-14T15:28:33.397996Z",
     "shell.execute_reply": "2025-06-14T15:28:33.397202Z",
     "shell.execute_reply.started": "2025-06-14T15:28:33.382914Z"
    }
   },
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        #self.unembed = Unembed(cfg)\n",
    "        self.cls_head = Classification_Head(cfg)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens [batch, position]\n",
    "        \"YOUR CODE HERE\"\n",
    "        embed = self.embed(tokens)\n",
    "        pos_embed = self.pos_embed(tokens)\n",
    "        residual = embed + pos_embed\n",
    "        attention_per_block = []\n",
    "        for block in self.blocks:\n",
    "            residual,attn = block(residual)\n",
    "            attention_per_block.append(attn)\n",
    "        #print(residual.shape)\n",
    "        normalized_resid_final = residual #self.ln_final(residual)\n",
    "        pad_indices = (tokens==0 ).int().argmax(dim=1)\n",
    "\n",
    "        #pad_mask = (torch.arange(tokens.size(1), device=device).unsqueeze(0) <= pad_indices.unsqueeze(1)).float()\n",
    "\n",
    "        \n",
    "        outputs = normalized_resid_final[np.arange(normalized_resid_final.size(0)),pad_indices,:]\n",
    "        #print(outputs.shape)\n",
    "        #normalized_resid_final = einsum(\"batch position dmodel, batch position -> batch position dmodel\", normalized_resid_final, pad_mask)\n",
    "        #outputs = einops.reduce(normalized_resid_final,\"batch position dmodel -> batch dmodel\",reduction=\"sum\"  ) /einops.reduce(pad_mask,\"batch position -> batch 1\",reduction=\"sum\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        outputs = self.cls_head(outputs)\n",
    "        \n",
    "        return outputs,attention_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:33.398980Z",
     "iopub.status.busy": "2025-06-14T15:28:33.398725Z",
     "iopub.status.idle": "2025-06-14T15:28:33.415170Z",
     "shell.execute_reply": "2025-06-14T15:28:33.414380Z",
     "shell.execute_reply.started": "2025-06-14T15:28:33.398955Z"
    }
   },
   "outputs": [],
   "source": [
    "def lm_cross_entropy_loss(logits, tokens):\n",
    "    # Measure next token loss\n",
    "    # Logits have shape [batch, position, d_vocab]\n",
    "    # Tokens have shape [batch, position]\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    pred_log_probs = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    return -pred_log_probs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:33.416551Z",
     "iopub.status.busy": "2025-06-14T15:28:33.416212Z",
     "iopub.status.idle": "2025-06-14T15:28:58.900966Z",
     "shell.execute_reply": "2025-06-14T15:28:58.900325Z",
     "shell.execute_reply.started": "2025-06-14T15:28:33.416532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word statistics files not found!\n",
      "Downloading... done!\n",
      "Unpacking... done!\n",
      "Reading twitter - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
      "Reading twitter - 2grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n"
     ]
    }
   ],
   "source": [
    "##### text preprocessor for ekphrasis\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    #corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "#### Bert tokenizer\n",
    "def custom_tokenize(sent,tokenizer,max_length=512):\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    try:\n",
    "\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
    "                            #max_length = max_length,\n",
    "                            # This function also supports truncation and conversion\n",
    "                            # to pytorch tensors, but we need to do padding, so we\n",
    "                            # can't use these features :( .\n",
    "                            #max_length = 128,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "\n",
    "    except ValueError:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            ' ',                      # Sentence to encode.\n",
    "                            add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_length,\n",
    "                    \n",
    "                       )\n",
    "          ### decide what to later\n",
    "\n",
    "    return encoded_sent\n",
    "\n",
    "\n",
    "#input: text\n",
    "#process: ekphrasis preprocesser + some extra processing  \n",
    "#output: list of tokens      \n",
    "def ek_extra_preprocess(text,tokenizer):\n",
    "    remove_words=['<allcaps>','</allcaps>','<hashtag>','</hashtag>','<elongated>','<emphasis>','<repeated>','\\'','s']\n",
    "    word_list=text_processor.pre_process_doc(text)\n",
    "\n",
    "    word_list=list(filter(lambda a: a not in remove_words, word_list)) \n",
    "    sent=\" \".join(word_list)\n",
    "    sent = re.sub(r\"[<\\*>]\", \" \",sent)\n",
    "    sub_word_list = custom_tokenize(sent,tokenizer)\n",
    "    return sub_word_list\n",
    "\n",
    "\n",
    "#input: text\n",
    "#process: remove html tags  \n",
    "#output: text with no html tags\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Preprocessing queries for raw text not needed for implementation\n",
    "special_cases = {}\n",
    "# Times\n",
    "for h in range(1, 12 + 1):\n",
    "    for period in [\"a.m.\", \"am\"]:\n",
    "        special_cases[\"%d%s\" % (h, period)] = [\n",
    "            {ORTH: \"%d\" % h},\n",
    "            {ORTH: period, LEMMA: \"a.m.\", NORM: \"a.m.\"},\n",
    "        ]\n",
    "    for period in [\"p.m.\", \"pm\"]:\n",
    "        special_cases[\"%d%s\" % (h, period)] = [\n",
    "            {ORTH: \"%d\" % h},\n",
    "            {ORTH: period, LEMMA: \"p.m.\", NORM: \"p.m.\"},\n",
    "        ]\n",
    "        \n",
    "for orth in [\n",
    "        \"a.m.\",\n",
    "        \"Adm.\",\n",
    "        \"Bros.\",\n",
    "        \"co.\",\n",
    "        \"Co.\",\n",
    "        \"Corp.\",\n",
    "        \"D.C.\",\n",
    "        \"Dr.\",\n",
    "        \"e.g.\",\n",
    "        \"E.g.\",\n",
    "        \"E.G.\",\n",
    "        \"Gen.\",\n",
    "        \"Gov.\",\n",
    "        \"i.e.\",\n",
    "        \"I.e.\",\n",
    "        \"I.E.\",\n",
    "        \"Inc.\",\n",
    "        \"Jr.\",\n",
    "        \"Ltd.\",\n",
    "        \"Md.\",\n",
    "        \"Messrs.\",\n",
    "        \"Mo.\",\n",
    "        \"Mont.\",\n",
    "        \"Mr.\",\n",
    "        \"Mrs.\",\n",
    "        \"Ms.\",\n",
    "        \"p.m.\",\n",
    "        \"Ph.D.\",\n",
    "        \"Prof.\",\n",
    "        \"Rep.\",\n",
    "        \"Rev.\",\n",
    "        \"Sen.\",\n",
    "        \"St.\",\n",
    "        \"vs.\",\n",
    "        \"v.s.\",\n",
    "        ]:\n",
    "    special_cases[orth] = [{ORTH: orth}]\n",
    "    \n",
    "#print (special_cases)\n",
    "\n",
    "\n",
    "\n",
    "def preProcessing(query):\n",
    "    queryLower = query.lower()\n",
    "    if queryLower.startswith('eli5'):\n",
    "        cutMarker = queryLower.find(' ') + 1\n",
    "        query = query[cutMarker:]\n",
    "    \n",
    "    \n",
    "    nlp2.tokenizer.rules = special_cases \n",
    "    \n",
    "    #simple_url_re = re.compile(r'''^https?://''')\n",
    "    #nlp2.tokenizer.token_match = {}\n",
    "    \n",
    "    #print(nlp.tokenizer.rules)\n",
    "    prefixes = (\n",
    "        [\"§\", \"%\", \"=\", \"—\", \"–\", r\"\\+(?![0-9])\"]\n",
    "        + LIST_PUNCT\n",
    "        + LIST_ELLIPSES\n",
    "        + LIST_QUOTES\n",
    "        + LIST_CURRENCY\n",
    "        + LIST_ICONS\n",
    "    )\n",
    "\n",
    "\n",
    "    suffixes = (\n",
    "        LIST_PUNCT\n",
    "        + LIST_ELLIPSES\n",
    "        + LIST_QUOTES\n",
    "        + LIST_ICONS\n",
    "        + [\"'s\", \"'S\", \"’s\", \"’S\", \"—\", \"–\"]\n",
    "        + [\n",
    "            r\"(?<=[0-9])\\+\",\n",
    "            r\"(?<=°[FfCcKk])\\.\",\n",
    "            r\"(?<=[0-9])(?:{c})\".format(c=CURRENCY),\n",
    "            r\"(?<=[0-9])(?:{u})\".format(u=UNITS),\n",
    "            r\"(?<=[0-9{al}{e}{p}(?:{q})])\\.\".format(\n",
    "                al=ALPHA_LOWER, e=r\"%²\\-\\+\", q=CONCAT_QUOTES, p=PUNCT\n",
    "            ),\n",
    "            r\"(?<=[{au}][{au}])\\.\".format(au=ALPHA_UPPER),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    infixes = (\n",
    "        LIST_ELLIPSES\n",
    "        + LIST_ICONS\n",
    "        + [\n",
    "            r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "            r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
    "                al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "            ),\n",
    "            r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "            #r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
    "            r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prefixes_re = compile_prefix_regex(prefixes)\n",
    "    nlp2.tokenizer.prefix_search=prefixes_re.search\n",
    "    \n",
    "    suffixes_re = compile_suffix_regex(suffixes)\n",
    "    nlp2.tokenizer.suffix_search=suffixes_re.search\n",
    "    \n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "    nlp2.tokenizer.infix_finditer = infix_re.finditer\n",
    "    \n",
    "    query = query.replace('\\n', ' ')\n",
    "    query = query.replace('\\t', ' ')\n",
    "    query = re.sub(r'(\\w\\w)\\?(\\w\\w)', r'\\1 ? \\2', query)\n",
    "    query = query.replace('(', ' ( ')\n",
    "    query = query.replace(')', ' ) ')\n",
    "    query = query.replace('   ', ' ')\n",
    "    query = query.replace('  ', ' ')\n",
    "   \n",
    "    doc = nlp2(query)#, disable=['parser', 'ner'])\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.text != ' ':\n",
    "            tokens.append(token.text) \n",
    "        \n",
    "    if len(tokens) == 0:\n",
    "        print(\"Zero token sentence detected!\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:58.902233Z",
     "iopub.status.busy": "2025-06-14T15:28:58.901965Z",
     "iopub.status.idle": "2025-06-14T15:28:58.940817Z",
     "shell.execute_reply": "2025-06-14T15:28:58.939917Z",
     "shell.execute_reply.started": "2025-06-14T15:28:58.902206Z"
    }
   },
   "outputs": [],
   "source": [
    "def returnMask(row,tokenizer):\n",
    "    max_length = 128\n",
    "    text_tokens=row['text']\n",
    "    ##### a very rare corner case\n",
    "    if(len(text_tokens)==0):\n",
    "        text_tokens=['dummy']\n",
    "        print(\"length of text ==0\")\n",
    "    mask_all = row['rationales']\n",
    "    \n",
    "    mask_all_temp=mask_all\n",
    "    count_temp=0\n",
    "    while(len(mask_all_temp)!=3):\n",
    "        mask_all_temp.append([0]*len(text_tokens))\n",
    "    #print(len(mask_all_temp),len(mask_all_temp[0]))\n",
    "    \n",
    "    word_mask_all=[]\n",
    "    word_tokens_all=[]\n",
    "    \n",
    "    \n",
    "    for mask in mask_all_temp:\n",
    "        if(mask[0]==-1):\n",
    "            mask=[0]*len(mask)\n",
    "        list_pos=[]\n",
    "        mask_pos=[]\n",
    "        flag=0\n",
    "        for i in range(0,len(mask)):\n",
    "            if(i==0 and mask[i]==0):\n",
    "                list_pos.append(0)\n",
    "                mask_pos.append(0)\n",
    "            if(flag==0 and mask[i]==1):\n",
    "                mask_pos.append(1)\n",
    "                list_pos.append(i)\n",
    "                flag=1\n",
    "            elif(flag==1 and mask[i]==0):\n",
    "                flag=0\n",
    "                mask_pos.append(0)\n",
    "                list_pos.append(i)\n",
    "        if(list_pos[-1]!=len(mask)):\n",
    "            list_pos.append(len(mask))\n",
    "            mask_pos.append(0)\n",
    "        string_parts=[]\n",
    "        for i in range(len(list_pos)-1):\n",
    "            string_parts.append(text_tokens[list_pos[i]:list_pos[i+1]])\n",
    "        #print(\"Flag 6\",string_parts)\n",
    "\n",
    "        word_tokens=[]\n",
    "        word_mask=[]\n",
    "\n",
    "        #print(len(string_parts))\n",
    "        for i in range(0,len(string_parts)):\n",
    "            \n",
    "            tokens=ek_extra_preprocess(\" \".join(string_parts[i]),tokenizer)\n",
    "            #print(\"Flag 5\",tokens)\n",
    "            #print(len(tokens))\n",
    "            masks=[mask_pos[i]]*len(tokens)\n",
    "            word_tokens+=tokens\n",
    "            word_mask+=masks\n",
    "        #print(len(word_tokens),len(word_mask))\n",
    "\n",
    "        \n",
    "        word_tokens=word_tokens[0:(max_length)]\n",
    "        word_mask=word_mask[0:(max_length)]\n",
    "        # word_tokens.append(102)\n",
    "        # word_mask.append(0)\n",
    "\n",
    "        word_mask_all.append(word_mask)\n",
    "        word_tokens_all.append(word_tokens)\n",
    "    if(len(mask_all)==0):\n",
    "        word_mask_all=[]\n",
    "    else:    \n",
    "        word_mask_all=word_mask_all[0:len(mask_all)]  \n",
    "    return word_tokens_all[0],word_mask_all    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:58.942033Z",
     "iopub.status.busy": "2025-06-14T15:28:58.941580Z",
     "iopub.status.idle": "2025-06-14T15:28:58.963587Z",
     "shell.execute_reply": "2025-06-14T15:28:58.962811Z",
     "shell.execute_reply.started": "2025-06-14T15:28:58.942002Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "# print(dataset)\n",
    "# print(dataset[0]['text'][:100])\n",
    "# tokens_dataset = tokenize_and_concatenate(dataset, reference_gpt2.tokenizer, streaming=False, max_length=model_cfg.n_ctx, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "# data_loader = torch.utils.data.DataLoader(tokens_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:58.965192Z",
     "iopub.status.busy": "2025-06-14T15:28:58.964420Z",
     "iopub.status.idle": "2025-06-14T15:28:58.978652Z",
     "shell.execute_reply": "2025-06-14T15:28:58.978030Z",
     "shell.execute_reply.started": "2025-06-14T15:28:58.965167Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"hatexplain\")\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(dataset[\"train\"], batch_size=1, shuffle=False)\n",
    "\n",
    "# testloader = torch.utils.data.DataLoader(dataset[\"test\"], batch_size=1, shuffle=False)\n",
    "# valloader = torch.utils.data.DataLoader(dataset[\"validation\"], batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:58.979754Z",
     "iopub.status.busy": "2025-06-14T15:28:58.979489Z",
     "iopub.status.idle": "2025-06-14T15:28:59.002600Z",
     "shell.execute_reply": "2025-06-14T15:28:59.002045Z",
     "shell.execute_reply.started": "2025-06-14T15:28:58.979732Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:28:59.003596Z",
     "iopub.status.busy": "2025-06-14T15:28:59.003400Z",
     "iopub.status.idle": "2025-06-14T15:29:00.215402Z",
     "shell.execute_reply": "2025-06-14T15:29:00.214591Z",
     "shell.execute_reply.started": "2025-06-14T15:28:59.003580Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/dataset.json', 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "dict_data=[]\n",
    "for key in data:\n",
    "    temp={}\n",
    "    temp['post_id']=key\n",
    "    temp['text']=data[key]['post_tokens']\n",
    "    final_label=[]\n",
    "    for i in range(1,4):\n",
    "        temp['annotatorid'+str(i)]=data[key]['annotators'][i-1]['annotator_id']\n",
    "        temp['target'+str(i)]=data[key]['annotators'][i-1]['target']\n",
    "        temp['label'+str(i)]=data[key]['annotators'][i-1]['label']\n",
    "        final_label.append(temp['label'+str(i)])\n",
    "\n",
    "    final_label_id=max(final_label,key=final_label.count)\n",
    "    temp['rationales']=data[key]['rationales']\n",
    "    #print(temp[\"rationales\"])\n",
    "    if(final_label.count(final_label_id)==1):\n",
    "        temp['final_label']='undecided'\n",
    "    else:\n",
    "        temp['final_label']=final_label_id    \n",
    "    dict_data.append(temp)    \n",
    "temp_read = pd.DataFrame(dict_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:34:37.981834Z",
     "iopub.status.busy": "2025-06-14T15:34:37.981506Z",
     "iopub.status.idle": "2025-06-14T15:34:37.987836Z",
     "shell.execute_reply": "2025-06-14T15:34:37.987122Z",
     "shell.execute_reply.started": "2025-06-14T15:34:37.981811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ion', 'hang', 'wit', 'bitches', 'who', 'niggas', 'are', 'insecure'],\n",
       " [[0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0]],\n",
       " 'offensive')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_read.iloc[83].text,temp_read.iloc[83].rationales,temp_read.iloc[83].final_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T15:33:45.512908Z",
     "iopub.status.busy": "2025-06-14T15:33:45.512150Z",
     "iopub.status.idle": "2025-06-14T15:33:45.519531Z",
     "shell.execute_reply": "2025-06-14T15:33:45.518751Z",
     "shell.execute_reply.started": "2025-06-14T15:33:45.512878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post_id                               1178780987049889794_twitter\n",
       "text            [this, nigger, calling, me, juicy, lips, over,...\n",
       "annotatorid1                                                   30\n",
       "target1                                                 [African]\n",
       "label1                                                 hatespeech\n",
       "annotatorid2                                                   31\n",
       "target2                                                 [African]\n",
       "label2                                                 hatespeech\n",
       "annotatorid3                                                   32\n",
       "target3                                                 [African]\n",
       "label3                                                 hatespeech\n",
       "rationales      [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, ...\n",
       "final_label                                            hatespeech\n",
       "Name: 99, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_read.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = temp_read\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
    "print('total_data',len(data))\n",
    "post_ids_list=[]\n",
    "text_list=[]\n",
    "rationales_list=[]\n",
    "label_list=[]\n",
    "for index,row in tqdm.tqdm(data.iterrows(),total=len(data)):\n",
    "    #print(params)\n",
    "    text=row['text']\n",
    "    post_id=row['post_id']\n",
    "\n",
    "    annotation_list=[row['label1'],row['label2'],row['label3']] \n",
    "    annotation=row['final_label']\n",
    "\n",
    "    #print(annotation_list,annotation)\n",
    "        \n",
    "    if(annotation != 'undecided'):\n",
    "        tokens,rationales = returnMask(row,tokenizer)\n",
    "        rationales_list.append(rationales)\n",
    "        text_list.append(tokens)\n",
    "        label_list.append(annotation)\n",
    "        post_ids_list.append(post_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/input/hatexplain-dataset/post_id_divisions.json', 'r') as fp:\n",
    "    post_id_dict=json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = pd.DataFrame(list(zip(post_ids_list,text_list,rationales_list,label_list)), \n",
    "                             columns =['Post_id','Text', 'Attention' , 'Label']) \n",
    "\n",
    "# print(len(tdata.iloc[0][\"Attention\"][0]))\n",
    "X_train=tdata[tdata['Post_id'].isin(post_id_dict['train'])]\n",
    "X_val=tdata[tdata['Post_id'].isin(post_id_dict['val'])]\n",
    "X_test=tdata[tdata['Post_id'].isin(post_id_dict['test'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.inputs = data[\"Text\"]\n",
    "        self.labels = data[\"Label\"]\n",
    "        self.rationales = data[\"Attention\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler,SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(tuple_data,is_train=False):\n",
    "    max_length =128\n",
    "    input_ids =  [ele[0] for ele in tuple_data]\n",
    "    att_vals = [ele[1] for ele in tuple_data]\n",
    "    labels = [ele [2] for ele in tuple_data]\n",
    "\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    encoder.classes_ = np.load('/kaggle/input/hatexplain-dataset/classes.npy',allow_pickle=True)\n",
    "    labels=encoder.transform(labels)\n",
    "    \n",
    "    input_ids = pad_sequences(input_ids,maxlen=max_length, \n",
    "                              dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "    #print(len(att_vals))\n",
    "    #print(att_vals,len(att_vals),\"Flag12\",len(att_vals[0]))\n",
    "\n",
    "    rationales_vals = []\n",
    "    for values in att_vals:\n",
    "        temp_rationales = pad_sequences(values,maxlen=max_length, \n",
    "                                     dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "        temp_rationales = torch.tensor(temp_rationales)\n",
    "        rationales_vals.append(temp_rationales)\n",
    "    rationales_vals = torch.stack(rationales_vals,dim=0)    \n",
    "    #print(att_vals,len(att_vals),\"Flag11\",len(att_vals[0]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    att_masks=custom_att_masks(input_ids)\n",
    "    dataloader=return_dataloader(input_ids,labels,rationales_vals,att_masks,is_train)\n",
    "    return dataloader\n",
    "\n",
    "def return_dataloader(input_ids,labels,att_vals,att_masks,is_train=False):\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    labels = torch.tensor(labels,dtype=torch.long)\n",
    "    masks = torch.tensor(np.array(att_masks),dtype=torch.uint8)\n",
    "    print(att_vals.shape)\n",
    "    #attention = torch.tensor(np.array(att_vals),dtype=torch.float)\n",
    "    attention = att_vals\n",
    "    #print(inputs.shape,attention.shape,masks.shape,labels.shape)\n",
    "    data = TensorDataset(inputs,attention,masks,labels)\n",
    "    if(is_train==False):\n",
    "        sampler = SequentialSampler(data)\n",
    "    else:\n",
    "        sampler = RandomSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=32)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_att_masks(input_ids):\n",
    "    attention_masks = []\n",
    "\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "\n",
    "        # Create the attention mask.256\n",
    "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "\n",
    "        # Store the attention mask for this sentence.\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeData(dataframe):\n",
    "    tuple_new_data=[]\n",
    "    for index,row in tqdm.tqdm(dataframe.iterrows(),total=len(dataframe)):\n",
    "        tuple_new_data.append((row['Text'],row['Attention'],row['Label']))\n",
    "    return tuple_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encodeData(X_train)\n",
    "X_val = encodeData(X_val)\n",
    "X_test = encodeData(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X_train to list of lists to be passed to combine features\n",
    "train_dataloader = combine_features(X_train,is_train=True)\n",
    "validation_dataloader = combine_features(X_val,is_train=False)\n",
    "test_dataloader=combine_features(X_test,is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = textDataset(X_train)\n",
    "# valset = textDataset(X_val)\n",
    "# testset = textDataset(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader = DataLoader(trainset, batch_size=16000, shuffle=False)\n",
    "# valloader = DataLoader(valset,batch_size=3000,shuffle=False)\n",
    "# testloader = DataLoader(testset,batch_size=3000,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,batch in enumerate(train_dataloader):\n",
    "    tokens, rationales, mask, labels = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 250\n",
    "max_steps = 500\n",
    "log_every = 480\n",
    "lr = 1e-3  #2e-5 works well\n",
    "weight_decay = 1e-5\n",
    "model_cfg = Config(debug=False, d_model=768, n_heads=1, d_head=768, d_mlp=512, n_layers=1, n_ctx=256, d_vocab=tokenizer.vocab_size)\n",
    "model = DemoTransformer(model_cfg)\n",
    "model.cuda()\n",
    "#\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr,weight_decay=weight_decay,momentum=0.99)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in model.named_parameters():\n",
    "    print(params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) * num_epochs\n",
    "print(total_steps)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = int(total_steps/10),num_training_steps = total_steps)\n",
    "#get_linear_schedule_with_warmup(optimizer, num_warmup_steps = int(total_steps/10), num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(model,dataloader,dataset=\"train\"):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    gt = []\n",
    "    for c, batch in tqdm.tqdm(enumerate(dataloader)):\n",
    "        inputs,rationales,mask,tlabels = batch\n",
    "        inputs,tlabels,mask = inputs.to(device),tlabels.to(device),mask.to(device)\n",
    "        outputs,_= model(inputs)\n",
    "        #print(outputs.shape)\n",
    "        pred.append(torch.argmax(outputs,dim=1).cpu())\n",
    "        gt.append(tlabels.cpu())\n",
    "        #print(len(pred))\n",
    "    pred = torch.hstack(pred)\n",
    "    #print(pred.size())\n",
    "    gt = torch.hstack(gt)\n",
    "    print(\"Acccuracy on \"+ dataset + \":\", sum(pred == gt)/len(pred))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Criterion = nn.CrossEntropyLoss()\n",
    "device = \"cuda\"\n",
    "\n",
    "print(\"Number of batches:\", len(train_dataloader))\n",
    "ep_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for c, batch in tqdm.tqdm(enumerate(train_dataloader)):\n",
    "        inputs,rationales,mask,tlabels = batch\n",
    "        #tokens, rationales, mask, labels\n",
    "        inputs,tlabels,mask = inputs.to(device),tlabels.to(device),mask.to(device)\n",
    "        outputs,_ = model(inputs)\n",
    "        \n",
    "        #print(outputs.shape,tlabels.squeeze(1).shape)\n",
    "        loss_cls = Criterion(outputs,tlabels)\n",
    "        #print(loss_cls)\n",
    "        #loss_ntp = lm_cross_entropy_loss(logits,inputs)\n",
    "        loss =  loss_cls\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        if c % log_every == 0:\n",
    "            print(f\"Step: {c}, Loss: {loss.item():.4f}, Loss Cls: {loss_cls.item():.4f}\")\n",
    "    ep_loss.append(np.mean(losses))\n",
    "    calculate_performance(model,train_dataloader,dataset=\"train_set\")\n",
    "    calculate_performance(model,validation_dataloader,dataset=\"validation_set\")\n",
    "    calculate_performance(model,test_dataloader,dataset=\"test_set\")\n",
    "        # if c > max_steps:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#px.line(y=losses, x=np.arange(len(losses))*(model_cfg.n_ctx * batch_size), labels={\"y\":\"Loss\", \"x\":\"Tokens\"}, title=\"Training curve for my tiny demo model!\")\n",
    "px.line(y=ep_loss,x=np.arange(len(ep_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_performance(model,train_dataloader,dataset=\"train_set\")\n",
    "calculate_performance(model,validation_dataloader,dataset=\"validation_set\")\n",
    "calculate_performance(model,test_dataloader,dataset=\"test_set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmaps(model,dataloader,name):\n",
    "    attn_values = []\n",
    "    prediction_values = []\n",
    "    for c, batch in tqdm.tqdm(enumerate(dataloader)):\n",
    "        inputs,rationales,mask,tlabels = batch\n",
    "        inputs,tlabels,mask = inputs.to(device),tlabels.to(device),mask.to(device)\n",
    "        outputs,attn= model(inputs)\n",
    "        outputs = nn.Softmax(dim=1)(outputs)\n",
    "        #print(attn[0].shape)\n",
    "        for i in range(len(attn[0])):\n",
    "            temp_attn = attn[0][i,0,-1].cpu().detach().numpy()\n",
    "            #print(temp_attn.shape)\n",
    "            #break\n",
    "            temp = 0 \n",
    "            for rationale in rationales[i]:\n",
    "                temp_rationale = rationale\n",
    "                indices = temp_rationale.long().cpu().numpy()\n",
    "                temp += sum(temp_attn[indices.astype(bool)])\n",
    "            attn_values.append(temp/3)\n",
    "            prediction_values.append(outputs[i,tlabels[i]].item())\n",
    "\n",
    "    print(\"attention\",sum( np.array(attn_values)<=0.33)/len(attn_values),sum(np.logical_and(np.array(attn_values)>0.33, np.array(attn_values)<=0.66))/len(attn_values),sum(np.array(attn_values)>0.66)/len(attn_values))\n",
    "    print(\"prediction\",sum( np.array(prediction_values)<=0.33)/len(prediction_values),sum(np.logical_and(np.array(prediction_values)>0.33, np.array(prediction_values)<=0.66))/len(prediction_values),sum(np.array(prediction_values)>0.66)/len(prediction_values))\n",
    "    fig, ax = plt.subplots()\n",
    "    h, xedges, yedges, im = ax.hist2d(np.array(attn_values),np.array(prediction_values),[[0,0.33,0.66,1.1],[0,0.33,0.66,1.1]])\n",
    "    plt.close(fig)\n",
    "    temp = (h.T/h.sum())*100\n",
    "\n",
    "    # Prevent automatic plotting by removing `plt.show()`\n",
    "    # or manually remove the image\n",
    "    im.remove()  # This prevents the heatmap from being displayed\n",
    "    plt.figure(figsize=(7,7))\n",
    "    ax = sns.heatmap(np.round(temp,2),vmin=5,vmax=70,annot=np.round(temp,2),fmt=\"g\",cmap=sns.color_palette(\"coolwarm\"),\n",
    "    yticklabels=[0.33,0.66,1.],\n",
    "    xticklabels=[0.33,0.66,1],annot_kws={\"size\":18},cbar=False)\n",
    "    ax.invert_yaxis()\n",
    "    plt.xlabel(r\"distinct token attention\",fontweight=\"bold\",fontsize=14)\n",
    "    plt.ylabel(r\"true token probability\",fontweight=\"bold\",fontsize=14) # change xlabel based on algo\n",
    "    plt.xticks([0,1,2,3],[0,0.33,0.66,1],weight=\"bold\",fontsize=14)\n",
    "    plt.yticks([0,1,2,3],[0,0.33,0.66,1],weight=\"bold\", va=\"top\",fontsize=14)\n",
    "    plt.savefig(name+\".pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting1 = \"same_lr_train_no_res_connection\"\n",
    "setting2 = \"same_lr_validation_no_res_connection\"\n",
    "setting3 = \"same_lr_test_no_res_connection\"\n",
    "plot_heatmaps(model,train_dataloader,name = setting1)\n",
    "plot_heatmaps(model,validation_dataloader,name = setting2)\n",
    "plot_heatmaps(model,test_dataloader,name = setting3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # model.eval()\n",
    "    # pred = []\n",
    "    # gt = []\n",
    "    # for c, batch in tqdm.tqdm(enumerate(dataloader)):\n",
    "    #     inputs,rationales,mask,tlabels = batch\n",
    "    #     inputs,tlabels,mask = inputs.to(device),tlabels.to(device),mask.to(device)\n",
    "    #     _,outputs,_= model(inputs)\n",
    "    #     #print(outputs.shape)\n",
    "    #     pred.append(torch.argmax(outputs,dim=1).cpu())\n",
    "    #     gt.append(tlabels.cpu())\n",
    "    #     #print(len(pred))\n",
    "    # pred = torch.hstack(pred)\n",
    "    # #print(pred.size())\n",
    "    # gt = torch.hstack(gt)\n",
    "    # print(\"Acccuracy on \"+ dataset + \":\", sum(pred == gt)/len(pred))\n",
    "    # model.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6760154,
     "sourceId": 10879915,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
