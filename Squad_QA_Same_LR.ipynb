{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=8\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
    "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load SQuAD dataset and select only 5 examples\n",
    "# squad = load_dataset(\"squad\", split=\"train[:1]\")\n",
    "\n",
    "# for example in squad:\n",
    "#     print(f\"Context: {example['context']}\")\n",
    "#     print(f\"Question: {example['question']}\")\n",
    "#     print(f\"Answer: {example['answers']['text'][0]}\")\n",
    "#     print(\"=\"*80)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def format_qa(example):\n",
    "#     prompt = f\"Context: {example['context']}\\nQuestion: {example['question']}\\nAnswer:\"\n",
    "#     answer = example['answers']['text'][0] + tokenizer.eos_token\n",
    "#     return {\"input_text\": prompt, \"output_text\": answer}\n",
    "\n",
    "# formatted = squad.map(format_qa)\n",
    "\n",
    "# def tokenize(example):\n",
    "#     input_ids = tokenizer(example['input_text'], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "#     labels = tokenizer(example['input_text'] + \" \" + example['output_text'], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "#     return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "# tokenized_dataset = formatted.map(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class QADataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             \"input_ids\": self.data[idx][\"input_ids\"],\n",
    "#             \"attention_mask\": self.data[idx][\"input_ids\"] != tokenizer.pad_token_id,\n",
    "#             \"labels\": self.data[idx][\"labels\"]\n",
    "#         }\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# qa_dataset = QADataset(tokenized_dataset)\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./qa-gpt2\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=2,\n",
    "#     logging_steps=1,\n",
    "#     save_steps=10,\n",
    "#     save_total_limit=1,\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=qa_dataset,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# def generate_answer(context, question):\n",
    "#     prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "#     output = model.generate(input_ids, max_new_tokens=10,  eos_token_id=tokenizer.eos_token_id,pad_token_id=tokenizer.eos_token_id)  # Avoid warning if pad token is undefined)\n",
    "#     answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#     return answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# # Test on training examples\n",
    "# for example in squad:\n",
    "#     print(\"Q:\", example['question'])\n",
    "#     print(\"Predicted A:\", generate_answer(example['context'], example['question']))\n",
    "#     print(\"True A:\", example['answers']['text'][0])\n",
    "#     print(\"=\"*60)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model with Separate Key Query and Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:32.844029Z",
     "iopub.status.busy": "2025-07-27T13:42:32.843496Z",
     "iopub.status.idle": "2025-07-27T13:42:40.510000Z",
     "shell.execute_reply": "2025-07-27T13:42:40.509239Z",
     "shell.execute_reply.started": "2025-07-27T13:42:32.844002Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2Model, GPT2PreTrainedModel\n",
    "from transformers.modeling_utils import Conv1D\n",
    "\n",
    "# --------- Custom Attention Module using Conv1D ----------\n",
    "class CustomGPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.q_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "        self.k_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "        self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Register causal mask buffer (same as original GPT2)\n",
    "        max_positions = config.max_position_embeddings\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "                1, 1, max_positions, max_positions\n",
    "            ),\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4))\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        # embed_dim should equal num_heads * head_dim\n",
    "        assert embed_dim == self.num_heads * self.head_dim, f\"Embed dim {embed_dim} != num_heads * head_dim {self.num_heads * self.head_dim}\"\n",
    "        \n",
    "        # reshape to (batch_size, seq_len, num_heads, head_dim)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        # permute to (batch_size, num_heads, seq_len, head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def _merge_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (self.embed_dim,)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def forward(self, hidden_states, layer_past=None, attention_mask=None,\n",
    "                head_mask=None, use_cache=False, output_attentions=False):\n",
    "    \n",
    "        # Project to Q, K, V\n",
    "        query = self.q_proj(hidden_states)\n",
    "        key = self.k_proj(hidden_states)\n",
    "        value = self.v_proj(hidden_states)\n",
    "\n",
    "        # Split heads\n",
    "        query = self._split_heads(query)\n",
    "        key = self._split_heads(key)\n",
    "        value = self._split_heads(value)\n",
    "\n",
    "        # Handle past keys/values for generation\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        if use_cache:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attn_weights = attn_weights * self.scale\n",
    "\n",
    "        # Apply causal mask\n",
    "        query_length, key_length = query.size(-2), key.size(-2)\n",
    "        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "        mask_value = torch.finfo(attn_weights.dtype).min\n",
    "        mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
    "        attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
    "\n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # Softmax\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = attn_weights.type(value.dtype)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Apply head mask if provided\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "        attn_output = self._merge_heads(attn_output)\n",
    "\n",
    "        # Final projection\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:40.512257Z",
     "iopub.status.busy": "2025-07-27T13:42:40.511508Z",
     "iopub.status.idle": "2025-07-27T13:42:40.518055Z",
     "shell.execute_reply": "2025-07-27T13:42:40.517278Z",
     "shell.execute_reply.started": "2025-07-27T13:42:40.512235Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------- Custom GPT2 Block ----------\n",
    "class CustomGPT2Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.attn = CustomGPT2Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = GPT2Block(config).mlp\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        layer_past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_outputs[0]\n",
    "        outputs = attn_outputs[1:]\n",
    "\n",
    "        hidden_states = residual + attn_output\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        return (hidden_states,) + outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:40.519032Z",
     "iopub.status.busy": "2025-07-27T13:42:40.518792Z",
     "iopub.status.idle": "2025-07-27T13:42:40.535290Z",
     "shell.execute_reply": "2025-07-27T13:42:40.534644Z",
     "shell.execute_reply.started": "2025-07-27T13:42:40.519005Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------- Custom GPT2 Model ----------\n",
    "class CustomGPT2Model(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.h = nn.ModuleList([CustomGPT2Block(config) for _ in range(config.num_hidden_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:40.536558Z",
     "iopub.status.busy": "2025-07-27T13:42:40.536321Z",
     "iopub.status.idle": "2025-07-27T13:42:40.553224Z",
     "shell.execute_reply": "2025-07-27T13:42:40.552637Z",
     "shell.execute_reply.started": "2025-07-27T13:42:40.536532Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        # Initialize the parent class first\n",
    "        super(GPT2PreTrainedModel, self).__init__(config)\n",
    "        \n",
    "        # Replace the transformer with our custom one\n",
    "        self.transformer = CustomGPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Forward through transformer (excluding labels)\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        hidden_states = transformer_outputs[0]\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past_key_values:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "\n",
    "            # Some generation methods already pass only the last input ID\n",
    "            if input_ids.shape[1] > past_length:\n",
    "                remove_prefix_length = past_length\n",
    "            else:\n",
    "                # Default to old behavior: keep only final ID\n",
    "                remove_prefix_length = input_ids.shape[1] - 1\n",
    "\n",
    "            input_ids = input_ids[:, remove_prefix_length:]\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "        else:\n",
    "            position_ids = None\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"position_ids\": position_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:40.554226Z",
     "iopub.status.busy": "2025-07-27T13:42:40.553923Z",
     "iopub.status.idle": "2025-07-27T13:42:40.573529Z",
     "shell.execute_reply": "2025-07-27T13:42:40.572963Z",
     "shell.execute_reply.started": "2025-07-27T13:42:40.554202Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------- Copy Weights from Original GPT2 Model ----------\n",
    "def copy_weights(original_model, custom_model):\n",
    "    orig_state_dict = original_model.state_dict()\n",
    "    custom_state_dict = custom_model.state_dict()\n",
    "\n",
    "    for name, param in orig_state_dict.items():\n",
    "        if \"attn.c_attn.weight\" in name:\n",
    "            layer_num = int(name.split('.')[2])\n",
    "            prefix = f'transformer.h.{layer_num}.attn.'\n",
    "            \n",
    "            # Original c_attn weight shape: (embed_dim, 3 * embed_dim)\n",
    "            # Need to split along dim=1 (the 3 * embed_dim dimension)\n",
    "            embed_dim = param.shape[0]\n",
    "            q_weight, k_weight, v_weight = torch.split(param, embed_dim, dim=1)\n",
    "            \n",
    "            # Conv1D weight shape is (input_dim, output_dim), no transpose needed\n",
    "            custom_state_dict[f'{prefix}q_proj.weight'].copy_(q_weight)\n",
    "            custom_state_dict[f'{prefix}k_proj.weight'].copy_(k_weight)  \n",
    "            custom_state_dict[f'{prefix}v_proj.weight'].copy_(v_weight)\n",
    "\n",
    "        elif \"attn.c_attn.bias\" in name:\n",
    "            layer_num = int(name.split('.')[2])\n",
    "            prefix = f'transformer.h.{layer_num}.attn.'\n",
    "            hidden_size = param.shape[0] // 3\n",
    "\n",
    "            q_bias, k_bias, v_bias = torch.split(param, hidden_size)\n",
    "            custom_state_dict[f'{prefix}q_proj.bias'].copy_(q_bias)\n",
    "            custom_state_dict[f'{prefix}k_proj.bias'].copy_(k_bias)\n",
    "            custom_state_dict[f'{prefix}v_proj.bias'].copy_(v_bias)\n",
    "\n",
    "        elif \"attn.c_proj.weight\" in name:\n",
    "            # Copy c_proj weights directly\n",
    "            layer_num = int(name.split('.')[2])\n",
    "            prefix = f'transformer.h.{layer_num}.attn.'\n",
    "            custom_state_dict[f'{prefix}c_proj.weight'].copy_(param)\n",
    "            \n",
    "        else:\n",
    "            if name in custom_state_dict:\n",
    "                custom_state_dict[name].copy_(param)\n",
    "\n",
    "    custom_model.load_state_dict(custom_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T13:42:41.681772Z",
     "iopub.status.busy": "2025-07-27T13:42:41.681479Z",
     "iopub.status.idle": "2025-07-27T13:42:41.685308Z",
     "shell.execute_reply": "2025-07-27T13:42:41.684528Z",
     "shell.execute_reply.started": "2025-07-27T13:42:41.681747Z"
    }
   },
   "outputs": [],
   "source": [
    "# config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "# original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# custom_model = CustomGPT2LMHeadModel(config)\n",
    "# copy_weights(original_model, custom_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:07:41.305338Z",
     "iopub.status.busy": "2025-07-27T16:07:41.305029Z",
     "iopub.status.idle": "2025-07-27T16:41:42.576131Z",
     "shell.execute_reply": "2025-07-27T16:41:42.575294Z",
     "shell.execute_reply.started": "2025-07-27T16:07:41.305319Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_scheduler\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "from torch.optim import AdamW,SGD\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# Load and prepare SQuAD dataset\n",
    "# -------------------------------\n",
    "squad = load_dataset(\"squad\", split=\"train[:20000]\")  # Small subset for testing\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Initialize the fast tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to EOS\n",
    "\n",
    "def format_qa(example):\n",
    "    prompt = f\"Context: {example['context']}\\nQuestion: {example['question']}\\nAnswer:\"\n",
    "    answer = example['answers']['text'][0] + tokenizer.eos_token\n",
    "    return {\"input_text\": prompt, \"output_text\": answer}\n",
    "\n",
    "formatted = squad.map(format_qa)\n",
    "\n",
    "def tokenize(example):\n",
    "    input_enc = tokenizer(example['input_text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    output_enc = tokenizer(example['input_text'] + \" \" + example['output_text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    return {\n",
    "        \"input_ids\": input_enc[\"input_ids\"],\n",
    "        \"attention_mask\": input_enc[\"attention_mask\"],\n",
    "        \"labels\": output_enc[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "tokenized = formatted.map(tokenize)\n",
    "\n",
    "# -------------------------------\n",
    "# PyTorch Dataset\n",
    "# -------------------------------\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.data[idx][\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(self.data[idx][\"attention_mask\"]),\n",
    "            \"labels\": torch.tensor(self.data[idx][\"labels\"]),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "qa_dataset = QADataset(tokenized)\n",
    "\n",
    "# DataLoader with default collate_fn\n",
    "dataloader = DataLoader(qa_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Model and Optimizer Setup\n",
    "# -------------------------------\n",
    "#model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = CustomGPT2LMHeadModel(config)\n",
    "copy_weights(original_model, model)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# optimizer = AdamW(\n",
    "#     [\n",
    "#         {\"params\": [p for n, p in model.named_parameters() if 'attn.q_proj' in n or 'attn.k_proj' in n], \"lr\": 2e-4},\n",
    "#         {\"params\": [p for n, p in model.named_parameters() if 'attn.q_proj' not in n and 'attn.k_proj' not in n], \"lr\": 5e-5}\n",
    "#     ],\n",
    "#     weight_decay=0.01,\n",
    "#     eps=1e-8\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epochs = 25\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Scheduler\n",
    "num_training_steps = len(dataloader) * epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop with Logging\n",
    "# -------------------------------\n",
    "logging_steps = 200\n",
    "global_step = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    loop = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for step, batch in enumerate(loop):\n",
    "        global_step += 1\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if global_step % logging_steps == 0:\n",
    "            print(f\"Step {global_step} - Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:41:42.577928Z",
     "iopub.status.busy": "2025-07-27T16:41:42.577633Z",
     "iopub.status.idle": "2025-07-27T16:41:42.581643Z",
     "shell.execute_reply": "2025-07-27T16:41:42.580833Z",
     "shell.execute_reply.started": "2025-07-27T16:41:42.577911Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:41:42.582723Z",
     "iopub.status.busy": "2025-07-27T16:41:42.582445Z",
     "iopub.status.idle": "2025-07-27T16:41:42.597443Z",
     "shell.execute_reply": "2025-07-27T16:41:42.596857Z",
     "shell.execute_reply.started": "2025-07-27T16:41:42.582701Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Lowercase, remove punctuation, and normalize whitespace.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def generate_outputs(model, tokenizer, input_ids, answer_ids, device):\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    generated = input_ids.clone()\n",
    "    all_attentions = []\n",
    "    true_token_probs = []\n",
    "\n",
    "    for step in range(len(answer_ids)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=generated, output_attentions=True)\n",
    "        logits = outputs.logits\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        true_token_id = answer_ids[step].item()\n",
    "        true_prob = probs[0, true_token_id].item()\n",
    "        true_token_probs.append(true_prob)\n",
    "\n",
    "        # Debug print (optional)\n",
    "        # print(f\"Step {step}: True token id = {true_token_id}, Prob = {true_prob:.8f}\")\n",
    "\n",
    "        next_token = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        # Save attention from last token (list of tensors: layers × [batch, heads, seq_len])\n",
    "        all_attentions.append([a[:, :, -1, :] for a in attentions])\n",
    "\n",
    "        generated = torch.cat([generated, next_token.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        if next_token.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode only the generated tokens AFTER the prompt\n",
    "    gen_tokens = generated[0, input_ids.shape[-1]:]\n",
    "    Final_output = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    return Final_output, all_attentions, true_token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:41:42.599243Z",
     "iopub.status.busy": "2025-07-27T16:41:42.598744Z",
     "iopub.status.idle": "2025-07-27T16:46:09.171979Z",
     "shell.execute_reply": "2025-07-27T16:46:09.171238Z",
     "shell.execute_reply.started": "2025-07-27T16:41:42.599225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main evaluation loop\n",
    "count = 0\n",
    "model.eval()\n",
    "analysis = []\n",
    "for example in tqdm(squad):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer_text = example[\"answers\"][\"text\"][0]\n",
    "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    # Encode prompt with offsets\n",
    "    encodings = tokenizer(prompt, return_offsets_mapping=True, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    offsets = encodings[\"offset_mapping\"][0].tolist()  # list of (start, end)\n",
    "\n",
    "    # Locate where context starts in prompt (to offset answer span properly)\n",
    "    context_start_in_prompt = prompt.find(context)\n",
    "    answer_start_char = example[\"answers\"][\"answer_start\"][0] + context_start_in_prompt\n",
    "    answer_end_char = answer_start_char + len(answer_text)\n",
    "\n",
    "    # Map character span of answer to token indices in the prompt\n",
    "    token_indices = [\n",
    "        i for i, (start, end) in enumerate(offsets)\n",
    "        if start < answer_end_char and end > answer_start_char\n",
    "    ]\n",
    "\n",
    "    # Encode ground-truth answer tokens WITH leading space because generated tokens usually include it\n",
    "    answer_ids = tokenizer.encode(\" \" + answer_text, add_special_tokens=False)\n",
    "    answer_ids = torch.tensor(answer_ids).to(device)\n",
    "\n",
    "    # Generate predicted answer + track true token probabilities\n",
    "    output, attns, true_token_probs = generate_outputs(model, tokenizer, input_ids, answer_ids, device)\n",
    "\n",
    "    # Normalize answers for fair comparison\n",
    "    norm_true = normalize_text(answer_text)\n",
    "    norm_pred = normalize_text(output)\n",
    "\n",
    "    # Average true token probability over all steps\n",
    "    avg_true_token_prob = sum(true_token_probs) / len(true_token_probs)\n",
    "\n",
    "    # Compute average attention score over heads and steps for true token indices\n",
    "    total_attention = 0.0\n",
    "    for step_attn in attns:\n",
    "        #print(len(step_attn),step_attn[0].shape)\n",
    "        last_layer_attn = step_attn[1][0]  # last layer, batch 0: (num_heads, seq_len)\n",
    "        step_total = 0.0\n",
    "        for h in range(last_layer_attn.shape[0]):\n",
    "            head_attn = last_layer_attn[h]\n",
    "            step_total += sum(head_attn[j] for j in token_indices if j < head_attn.shape[0])\n",
    "        avg_step_attn = step_total / last_layer_attn.shape[0]  # average over heads\n",
    "        total_attention += avg_step_attn\n",
    "    true_token_attention_score = total_attention / len(attns)  # average over steps\n",
    "\n",
    "    # Print results\n",
    "    #print(\"=\" * 60)\n",
    "    #print(f\"Example #{count + 1}\")\n",
    "    #print(\"True Answer     :\", answer_text)\n",
    "    #print(\"Predicted Answer:\", output)\n",
    "    #print(\"Normalized True :\", norm_true)\n",
    "    #print(\"Normalized Pred :\", norm_pred)\n",
    "    #print(\"Avg True Token Prob: {:.6f}\".format(avg_true_token_prob))\n",
    "    #print(\"True Token Attention Score: {:.6f}\".format(true_token_attention_score.item()))\n",
    "\n",
    "    analysis.append([norm_true,norm_pred,avg_true_token_prob,true_token_attention_score.item()])\n",
    "\n",
    "    # count += 1\n",
    "    # if count >= 200:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:46:09.173128Z",
     "iopub.status.busy": "2025-07-27T16:46:09.172878Z",
     "iopub.status.idle": "2025-07-27T16:46:09.201571Z",
     "shell.execute_reply": "2025-07-27T16:46:09.201057Z",
     "shell.execute_reply.started": "2025-07-27T16:46:09.173087Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save to file\n",
    "with open(\"same_train_qa_eval_results_layer_1.json\", \"w\") as f:\n",
    "    json.dump(analysis, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:46:09.202450Z",
     "iopub.status.busy": "2025-07-27T16:46:09.202230Z",
     "iopub.status.idle": "2025-07-27T16:46:09.229197Z",
     "shell.execute_reply": "2025-07-27T16:46:09.228647Z",
     "shell.execute_reply.started": "2025-07-27T16:46:09.202433Z"
    }
   },
   "outputs": [],
   "source": [
    "tt = np.array(analysis)[:,-1].astype(float)\n",
    "sum(tt<=0.05), sum(tt>0.05)\n",
    "#386 114 # 10 times  500 dp\n",
    "#325, 208# 100 times 500 dp\n",
    "#419, 81  500 dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:46:09.229996Z",
     "iopub.status.busy": "2025-07-27T16:46:09.229791Z",
     "iopub.status.idle": "2025-07-27T16:46:09.234760Z",
     "shell.execute_reply": "2025-07-27T16:46:09.234134Z",
     "shell.execute_reply.started": "2025-07-27T16:46:09.229980Z"
    }
   },
   "outputs": [],
   "source": [
    "tt,1283/5000,149/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:46:23.764424Z",
     "iopub.status.busy": "2025-07-27T16:46:23.764173Z",
     "iopub.status.idle": "2025-07-27T16:46:24.020628Z",
     "shell.execute_reply": "2025-07-27T16:46:24.019614Z",
     "shell.execute_reply.started": "2025-07-27T16:46:23.764408Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "bin_edges = [np.array([0, 0.05, 0.1, 0.5, 1]), np.array([0, 0.05, 0.1, 0.5,  1])]\n",
    "\n",
    "num = 200\n",
    "\n",
    "hist,_,_= np.histogram2d( np.array(analysis)[:,-1].astype(float),  np.array(analysis)[:,-2].astype(float), bins=bin_edges)\n",
    "\n",
    "hist = hist/num\n",
    "# Assuming `mean_hist_norm` and `var_hist_norm` are your 2D arrays of the same shape\n",
    "# If you prefer to show std instead of variance in the annotation, convert variance to std:\n",
    "\n",
    "\n",
    "# Create annotation labels with both mean and std (formatted as strings)\n",
    "annot_array = np.empty_like(hist, dtype=object)\n",
    "\n",
    "\n",
    "for i in range(hist.shape[0]):\n",
    "    for j in range(hist.shape[1]):\n",
    "        annot_array[i, j] = f\"{hist[i, j]:.1f}\"#\\n(±{std_hist_norm[i, j]:.1f})\"  # mean ± std\n",
    "\n",
    "# # Define tick positions for edges of bins (for correct labeling)\n",
    "x_edges = np.array([0, 0.05, 0.1, 0.5, 1])\n",
    "y_edges = np.array([0, 0.05, 0.1, 0.5, 1])\n",
    "xtick_positions = np.arange(len(x_edges) - 1) + 1.0  # Right edges\n",
    "ytick_positions = np.arange(len(y_edges) - 1) + 1.0  # Top edges\n",
    "\n",
    "# Plot heatmap for mean values (color intensity)\n",
    "plt.figure(figsize=(6, 6))\n",
    "ax = sns.heatmap(hist.T, annot=annot_array.T, cmap=sns.color_palette(\"coolwarm\"), \n",
    "                  annot_kws={\"size\":18}, cbar=False, vmin=5, vmax=70,fmt=\"\")\n",
    "\n",
    "# Adjust tick positions for x and y axes (move them to the edges)\n",
    "ax.set_xticks(xtick_positions)\n",
    "ax.set_yticks(ytick_positions)\n",
    "\n",
    "# Set the labels for ticks (x and y edges)\n",
    "ax.set_xticklabels(x_edges[1:])\n",
    "ax.set_yticklabels(y_edges[1:])\n",
    "\n",
    "# # Make the tick labels bold\n",
    "ax.tick_params(axis='x', labelsize=14)  # Bold x-axis labels\n",
    "ax.tick_params(axis='y', labelsize=14)  # Bold y-axis labels\n",
    "\n",
    "\n",
    "# Invert y-axis to align with typical heatmap style\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(r\"Distinct Token Attention\", fontweight=\"bold\", fontsize=16)\n",
    "plt.ylabel(r\"Relevant Token Probability\", fontweight=\"bold\", fontsize=16)\n",
    "#plt.title(\"Mean (±Std Dev) Heatmap\")\n",
    "\n",
    "# # Save the figure\n",
    "plt.savefig(\"faster_qk_10_times_squad_train_l1.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:49:55.982762Z",
     "iopub.status.busy": "2025-07-27T16:49:55.982170Z",
     "iopub.status.idle": "2025-07-27T16:49:55.986376Z",
     "shell.execute_reply": "2025-07-27T16:49:55.985595Z",
     "shell.execute_reply.started": "2025-07-27T16:49:55.982740Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# def generate_answer(context, question):\n",
    "#     prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "#     output = model.generate(input_ids, max_new_tokens=10,  eos_token_id=tokenizer.eos_token_id,pad_token_id=tokenizer.eos_token_id)  # Avoid warning if pad token is undefined)\n",
    "#     answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#     return answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# count = 0 \n",
    "# # Test on training examples\n",
    "# for example in squad:\n",
    "#     print(\"Q:\", example['question'])\n",
    "#     print(\"Predicted A:\", generate_answer(example['context'], example['question']))\n",
    "#     print(\"True A:\", example['answers']['text'][0])\n",
    "#     print(\"=\"*60)\n",
    "#     count +=1\n",
    "#     if count>50:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:49:56.943020Z",
     "iopub.status.busy": "2025-07-27T16:49:56.942751Z",
     "iopub.status.idle": "2025-07-27T16:54:47.539988Z",
     "shell.execute_reply": "2025-07-27T16:54:47.539300Z",
     "shell.execute_reply.started": "2025-07-27T16:49:56.942999Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Normalize answer text for comparison\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.IGNORECASE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    \"\"\"Split text into tokens\"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact_match(a_gold, a_pred):\n",
    "    \"\"\"Compute exact match score\"\"\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    \"\"\"Compute F1 score\"\"\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    \n",
    "    if not gold_toks and not pred_toks:\n",
    "        return 1.0\n",
    "    \n",
    "    if not gold_toks or not pred_toks:\n",
    "        return 0.0\n",
    "    \n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = num_same / len(pred_toks)\n",
    "    recall = num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Your evaluation code with metrics\n",
    "model.eval()\n",
    "\n",
    "def generate_answer(context, question):\n",
    "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "    output = model.generate(input_ids, max_new_tokens=10, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Initialize metrics tracking\n",
    "count = 0\n",
    "exact_match_scores = []\n",
    "f1_scores = []\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "print(\"Evaluating model on SQuAD examples...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test on training examples\n",
    "for example in tqdm(squad):\n",
    "    # Generate prediction\n",
    "    predicted_answer = generate_answer(example['context'], example['question'])\n",
    "    true_answers = example['answers']['text']  # List of all valid answers\n",
    "    \n",
    "    # Take the best score across all possible answers (SQuAD style)\n",
    "    em_score = max(compute_exact_match(true_ans, predicted_answer) for true_ans in true_answers)\n",
    "    f1_score = max(compute_f1(true_ans, predicted_answer) for true_ans in true_answers)\n",
    "    \n",
    "    # Store scores\n",
    "    exact_match_scores.append(em_score)\n",
    "    f1_scores.append(f1_score)\n",
    "    predictions.append(predicted_answer)\n",
    "    ground_truths.append(true_answers[0])  # First answer for display\n",
    "    \n",
    "    # Display results\n",
    "    #print(f\"Question {count + 1}:\")\n",
    "    #print(f\"Q: {example['question']}\")\n",
    "    #print(f\"Predicted A: {predicted_answer}\")\n",
    "    #print(f\"True A: {true_answers[0]}\")\n",
    "    #print(f\"EM Score: {em_score} | F1 Score: {f1_score:.3f}\")\n",
    "    #print(\"=\" * 60)\n",
    "    \n",
    "    # count += 1\n",
    "    # if count > 5000:\n",
    "    #     break\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_em = sum(exact_match_scores) / len(exact_match_scores) * 100\n",
    "overall_f1 = sum(f1_scores) / len(f1_scores) * 100\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Questions: {len(exact_match_scores)}\")\n",
    "print(f\"Exact Match Score: {overall_em:.2f}%\")\n",
    "print(f\"F1 Score: {overall_f1:.2f}%\")\n",
    "\n",
    "# Additional statistics\n",
    "print(f\"\\nDetailed Statistics:\")\n",
    "print(f\"Questions with EM = 1: {sum(exact_match_scores)} ({sum(exact_match_scores)/len(exact_match_scores)*100:.1f}%)\")\n",
    "print(f\"Questions with F1 > 0.5: {sum(1 for f1 in f1_scores if f1 > 0.5)} ({sum(1 for f1 in f1_scores if f1 > 0.5)/len(f1_scores)*100:.1f}%)\")\n",
    "print(f\"Average F1 for non-zero scores: {sum(f1 for f1 in f1_scores if f1 > 0) / max(1, sum(1 for f1 in f1_scores if f1 > 0)):.3f}\")\n",
    "\n",
    "# Show some examples of different performance levels\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find examples with different performance levels\n",
    "perfect_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if em == 1]\n",
    "partial_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if em == 0 and f1 > 0.3]\n",
    "no_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if f1 == 0]\n",
    "\n",
    "if perfect_matches:\n",
    "    print(f\"\\nPerfect Matches (EM=1): {len(perfect_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(perfect_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}'\")\n",
    "\n",
    "if partial_matches:\n",
    "    print(f\"\\nPartial Matches (EM=0, F1>0.3): {len(partial_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(partial_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}' | F1: {f1_scores[idx]:.3f}\")\n",
    "\n",
    "if no_matches:\n",
    "    print(f\"\\nNo Matches (F1=0): {len(no_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(no_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}'\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save\n",
    "save_path = \"./faster_saved\"\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_validation = load_dataset(\"squad\", split=\"validation[:5000]\")  # Small subset for testing\n",
    "\n",
    "\n",
    "formatted_validation = squad_validation.map(format_qa) # format_qa is a function\n",
    "\n",
    "\n",
    "tokenized_validation = formatted_validation.map(tokenize) # tokenize is a function\n",
    "\n",
    "\n",
    "\n",
    "qa_dataset_validation = QADataset(tokenized_validation)\n",
    "\n",
    "# DataLoader with default collate_fn\n",
    "dataloader_validation = DataLoader(qa_dataset_validation, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evaluation loop\n",
    "count = 0\n",
    "model.eval()\n",
    "analysis_validation = []\n",
    "for example in tqdm(squad_validation):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer_text = example[\"answers\"][\"text\"][0]\n",
    "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    # Encode prompt with offsets\n",
    "    encodings = tokenizer(prompt, return_offsets_mapping=True, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    offsets = encodings[\"offset_mapping\"][0].tolist()  # list of (start, end)\n",
    "\n",
    "    # Locate where context starts in prompt (to offset answer span properly)\n",
    "    context_start_in_prompt = prompt.find(context)\n",
    "    answer_start_char = example[\"answers\"][\"answer_start\"][0] + context_start_in_prompt\n",
    "    answer_end_char = answer_start_char + len(answer_text)\n",
    "\n",
    "    # Map character span of answer to token indices in the prompt\n",
    "    token_indices = [\n",
    "        i for i, (start, end) in enumerate(offsets)\n",
    "        if start < answer_end_char and end > answer_start_char\n",
    "    ]\n",
    "\n",
    "    # Encode ground-truth answer tokens WITH leading space because generated tokens usually include it\n",
    "    answer_ids = tokenizer.encode(\" \" + answer_text, add_special_tokens=False)\n",
    "    answer_ids = torch.tensor(answer_ids).to(device)\n",
    "\n",
    "    # Generate predicted answer + track true token probabilities\n",
    "    output, attns, true_token_probs = generate_outputs(model, tokenizer, input_ids, answer_ids, device)\n",
    "\n",
    "    # Normalize answers for fair comparison\n",
    "    norm_true = normalize_text(answer_text)\n",
    "    norm_pred = normalize_text(output)\n",
    "\n",
    "    # Average true token probability over all steps\n",
    "    avg_true_token_prob = sum(true_token_probs) / len(true_token_probs)\n",
    "\n",
    "    # Compute average attention score over heads and steps for true token indices\n",
    "    total_attention = 0.0\n",
    "    for step_attn in attns:\n",
    "        last_layer_attn = step_attn[1][0]  # last layer, batch 0: (num_heads, seq_len)\n",
    "        step_total = 0.0\n",
    "        for h in range(last_layer_attn.shape[0]):\n",
    "            head_attn = last_layer_attn[h]\n",
    "            step_total += sum(head_attn[j] for j in token_indices if j < head_attn.shape[0])\n",
    "        avg_step_attn = step_total / last_layer_attn.shape[0]  # average over heads\n",
    "        total_attention += avg_step_attn\n",
    "    true_token_attention_score = total_attention / len(attns)  # average over steps\n",
    "\n",
    "    # Print results\n",
    "    #print(\"=\" * 60)\n",
    "    #print(f\"Example #{count + 1}\")\n",
    "    #print(\"True Answer     :\", answer_text)\n",
    "    #print(\"Predicted Answer:\", output)\n",
    "    #print(\"Normalized True :\", norm_true)\n",
    "    #print(\"Normalized Pred :\", norm_pred)\n",
    "    #print(\"Avg True Token Prob: {:.6f}\".format(avg_true_token_prob))\n",
    "    #print(\"True Token Attention Score: {:.6f}\".format(true_token_attention_score.item()))\n",
    "\n",
    "    analysis_validation.append([norm_true,norm_pred,avg_true_token_prob,true_token_attention_score.item()])\n",
    "\n",
    "    # count += 1\n",
    "    # if count >= 200:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save to file\n",
    "with open(\"same_validation_qa_eval_results_validation_l1.json\", \"w\") as f:\n",
    "    json.dump(analysis_validation, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.array(analysis_validation)[:,-1].astype(float)\n",
    "sum(tt<=0.05), sum(tt>0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "bin_edges = [np.array([0, 0.05, 0.1, 0.5, 1]), np.array([0, 0.05, 0.1, 0.5,  1])]\n",
    "\n",
    "num = 50\n",
    "\n",
    "hist,_,_= np.histogram2d( np.array(analysis_validation)[:,-1].astype(float),  np.array(analysis_validation)[:,-2].astype(float), bins=bin_edges)\n",
    "\n",
    "hist = hist/num\n",
    "# Assuming `mean_hist_norm` and `var_hist_norm` are your 2D arrays of the same shape\n",
    "# If you prefer to show std instead of variance in the annotation, convert variance to std:\n",
    "\n",
    "\n",
    "# Create annotation labels with both mean and std (formatted as strings)\n",
    "annot_array = np.empty_like(hist, dtype=object)\n",
    "\n",
    "\n",
    "for i in range(hist.shape[0]):\n",
    "    for j in range(hist.shape[1]):\n",
    "        annot_array[i, j] = f\"{hist[i, j]:.1f}\"#\\n(±{std_hist_norm[i, j]:.1f})\"  # mean ± std\n",
    "\n",
    "# # Define tick positions for edges of bins (for correct labeling)\n",
    "x_edges = np.array([0, 0.05, 0.1, 0.5, 1])\n",
    "y_edges = np.array([0, 0.05, 0.1, 0.5, 1])\n",
    "xtick_positions = np.arange(len(x_edges) - 1) + 1.0  # Right edges\n",
    "ytick_positions = np.arange(len(y_edges) - 1) + 1.0  # Top edges\n",
    "\n",
    "# Plot heatmap for mean values (color intensity)\n",
    "plt.figure(figsize=(6, 6))\n",
    "ax = sns.heatmap(hist.T, annot=annot_array.T, cmap=sns.color_palette(\"coolwarm\"), \n",
    "                  annot_kws={\"size\":18}, cbar=False, vmin=5, vmax=70,fmt=\"\")\n",
    "\n",
    "# Adjust tick positions for x and y axes (move them to the edges)\n",
    "ax.set_xticks(xtick_positions)\n",
    "ax.set_yticks(ytick_positions)\n",
    "\n",
    "# Set the labels for ticks (x and y edges)\n",
    "ax.set_xticklabels(x_edges[1:])\n",
    "ax.set_yticklabels(y_edges[1:])\n",
    "\n",
    "# # Make the tick labels bold\n",
    "ax.tick_params(axis='x', labelsize=14)  # Bold x-axis labels\n",
    "ax.tick_params(axis='y', labelsize=14)  # Bold y-axis labels\n",
    "\n",
    "\n",
    "# Invert y-axis to align with typical heatmap style\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(r\"Distinct Token Attention\", fontweight=\"bold\", fontsize=16)\n",
    "plt.ylabel(r\"Relevant Token Probability\", fontweight=\"bold\", fontsize=16)\n",
    "#plt.title(\"Mean (±Std Dev) Heatmap\")\n",
    "\n",
    "# # Save the figure\n",
    "plt.savefig(\"faster_qk_10_times_squad_validation_l1.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics tracking\n",
    "count = 0\n",
    "exact_match_scores = []\n",
    "f1_scores = []\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "print(\"Evaluating model on SQuAD examples...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test on training examples\n",
    "for example in tqdm(squad_validation):\n",
    "    # Generate prediction\n",
    "    predicted_answer = generate_answer(example['context'], example['question'])\n",
    "    true_answers = example['answers']['text']  # List of all valid answers\n",
    "    \n",
    "    # Take the best score across all possible answers (SQuAD style)\n",
    "    em_score = max(compute_exact_match(true_ans, predicted_answer) for true_ans in true_answers)\n",
    "    f1_score = max(compute_f1(true_ans, predicted_answer) for true_ans in true_answers)\n",
    "    \n",
    "    # Store scores\n",
    "    exact_match_scores.append(em_score)\n",
    "    f1_scores.append(f1_score)\n",
    "    predictions.append(predicted_answer)\n",
    "    ground_truths.append(true_answers[0])  # First answer for display\n",
    "    \n",
    "    # Display results\n",
    "    #print(f\"Question {count + 1}:\")\n",
    "    #print(f\"Q: {example['question']}\")\n",
    "    #print(f\"Predicted A: {predicted_answer}\")\n",
    "    #print(f\"True A: {true_answers[0]}\")\n",
    "    #print(f\"EM Score: {em_score} | F1 Score: {f1_score:.3f}\")\n",
    "    #print(\"=\" * 60)\n",
    "    \n",
    "    # count += 1\n",
    "    # if count > 5000:\n",
    "    #     break\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_em = sum(exact_match_scores) / len(exact_match_scores) * 100\n",
    "overall_f1 = sum(f1_scores) / len(f1_scores) * 100\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Questions: {len(exact_match_scores)}\")\n",
    "print(f\"Exact Match Score: {overall_em:.2f}%\")\n",
    "print(f\"F1 Score: {overall_f1:.2f}%\")\n",
    "\n",
    "# Additional statistics\n",
    "print(f\"\\nDetailed Statistics:\")\n",
    "print(f\"Questions with EM = 1: {sum(exact_match_scores)} ({sum(exact_match_scores)/len(exact_match_scores)*100:.1f}%)\")\n",
    "print(f\"Questions with F1 > 0.5: {sum(1 for f1 in f1_scores if f1 > 0.5)} ({sum(1 for f1 in f1_scores if f1 > 0.5)/len(f1_scores)*100:.1f}%)\")\n",
    "print(f\"Average F1 for non-zero scores: {sum(f1 for f1 in f1_scores if f1 > 0) / max(1, sum(1 for f1 in f1_scores if f1 > 0)):.3f}\")\n",
    "\n",
    "# Show some examples of different performance levels\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find examples with different performance levels\n",
    "perfect_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if em == 1]\n",
    "partial_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if em == 0 and f1 > 0.3]\n",
    "no_matches = [(i, predictions[i], ground_truths[i]) for i, (em, f1) in enumerate(zip(exact_match_scores, f1_scores)) if f1 == 0]\n",
    "\n",
    "if perfect_matches:\n",
    "    print(f\"\\nPerfect Matches (EM=1): {len(perfect_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(perfect_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}'\")\n",
    "\n",
    "if partial_matches:\n",
    "    print(f\"\\nPartial Matches (EM=0, F1>0.3): {len(partial_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(partial_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}' | F1: {f1_scores[idx]:.3f}\")\n",
    "\n",
    "if no_matches:\n",
    "    print(f\"\\nNo Matches (F1=0): {len(no_matches)} examples\")\n",
    "    for i, (idx, pred, true) in enumerate(no_matches[:3]):\n",
    "        print(f\"  {i+1}. Predicted: '{pred}' | True: '{true}'\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model and GPT Pretrained Model Weight and Output Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import GPT2LMHeadModel, GPT2Config\n",
    "# from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2Model, GPT2PreTrainedModel\n",
    "# from transformers.modeling_utils import Conv1D\n",
    "\n",
    "# # --------- Custom Attention Module using Conv1D ----------\n",
    "# class CustomGPT2Attention(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = config.hidden_size\n",
    "#         self.num_heads = config.num_attention_heads\n",
    "#         self.head_dim = self.embed_dim // self.num_heads\n",
    "#         assert self.head_dim * self.num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "#         self.q_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "#         self.k_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "#         self.v_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "#         self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
    "\n",
    "#         self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "#         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "#         self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "#         # Register causal mask buffer (same as original GPT2)\n",
    "#         max_positions = config.max_position_embeddings\n",
    "#         self.register_buffer(\n",
    "#             \"bias\",\n",
    "#             torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "#                 1, 1, max_positions, max_positions\n",
    "#             ),\n",
    "#         )\n",
    "#         self.register_buffer(\"masked_bias\", torch.tensor(-1e4))\n",
    "\n",
    "#     def _split_heads(self, x):\n",
    "#         batch_size, seq_len, embed_dim = x.size()\n",
    "#         # embed_dim should equal num_heads * head_dim\n",
    "#         assert embed_dim == self.num_heads * self.head_dim, f\"Embed dim {embed_dim} != num_heads * head_dim {self.num_heads * self.head_dim}\"\n",
    "        \n",
    "#         # reshape to (batch_size, seq_len, num_heads, head_dim)\n",
    "#         x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "#         # permute to (batch_size, num_heads, seq_len, head_dim)\n",
    "#         return x.permute(0, 2, 1, 3)\n",
    "\n",
    "#     def _merge_heads(self, x):\n",
    "#         x = x.permute(0, 2, 1, 3).contiguous()\n",
    "#         new_shape = x.size()[:-2] + (self.embed_dim,)\n",
    "#         return x.view(*new_shape)\n",
    "\n",
    "#     def forward(self, hidden_states, layer_past=None, attention_mask=None,\n",
    "#                 head_mask=None, use_cache=False, output_attentions=False):\n",
    "    \n",
    "#         # Project to Q, K, V\n",
    "#         query = self.q_proj(hidden_states)\n",
    "#         key = self.k_proj(hidden_states)\n",
    "#         value = self.v_proj(hidden_states)\n",
    "\n",
    "#         # Split heads\n",
    "#         query = self._split_heads(query)\n",
    "#         key = self._split_heads(key)\n",
    "#         value = self._split_heads(value)\n",
    "\n",
    "#         # Handle past keys/values for generation\n",
    "#         if layer_past is not None:\n",
    "#             past_key, past_value = layer_past\n",
    "#             key = torch.cat((past_key, key), dim=-2)\n",
    "#             value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "#         if use_cache:\n",
    "#             present = (key, value)\n",
    "#         else:\n",
    "#             present = None\n",
    "\n",
    "#         # Compute attention weights\n",
    "#         attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "#         attn_weights = attn_weights * self.scale\n",
    "\n",
    "#         # Apply causal mask\n",
    "#         query_length, key_length = query.size(-2), key.size(-2)\n",
    "#         causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "#         mask_value = torch.finfo(attn_weights.dtype).min\n",
    "#         mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
    "#         attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
    "\n",
    "#         # Apply attention mask if provided\n",
    "#         if attention_mask is not None:\n",
    "#             attn_weights = attn_weights + attention_mask\n",
    "\n",
    "#         # Softmax\n",
    "#         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "#         attn_weights = attn_weights.type(value.dtype)\n",
    "#         attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "#         # Apply head mask if provided\n",
    "#         if head_mask is not None:\n",
    "#             attn_weights = attn_weights * head_mask\n",
    "\n",
    "#         # Apply attention to values\n",
    "#         attn_output = torch.matmul(attn_weights, value)\n",
    "#         attn_output = self._merge_heads(attn_output)\n",
    "\n",
    "#         # Final projection\n",
    "#         attn_output = self.c_proj(attn_output)\n",
    "#         attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "#         outputs = (attn_output, present)\n",
    "#         if output_attentions:\n",
    "#             outputs += (attn_weights,)\n",
    "\n",
    "#         return outputs\n",
    "# # --------- Custom GPT2 Block ----------\n",
    "# class CustomGPT2Block(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.ln_1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "#         self.attn = CustomGPT2Attention(config)\n",
    "#         self.ln_2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "#         self.mlp = GPT2Block(config).mlp\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         hidden_states,\n",
    "#         layer_past=None,\n",
    "#         attention_mask=None,\n",
    "#         head_mask=None,\n",
    "#         encoder_hidden_states=None,\n",
    "#         encoder_attention_mask=None,\n",
    "#         use_cache=False,\n",
    "#         output_attentions=False,\n",
    "#     ):\n",
    "#         residual = hidden_states\n",
    "#         hidden_states = self.ln_1(hidden_states)\n",
    "\n",
    "#         attn_outputs = self.attn(\n",
    "#             hidden_states,\n",
    "#             layer_past=layer_past,\n",
    "#             attention_mask=attention_mask,\n",
    "#             head_mask=head_mask,\n",
    "#             use_cache=use_cache,\n",
    "#             output_attentions=output_attentions,\n",
    "#         )\n",
    "\n",
    "#         attn_output = attn_outputs[0]\n",
    "#         outputs = attn_outputs[1:]\n",
    "\n",
    "#         hidden_states = residual + attn_output\n",
    "\n",
    "#         residual = hidden_states\n",
    "#         hidden_states = self.ln_2(hidden_states)\n",
    "#         feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "#         hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "#         return (hidden_states,) + outputs\n",
    "\n",
    "# # --------- Custom GPT2 Model ----------\n",
    "# class CustomGPT2Model(GPT2Model):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.h = nn.ModuleList([CustomGPT2Block(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "# # --------- Custom GPT2 LM Model ----------\n",
    "# class CustomGPT2LMHeadModel(GPT2PreTrainedModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.transformer = CustomGPT2Model(config)\n",
    "#         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def forward(self, input_ids=None, **kwargs):\n",
    "#         transformer_outputs = self.transformer(input_ids=input_ids, **kwargs)\n",
    "#         hidden_states = transformer_outputs[0]\n",
    "#         lm_logits = self.lm_head(hidden_states)\n",
    "#         return lm_logits\n",
    "\n",
    "# # --------- Copy Weights from Original GPT2 Model ----------\n",
    "# def copy_weights(original_model, custom_model):\n",
    "#     orig_state_dict = original_model.state_dict()\n",
    "#     custom_state_dict = custom_model.state_dict()\n",
    "\n",
    "#     for name, param in orig_state_dict.items():\n",
    "#         if \"attn.c_attn.weight\" in name:\n",
    "#             layer_num = int(name.split('.')[2])\n",
    "#             prefix = f'transformer.h.{layer_num}.attn.'\n",
    "            \n",
    "#             # Original c_attn weight shape: (embed_dim, 3 * embed_dim)\n",
    "#             # Need to split along dim=1 (the 3 * embed_dim dimension)\n",
    "#             embed_dim = param.shape[0]\n",
    "#             q_weight, k_weight, v_weight = torch.split(param, embed_dim, dim=1)\n",
    "            \n",
    "#             # Conv1D weight shape is (input_dim, output_dim), no transpose needed\n",
    "#             custom_state_dict[f'{prefix}q_proj.weight'].copy_(q_weight)\n",
    "#             custom_state_dict[f'{prefix}k_proj.weight'].copy_(k_weight)  \n",
    "#             custom_state_dict[f'{prefix}v_proj.weight'].copy_(v_weight)\n",
    "\n",
    "#         elif \"attn.c_attn.bias\" in name:\n",
    "#             layer_num = int(name.split('.')[2])\n",
    "#             prefix = f'transformer.h.{layer_num}.attn.'\n",
    "#             hidden_size = param.shape[0] // 3\n",
    "\n",
    "#             q_bias, k_bias, v_bias = torch.split(param, hidden_size)\n",
    "#             custom_state_dict[f'{prefix}q_proj.bias'].copy_(q_bias)\n",
    "#             custom_state_dict[f'{prefix}k_proj.bias'].copy_(k_bias)\n",
    "#             custom_state_dict[f'{prefix}v_proj.bias'].copy_(v_bias)\n",
    "\n",
    "#         elif \"attn.c_proj.weight\" in name:\n",
    "#             # Copy c_proj weights directly\n",
    "#             layer_num = int(name.split('.')[2])\n",
    "#             prefix = f'transformer.h.{layer_num}.attn.'\n",
    "#             custom_state_dict[f'{prefix}c_proj.weight'].copy_(param)\n",
    "            \n",
    "#         else:\n",
    "#             if name in custom_state_dict:\n",
    "#                 custom_state_dict[name].copy_(param)\n",
    "\n",
    "#     custom_model.load_state_dict(custom_state_dict)\n",
    "\n",
    "# # --------- Parameter Comparison ----------\n",
    "# def compare_model_parameters(original_model, custom_model):\n",
    "#     orig_params = dict(original_model.named_parameters())\n",
    "#     cust_params = dict(custom_model.named_parameters())\n",
    "\n",
    "#     print(\"--- Comparing Non-Split Parameters ---\")\n",
    "#     for name, orig_param in orig_params.items():\n",
    "#         if \"attn.c_attn\" in name or \"attn.c_proj.weight\" in name:\n",
    "#             continue\n",
    "#         if name not in cust_params:\n",
    "#             print(f\"Parameter {name} missing in custom model\")\n",
    "#             continue\n",
    "#         cust_param = cust_params[name]\n",
    "#         if torch.allclose(orig_param, cust_param, atol=1e-6):\n",
    "#             c = 1 #print(f\"Parameter {name} matches.\")\n",
    "#         else:\n",
    "#             print(f\"Parameter {name} differs!\")\n",
    "\n",
    "#     print(\"\\n--- Comparing Split QKV Parameters ---\")\n",
    "#     for i in range(original_model.config.num_hidden_layers):\n",
    "#         prefix = f'transformer.h.{i}.'\n",
    "#         orig_w = orig_params[f'{prefix}attn.c_attn.weight']\n",
    "#         orig_b = orig_params[f'{prefix}attn.c_attn.bias']\n",
    "#         orig_proj_w = orig_params[f'{prefix}attn.c_proj.weight']\n",
    "        \n",
    "#         hidden_size = orig_w.shape[0]\n",
    "\n",
    "#         # Split the original concatenated weights\n",
    "#         orig_qw, orig_kw, orig_vw = torch.split(orig_w, hidden_size, dim=1)\n",
    "#         orig_qb, orig_kb, orig_vb = torch.split(orig_b, hidden_size)\n",
    "\n",
    "#         # Get custom model parameters\n",
    "#         cust_qw = cust_params[f'{prefix}attn.q_proj.weight']\n",
    "#         cust_qb = cust_params[f'{prefix}attn.q_proj.bias']\n",
    "#         cust_kw = cust_params[f'{prefix}attn.k_proj.weight']\n",
    "#         cust_kb = cust_params[f'{prefix}attn.k_proj.bias']\n",
    "#         cust_vw = cust_params[f'{prefix}attn.v_proj.weight']\n",
    "#         cust_vb = cust_params[f'{prefix}attn.v_proj.bias']\n",
    "#         cust_proj_w = cust_params[f'{prefix}attn.c_proj.weight']\n",
    "\n",
    "#         # Compare weights (no transpose needed for Conv1D)\n",
    "#         if torch.allclose(orig_qw, cust_qw, atol=1e-6): c = 1#print(f\"Layer {i} Q weight matches.\")\n",
    "#         else: print(f\"Layer {i} Q weight differs!\")\n",
    "        \n",
    "#         if torch.allclose(orig_kw, cust_kw, atol=1e-6): c =1 #print(f\"Layer {i} K weight matches.\")\n",
    "#         else: print(f\"Layer {i} K weight differs!\")\n",
    "        \n",
    "#         if torch.allclose(orig_vw, cust_vw, atol=1e-6): c =1 #print(f\"Layer {i} V weight matches.\")\n",
    "#         else: print(f\"Layer {i} V weight differs!\")\n",
    "        \n",
    "#         if torch.allclose(orig_proj_w, cust_proj_w, atol=1e-6): c= 1#print(f\"Layer {i} c_proj weight matches.\")\n",
    "#         else: print(f\"Layer {i} c_proj weight differs!\")\n",
    "\n",
    "#         # Compare biases (no transpose needed)\n",
    "#         if torch.allclose(orig_qb, cust_qb, atol=1e-6): c= 1#print(f\"Layer {i} Q bias matches.\")\n",
    "#         else: print(f\"Layer {i} Q bias differs!\")\n",
    "        \n",
    "#         if torch.allclose(orig_kb, cust_kb, atol=1e-6): c =1 #print(f\"Layer {i} K bias matches.\")\n",
    "#         else: print(f\"Layer {i} K bias differs!\")\n",
    "        \n",
    "#         if torch.allclose(orig_vb, cust_vb, atol=1e-6): c = 1#print(f\"Layer {i} V bias matches.\")\n",
    "#         else: print(f\"Layer {i} V bias differs!\")\n",
    "\n",
    "# # --------- Output Comparison ----------\n",
    "# def check_outputs_identical(original_model, custom_model):\n",
    "#     original_model.eval()\n",
    "#     custom_model.eval()\n",
    "\n",
    "#     batch_size = 2\n",
    "#     seq_len = 16\n",
    "#     vocab_size = original_model.config.vocab_size\n",
    "#     input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         orig_logits = original_model(input_ids).logits\n",
    "#         cust_logits = custom_model(input_ids)\n",
    "\n",
    "#     max_diff = torch.max(torch.abs(orig_logits - cust_logits)).item()\n",
    "#     print(f\"\\n--- Comparing Model Outputs ---\")\n",
    "#     print(f\"Max absolute difference between outputs: {max_diff:.10f}\")# Additional debugging steps to minimize differences\n",
    "\n",
    "# def debug_attention_step_by_step(original_model, custom_model, input_ids):\n",
    "#     \"\"\"Compare intermediate outputs in the first attention layer\"\"\"\n",
    "#     original_model.eval()\n",
    "#     custom_model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Get embeddings (should be identical)\n",
    "#         orig_embeds = original_model.transformer.wte(input_ids) + original_model.transformer.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "#         cust_embeds = custom_model.transformer.wte(input_ids) + custom_model.transformer.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "        \n",
    "#         print(\"Embedding difference:\", torch.max(torch.abs(orig_embeds - cust_embeds)).item())\n",
    "        \n",
    "#         # First layer norm\n",
    "#         orig_ln1 = original_model.transformer.h[0].ln_1(orig_embeds)\n",
    "#         cust_ln1 = custom_model.transformer.h[0].ln_1(cust_embeds)\n",
    "        \n",
    "#         print(\"First LayerNorm difference:\", torch.max(torch.abs(orig_ln1 - cust_ln1)).item())\n",
    "        \n",
    "#         # QKV projections\n",
    "#         orig_qkv = original_model.transformer.h[0].attn.c_attn(orig_ln1)\n",
    "#         orig_q, orig_k, orig_v = orig_qkv.split(original_model.config.hidden_size, dim=2)\n",
    "        \n",
    "#         cust_q = custom_model.transformer.h[0].attn.q_proj(cust_ln1)\n",
    "#         cust_k = custom_model.transformer.h[0].attn.k_proj(cust_ln1)\n",
    "#         cust_v = custom_model.transformer.h[0].attn.v_proj(cust_ln1)\n",
    "        \n",
    "#         print(\"Q projection difference:\", torch.max(torch.abs(orig_q - cust_q)).item())\n",
    "#         print(\"K projection difference:\", torch.max(torch.abs(orig_k - cust_k)).item())\n",
    "#         print(\"V projection difference:\", torch.max(torch.abs(orig_v - cust_v)).item())\n",
    "\n",
    "# def ensure_identical_initialization():\n",
    "#     \"\"\"Ensure both models have identical random initialization states\"\"\"\n",
    "#     torch.manual_seed(42)\n",
    "#     torch.cuda.manual_seed_all(42)\n",
    "#     # Set deterministic behavior\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# # Alternative: Use double precision for comparison\n",
    "# def check_outputs_double_precision(original_model, custom_model):\n",
    "#     \"\"\"Check outputs using double precision for more accurate comparison\"\"\"\n",
    "#     original_model.eval()\n",
    "#     custom_model.eval()\n",
    "    \n",
    "#     # Convert to double precision\n",
    "#     original_model = original_model.double()\n",
    "#     custom_model = custom_model.double()\n",
    "    \n",
    "#     batch_size = 2\n",
    "#     seq_len = 16\n",
    "#     vocab_size = original_model.config.vocab_size\n",
    "#     input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         orig_logits = original_model(input_ids).logits\n",
    "#         cust_logits = custom_model(input_ids)\n",
    "\n",
    "#     max_diff = torch.max(torch.abs(orig_logits - cust_logits)).item()\n",
    "#     print(f\"Double precision max difference: {max_diff:.15f}\")\n",
    "#     return max_diff\n",
    "#     if max_diff < 1e-4:\n",
    "#         print(\"Success! Outputs match within tolerance.\")\n",
    "#     else:\n",
    "#         print(\"Failure! Outputs still differ.\")\n",
    "\n",
    "\n",
    "# config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "\n",
    "# print(\"Loading original GPT-2 model...\")\n",
    "# original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# print(\"Creating custom GPT-2 model with split Q,K,V...\")\n",
    "# custom_model = CustomGPT2LMHeadModel(config)\n",
    "\n",
    "# print(\"Copying weights...\")\n",
    "# copy_weights(original_model, custom_model)\n",
    "\n",
    "# print(\"\\nComparing parameters...\")\n",
    "# compare_model_parameters(original_model, custom_model)\n",
    "\n",
    "# check_outputs_identical(original_model, custom_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional debugging steps to minimize differences\n",
    "\n",
    "# def debug_attention_step_by_step(original_model, custom_model, input_ids):\n",
    "#     \"\"\"Compare intermediate outputs in the first attention layer\"\"\"\n",
    "#     original_model.eval()\n",
    "#     custom_model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Get embeddings (should be identical)\n",
    "#         orig_embeds = original_model.transformer.wte(input_ids) + original_model.transformer.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "#         cust_embeds = custom_model.transformer.wte(input_ids) + custom_model.transformer.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "        \n",
    "#         print(\"Embedding difference:\", torch.max(torch.abs(orig_embeds - cust_embeds)).item())\n",
    "        \n",
    "#         # First layer norm\n",
    "#         orig_ln1 = original_model.transformer.h[0].ln_1(orig_embeds)\n",
    "#         cust_ln1 = custom_model.transformer.h[0].ln_1(cust_embeds)\n",
    "        \n",
    "#         print(\"First LayerNorm difference:\", torch.max(torch.abs(orig_ln1 - cust_ln1)).item())\n",
    "        \n",
    "#         # QKV projections\n",
    "#         orig_qkv = original_model.transformer.h[0].attn.c_attn(orig_ln1)\n",
    "#         orig_q, orig_k, orig_v = orig_qkv.split(original_model.config.hidden_size, dim=2)\n",
    "        \n",
    "#         cust_q = custom_model.transformer.h[0].attn.q_proj(cust_ln1)\n",
    "#         cust_k = custom_model.transformer.h[0].attn.k_proj(cust_ln1)\n",
    "#         cust_v = custom_model.transformer.h[0].attn.v_proj(cust_ln1)\n",
    "        \n",
    "#         print(\"Q projection difference:\", torch.max(torch.abs(orig_q - cust_q)).item())\n",
    "#         print(\"K projection difference:\", torch.max(torch.abs(orig_k - cust_k)).item())\n",
    "#         print(\"V projection difference:\", torch.max(torch.abs(orig_v - cust_v)).item())\n",
    "\n",
    "# def ensure_identical_initialization():\n",
    "#     \"\"\"Ensure both models have identical random initialization states\"\"\"\n",
    "#     torch.manual_seed(42)\n",
    "#     torch.cuda.manual_seed_all(42)\n",
    "#     # Set deterministic behavior\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# # Alternative: Use double precision for comparison\n",
    "# def check_outputs_double_precision(original_model, custom_model):\n",
    "#     \"\"\"Check outputs using double precision for more accurate comparison\"\"\"\n",
    "#     original_model.eval()\n",
    "#     custom_model.eval()\n",
    "    \n",
    "#     # Convert to double precision\n",
    "#     original_model = original_model.double()\n",
    "#     custom_model = custom_model.double()\n",
    "    \n",
    "#     batch_size = 2\n",
    "#     seq_len = 16\n",
    "#     vocab_size = original_model.config.vocab_size\n",
    "#     input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         orig_logits = original_model(input_ids).logits\n",
    "#         cust_logits = custom_model(input_ids)\n",
    "\n",
    "#     max_diff = torch.max(torch.abs(orig_logits - cust_logits)).item()\n",
    "#     print(f\"Double precision max difference: {max_diff:.15f}\")\n",
    "    \n",
    "#     # Convert back to float\n",
    "#     original_model = original_model.float()\n",
    "#     custom_model = custom_model.float()\n",
    "    \n",
    "#     return max_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = original_model.config.vocab_size\n",
    "# input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# debug_attention_step_by_step(original_model, custom_model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_outputs_double_precision(original_model, custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
